{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/notebooks/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/notebooks/venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "from utils.common import (\n",
    "    m2f_dataset_collate,\n",
    "    m2f_extract_pred_maps_and_masks,\n",
    "    BG_VALUE_255,\n",
    "    set_seed,\n",
    "    pixel_mean_std,\n",
    "    CADIS_PIXEL_MEAN,\n",
    "    CADIS_PIXEL_STD,\n",
    "    FULL_MERGE_PIXEL_MEAN,\n",
    "    FULL_MERGE_PIXEL_STD,\n",
    ")\n",
    "from utils.dataset_utils import (\n",
    "    get_cadisv2_dataset,\n",
    "    get_cataract1k_dataset,\n",
    "    ZEISS_CATEGORIES,\n",
    ")\n",
    "from utils.medical_datasets import Mask2FormerDataset\n",
    "from transformers import (\n",
    "    Mask2FormerForUniversalSegmentation,\n",
    "    SwinModel,\n",
    "    SwinConfig,\n",
    "    Mask2FormerConfig,\n",
    "    AutoImageProcessor,\n",
    "    Mask2FormerImageProcessor\n",
    ")\n",
    "from torch.utils.data import DataLoader\n",
    "import evaluate\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "set_seed(42) # seed everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/notebooks/venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not Matched: hidden_states_norms.stage1.weight != layernorm.weight\n",
      "Not Matched: hidden_states_norms.stage1.bias != layernorm.bias\n"
     ]
    }
   ],
   "source": [
    "NUM_CLASSES = len(ZEISS_CATEGORIES) - 3  # Remove class incremental\n",
    "SWIN_BACKBONE = \"microsoft/swin-tiny-patch4-window7-224\"#\"microsoft/swin-large-patch4-window12-384\"\n",
    "\n",
    "# Download pretrained swin model\n",
    "swin_model = SwinModel.from_pretrained(\n",
    "    SWIN_BACKBONE, out_features=[\"stage1\", \"stage2\", \"stage3\", \"stage4\"]\n",
    ")\n",
    "swin_config = SwinConfig.from_pretrained(\n",
    "    SWIN_BACKBONE, out_features=[\"stage1\", \"stage2\", \"stage3\", \"stage4\"]\n",
    ")\n",
    "\n",
    "# Create Mask2Former configuration based on Swin's configuration\n",
    "mask2former_config = Mask2FormerConfig(\n",
    "    backbone_config=swin_config, num_labels=NUM_CLASSES #, ignore_value=BG_VALUE\n",
    ")\n",
    "\n",
    "# Create the Mask2Former model with this configuration\n",
    "model = Mask2FormerForUniversalSegmentation(mask2former_config)\n",
    "\n",
    "# Reuse pretrained parameters\n",
    "for swin_param, m2f_param in zip(\n",
    "    swin_model.named_parameters(),\n",
    "    model.model.pixel_level_module.encoder.named_parameters(),\n",
    "):\n",
    "    m2f_param_name = f\"model.pixel_level_module.encoder.{m2f_param[0]}\"\n",
    "\n",
    "    if swin_param[0] == m2f_param[0]:\n",
    "        model.state_dict()[m2f_param_name].copy_(swin_param[1])\n",
    "        continue\n",
    "\n",
    "    print(f\"Not Matched: {m2f_param[0]} != {swin_param[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/notebooks/venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A': {'train': <torch.utils.data.dataloader.DataLoader object at 0x7f33710151f0>, 'val': <torch.utils.data.dataloader.DataLoader object at 0x7f3371014470>, 'test': <torch.utils.data.dataloader.DataLoader object at 0x7f33710140b0>}, 'B': {'train': <torch.utils.data.dataloader.DataLoader object at 0x7f3371015c40>, 'val': <torch.utils.data.dataloader.DataLoader object at 0x7f33710155b0>, 'test': <torch.utils.data.dataloader.DataLoader object at 0x7f3371016540>}}\n"
     ]
    }
   ],
   "source": [
    "# Helper function to load datasets\n",
    "def load_dataset(dataset_getter, data_path, domain_incremental):\n",
    "    return dataset_getter(data_path, domain_incremental=domain_incremental)\n",
    "\n",
    "\n",
    "# Helper function to create dataloaders for a dataset\n",
    "def create_dataloaders(\n",
    "    dataset, batch_size, shuffle, num_workers, drop_last, pin_memory, collate_fn\n",
    "):\n",
    "    return {\n",
    "        \"train\": DataLoader(\n",
    "            dataset[\"train\"],\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle,\n",
    "            num_workers=num_workers,\n",
    "            drop_last=drop_last,\n",
    "            pin_memory=pin_memory,\n",
    "            collate_fn=collate_fn,\n",
    "        ),\n",
    "        \"val\": DataLoader(\n",
    "            dataset[\"val\"],\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle,\n",
    "            num_workers=num_workers,\n",
    "            drop_last=drop_last,\n",
    "            pin_memory=pin_memory,\n",
    "            collate_fn=collate_fn,\n",
    "        ),\n",
    "        \"test\": DataLoader(\n",
    "            dataset[\"test\"],\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle,\n",
    "            num_workers=num_workers,\n",
    "            drop_last=drop_last,\n",
    "            pin_memory=pin_memory,\n",
    "            collate_fn=collate_fn,\n",
    "        ),\n",
    "    }\n",
    "\n",
    "\n",
    "# Load datasets\n",
    "datasets = {\n",
    "    \"A\": load_dataset(get_cadisv2_dataset, \"../../storage/data/CaDISv2\", True),\n",
    "    \"B\": load_dataset(get_cataract1k_dataset, \"../../storage/data/cataract-1k\", True),\n",
    "}\n",
    "# pixel_mean_A,pixel_std_A=pixel_mean_std(datasets[\"A\"][0])\n",
    "pixel_mean_A = CADIS_PIXEL_MEAN\n",
    "pixel_std_A = CADIS_PIXEL_STD\n",
    "\n",
    "# This time define the B train dataset such that it replays all the training samples from A\n",
    "new_train = torch.utils.data.ConcatDataset([datasets[\"A\"][0], datasets[\"B\"][0]])\n",
    "\n",
    "# pixel_mean_B,pixel_std_B=pixel_mean_std(new_train)\n",
    "pixel_mean_B = FULL_MERGE_PIXEL_MEAN\n",
    "pixel_std_B = FULL_MERGE_PIXEL_STD\n",
    "\n",
    "datasets[\"B\"] = (new_train, datasets[\"B\"][1], datasets[\"B\"][2])\n",
    "\n",
    "# Define preprocessor\n",
    "swin_processor = AutoImageProcessor.from_pretrained(SWIN_BACKBONE)\n",
    "m2f_preprocessor_A = Mask2FormerImageProcessor(\n",
    "    reduce_labels=True,\n",
    "    ignore_index=255,\n",
    "    do_resize=False,\n",
    "    do_rescale=True,\n",
    "    do_normalize=True,\n",
    "    image_std=pixel_std_A,\n",
    "    image_mean=pixel_mean_A,\n",
    ")\n",
    "\n",
    "m2f_preprocessor_B = Mask2FormerImageProcessor(\n",
    "    reduce_labels=True,\n",
    "    ignore_index=255,\n",
    "    do_resize=False,\n",
    "    do_rescale=True,\n",
    "    do_normalize=True,\n",
    "    image_std=pixel_std_B,\n",
    "    image_mean=pixel_mean_B,\n",
    ")\n",
    "\n",
    "# Create Mask2Former Datasets\n",
    "\n",
    "m2f_datasets = {\n",
    "    \"A\": {\n",
    "        \"train\": Mask2FormerDataset(datasets[\"A\"][0], m2f_preprocessor_A),\n",
    "        \"val\": Mask2FormerDataset(datasets[\"A\"][1], m2f_preprocessor_A),\n",
    "        \"test\": Mask2FormerDataset(datasets[\"A\"][2], m2f_preprocessor_A),\n",
    "    },\n",
    "    \"B\": {\n",
    "        \"train\": Mask2FormerDataset(datasets[\"B\"][0], m2f_preprocessor_B),\n",
    "        \"val\": Mask2FormerDataset(datasets[\"B\"][1], m2f_preprocessor_B),\n",
    "        \"test\": Mask2FormerDataset(datasets[\"B\"][2], m2f_preprocessor_B),\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "# DataLoader parameters\n",
    "N_WORKERS = 4\n",
    "BATCH_SIZE = 16\n",
    "SHUFFLE = True\n",
    "DROP_LAST = True\n",
    "\n",
    "dataloader_params = {\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"shuffle\": SHUFFLE,\n",
    "    \"num_workers\": N_WORKERS,\n",
    "    \"drop_last\": DROP_LAST,\n",
    "    \"pin_memory\": True,\n",
    "    \"collate_fn\": m2f_dataset_collate,\n",
    "}\n",
    "\n",
    "# Create DataLoaders\n",
    "dataloaders = {\n",
    "    key: create_dataloaders(m2f_datasets[key], **dataloader_params)\n",
    "    for key in m2f_datasets\n",
    "}\n",
    "\n",
    "print(dataloaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  4, 10, 11], dtype=torch.int32)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.unique(datasets[\"A\"][0][0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[tensor([ 3,  9, 10])]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m2f_datasets[\"A\"][\"train\"][0][\"class_labels\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA is available, otherwise use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensorboard extension is already loaded. To reload it, use:\n",
      "  %reload_ext tensorboard\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 1235), started 2:51:18 ago. (Use '!kill 1235' to kill it.)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-bdd640fb06671ad1\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-bdd640fb06671ad1\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tensorboard setup\n",
    "out_dir=\"outputs/\"\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "if not os.path.exists(out_dir+\"runs\"):\n",
    "    os.makedirs(out_dir+\"runs\")\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir outputs/runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!CUDA_LAUNCH_BLOCKING=1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# First train on dataset A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 100%|██████████| 12.9k/12.9k [00:00<00:00, 17.2MB/s]\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "NUM_EPOCHS = 200\n",
    "LEARNING_RATE = 1e-4\n",
    "LR_MULTIPLIER = 0.1\n",
    "BACKBONE_LR = LEARNING_RATE * LR_MULTIPLIER\n",
    "WEIGHT_DECAY = 0.05\n",
    "PATIENCE=15\n",
    "metric = evaluate.load(\"mean_iou\") # mIoU will be used to pick the best performing model using val set\n",
    "encoder_params = [\n",
    "    param\n",
    "    for name, param in model.named_parameters()\n",
    "    if name.startswith(\"model.pixel_level_module.encoder\")\n",
    "]\n",
    "decoder_params = [\n",
    "    param\n",
    "    for name, param in model.named_parameters()\n",
    "    if name.startswith(\"model.pixel_level_module.decoder\")\n",
    "]\n",
    "transformer_params = [\n",
    "    param\n",
    "    for name, param in model.named_parameters()\n",
    "    if name.startswith(\"model.transformer_module\")\n",
    "]\n",
    "optimizer = optim.AdamW(\n",
    "    [\n",
    "        {\"params\": encoder_params, \"lr\": BACKBONE_LR},\n",
    "        {\"params\": decoder_params},\n",
    "        {\"params\": transformer_params},\n",
    "    ],\n",
    "    lr=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    ")\n",
    "\n",
    "scheduler = optim.lr_scheduler.PolynomialLR(\n",
    "    optimizer, total_iters=NUM_EPOCHS, power=0.9\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "  ········\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# WandB for team usage !!!!\n",
    "\n",
    "wandb.login() # use this one if a different person is going to run the notebook\n",
    "#wandb.login(relogin=False) # if the same person in the last run is going to run the notebook again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mge85ket\u001b[0m (\u001b[33mcontinual-learning-tum\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/notebooks/continual-learning/wandb/run-20240522_040650-sy8i0ag3</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/continual-learning-tum/M2F_original/runs/sy8i0ag3' target=\"_blank\">M2F-Swin-Tiny-Train_Cadis</a></strong> to <a href='https://wandb.ai/continual-learning-tum/M2F_original' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/continual-learning-tum/M2F_original' target=\"_blank\">https://wandb.ai/continual-learning-tum/M2F_original</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/continual-learning-tum/M2F_original/runs/sy8i0ag3' target=\"_blank\">https://wandb.ai/continual-learning-tum/M2F_original/runs/sy8i0ag3</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/continual-learning-tum/M2F_original/runs/sy8i0ag3?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7ff5f8708a10>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(\n",
    "    project=\"M2F_original\",\n",
    "    config={\n",
    "        \"learning_rate\": LEARNING_RATE,\n",
    "        \"learning_rate_multiplier\": LR_MULTIPLIER,\n",
    "        \"backbone_learning_rate\": BACKBONE_LR,\n",
    "        \"learning_rate_scheduler\": scheduler.__class__.__name__,\n",
    "        \"optimizer\": optimizer.__class__.__name__,\n",
    "        \"backbone\": SWIN_BACKBONE,\n",
    "        \"m2f_preprocessor\": m2f_preprocessor_A.__dict__,\n",
    "        \"m2f_model_config\": model.config\n",
    "    },\n",
    "    name=\"M2F-Swin-Tiny-Train_Cadis\",\n",
    "    notes=\"M2F with tiny Swin backbone pretrained on ImageNet-1K. \\\n",
    "        Scenario: Train on A, Test on A\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorboard logging\n",
    "writer = SummaryWriter(log_dir=out_dir + \"runs\")\n",
    "\n",
    "# Model checkpointing\n",
    "base_model_name=\"m2f_swin_backbone_train_cadis\"\n",
    "model_dir = out_dir + \"models/\"\n",
    "if not os.path.exists(model_dir):\n",
    "    print(\"Store weights in: \", model_dir)\n",
    "    os.makedirs(model_dir)\n",
    "\n",
    "best_model_dir = model_dir + f\"{base_model_name}/best_model/\"\n",
    "if not os.path.exists(best_model_dir):\n",
    "    print(\"Store best model weights in: \", best_model_dir)\n",
    "    os.makedirs(best_model_dir)\n",
    "final_model_dir = model_dir + f\"{base_model_name}/final_model/\"\n",
    "if not os.path.exists(final_model_dir):\n",
    "    print(\"Store final model weights in: \", final_model_dir)\n",
    "    os.makedirs(final_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['outputs/models/m2f_swin_backbone_train_cadis/preprocessor_config.json']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the preprocessor\n",
    "m2f_preprocessor_A.save_pretrained(model_dir + base_model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/200 Training: 100%|██████████| 221/221 [07:11<00:00,  1.95s/it, loss=401.1506] \n",
      "Epoch 1/200 Validation: 100%|██████████| 33/33 [00:44<00:00,  1.34s/it, loss=483.8031]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200, Train Loss: 59.9673, Train mIoU: 0.1127, Validation Loss: 28.1699, Validation mIoU: 0.1739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/200 Training: 100%|██████████| 221/221 [07:05<00:00,  1.92s/it, loss=299.5568]\n",
      "Epoch 2/200 Validation: 100%|██████████| 33/33 [00:40<00:00,  1.21s/it, loss=255.4191]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/200, Train Loss: 21.9250, Train mIoU: 0.3603, Validation Loss: 18.8730, Validation mIoU: 0.5495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/200 Training: 100%|██████████| 221/221 [07:07<00:00,  1.93s/it, loss=223.5976]\n",
      "Epoch 3/200 Validation: 100%|██████████| 33/33 [00:38<00:00,  1.17s/it, loss=507.0912]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/200, Train Loss: 16.0550, Train mIoU: 0.6059, Validation Loss: 16.0244, Validation mIoU: 0.6703\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/200 Training: 100%|██████████| 221/221 [07:06<00:00,  1.93s/it, loss=225.2377]\n",
      "Epoch 4/200 Validation: 100%|██████████| 33/33 [00:42<00:00,  1.28s/it, loss=211.8082]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/200, Train Loss: 13.6189, Train mIoU: 0.6810, Validation Loss: 14.8552, Validation mIoU: 0.6770\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/200 Training: 100%|██████████| 221/221 [07:09<00:00,  1.94s/it, loss=185.5254]\n",
      "Epoch 5/200 Validation: 100%|██████████| 33/33 [00:41<00:00,  1.26s/it, loss=200.0621]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/200, Train Loss: 12.2627, Train mIoU: 0.7518, Validation Loss: 14.0428, Validation mIoU: 0.7407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/200 Training: 100%|██████████| 221/221 [07:07<00:00,  1.93s/it, loss=174.2913]\n",
      "Epoch 6/200 Validation: 100%|██████████| 33/33 [00:39<00:00,  1.20s/it, loss=241.7548]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/200, Train Loss: 11.0515, Train mIoU: 0.8090, Validation Loss: 13.5142, Validation mIoU: 0.7556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/200 Training: 100%|██████████| 221/221 [07:07<00:00,  1.93s/it, loss=172.0493]\n",
      "Epoch 7/200 Validation: 100%|██████████| 33/33 [00:39<00:00,  1.19s/it, loss=180.1881]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/200, Train Loss: 10.2347, Train mIoU: 0.8442, Validation Loss: 13.0086, Validation mIoU: 0.7746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/200 Training: 100%|██████████| 221/221 [07:09<00:00,  1.94s/it, loss=178.9977]\n",
      "Epoch 8/200 Validation: 100%|██████████| 33/33 [00:38<00:00,  1.16s/it, loss=202.4963]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/200, Train Loss: 9.7156, Train mIoU: 0.8640, Validation Loss: 12.8888, Validation mIoU: 0.8009\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/200 Training: 100%|██████████| 221/221 [07:03<00:00,  1.92s/it, loss=150.3341]\n",
      "Epoch 9/200 Validation: 100%|██████████| 33/33 [00:37<00:00,  1.15s/it, loss=182.8118]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/200, Train Loss: 9.3311, Train mIoU: 0.8710, Validation Loss: 13.0399, Validation mIoU: 0.7448\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/200 Training: 100%|██████████| 221/221 [07:06<00:00,  1.93s/it, loss=144.6579]\n",
      "Epoch 10/200 Validation: 100%|██████████| 33/33 [00:42<00:00,  1.28s/it, loss=157.7974]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/200, Train Loss: 8.9172, Train mIoU: 0.8876, Validation Loss: 12.7428, Validation mIoU: 0.8042\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/200 Training: 100%|██████████| 221/221 [07:06<00:00,  1.93s/it, loss=126.5559]\n",
      "Epoch 11/200 Validation: 100%|██████████| 33/33 [00:39<00:00,  1.20s/it, loss=141.9452]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/200, Train Loss: 8.5717, Train mIoU: 0.8973, Validation Loss: 12.8770, Validation mIoU: 0.8107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/200 Training: 100%|██████████| 221/221 [07:12<00:00,  1.96s/it, loss=125.8806]\n",
      "Epoch 12/200 Validation: 100%|██████████| 33/33 [00:39<00:00,  1.21s/it, loss=343.2729]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/200, Train Loss: 8.3852, Train mIoU: 0.9048, Validation Loss: 12.3538, Validation mIoU: 0.7602\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/200 Training: 100%|██████████| 221/221 [07:12<00:00,  1.96s/it, loss=113.7843]\n",
      "Epoch 13/200 Validation: 100%|██████████| 33/33 [00:40<00:00,  1.21s/it, loss=140.8690]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/200, Train Loss: 8.0266, Train mIoU: 0.9076, Validation Loss: 12.3803, Validation mIoU: 0.8098\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/200 Training: 100%|██████████| 221/221 [07:07<00:00,  1.93s/it, loss=106.1232]\n",
      "Epoch 14/200 Validation: 100%|██████████| 33/33 [00:42<00:00,  1.29s/it, loss=141.3668]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/200, Train Loss: 7.8349, Train mIoU: 0.9120, Validation Loss: 12.5519, Validation mIoU: 0.7572\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/200 Training: 100%|██████████| 221/221 [07:04<00:00,  1.92s/it, loss=127.3804]\n",
      "Epoch 15/200 Validation: 100%|██████████| 33/33 [00:40<00:00,  1.23s/it, loss=162.7226]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/200, Train Loss: 7.5868, Train mIoU: 0.9169, Validation Loss: 12.4464, Validation mIoU: 0.7668\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/200 Training: 100%|██████████| 221/221 [07:03<00:00,  1.92s/it, loss=109.6527]\n",
      "Epoch 16/200 Validation: 100%|██████████| 33/33 [00:40<00:00,  1.23s/it, loss=168.0958]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/200, Train Loss: 7.4564, Train mIoU: 0.9121, Validation Loss: 12.5355, Validation mIoU: 0.8312\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/200 Training: 100%|██████████| 221/221 [07:13<00:00,  1.96s/it, loss=122.7549]\n",
      "Epoch 17/200 Validation: 100%|██████████| 33/33 [00:40<00:00,  1.23s/it, loss=217.2358]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/200, Train Loss: 7.3812, Train mIoU: 0.9274, Validation Loss: 12.6641, Validation mIoU: 0.7958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/200 Training: 100%|██████████| 221/221 [07:11<00:00,  1.95s/it, loss=113.9071]\n",
      "Epoch 18/200 Validation: 100%|██████████| 33/33 [00:39<00:00,  1.19s/it, loss=212.6597]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/200, Train Loss: 7.2872, Train mIoU: 0.9248, Validation Loss: 12.2307, Validation mIoU: 0.7793\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/200 Training: 100%|██████████| 221/221 [07:07<00:00,  1.93s/it, loss=113.8650]\n",
      "Epoch 19/200 Validation: 100%|██████████| 33/33 [00:41<00:00,  1.26s/it, loss=213.1660]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/200, Train Loss: 6.9397, Train mIoU: 0.9283, Validation Loss: 13.0369, Validation mIoU: 0.7093\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/200 Training: 100%|██████████| 221/221 [07:09<00:00,  1.94s/it, loss=107.2071]\n",
      "Epoch 20/200 Validation: 100%|██████████| 33/33 [00:41<00:00,  1.25s/it, loss=157.6948]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/200, Train Loss: 6.8617, Train mIoU: 0.9328, Validation Loss: 12.7653, Validation mIoU: 0.7682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/200 Training: 100%|██████████| 221/221 [07:06<00:00,  1.93s/it, loss=112.2943]\n",
      "Epoch 21/200 Validation: 100%|██████████| 33/33 [00:41<00:00,  1.25s/it, loss=177.6266]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/200, Train Loss: 6.6442, Train mIoU: 0.9404, Validation Loss: 12.9168, Validation mIoU: 0.7992\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/200 Training: 100%|██████████| 221/221 [07:06<00:00,  1.93s/it, loss=114.5106]\n",
      "Epoch 22/200 Validation: 100%|██████████| 33/33 [00:42<00:00,  1.29s/it, loss=170.0397]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/200, Train Loss: 6.4627, Train mIoU: 0.9407, Validation Loss: 12.7133, Validation mIoU: 0.7706\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/200 Training: 100%|██████████| 221/221 [07:11<00:00,  1.95s/it, loss=104.8699]\n",
      "Epoch 23/200 Validation: 100%|██████████| 33/33 [00:41<00:00,  1.26s/it, loss=187.9654]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/200, Train Loss: 6.6558, Train mIoU: 0.9238, Validation Loss: 13.4670, Validation mIoU: 0.7338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/200 Training: 100%|██████████| 221/221 [07:12<00:00,  1.96s/it, loss=112.5719]\n",
      "Epoch 24/200 Validation: 100%|██████████| 33/33 [00:42<00:00,  1.30s/it, loss=200.1590]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/200, Train Loss: 6.3335, Train mIoU: 0.9302, Validation Loss: 12.6056, Validation mIoU: 0.8208\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/200 Training: 100%|██████████| 221/221 [07:13<00:00,  1.96s/it, loss=95.5995] \n",
      "Epoch 25/200 Validation: 100%|██████████| 33/33 [00:42<00:00,  1.30s/it, loss=265.8563]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/200, Train Loss: 6.1377, Train mIoU: 0.9454, Validation Loss: 12.9725, Validation mIoU: 0.8183\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/200 Training: 100%|██████████| 221/221 [07:10<00:00,  1.95s/it, loss=85.4450] \n",
      "Epoch 26/200 Validation: 100%|██████████| 33/33 [00:42<00:00,  1.28s/it, loss=274.1076]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/200, Train Loss: 6.0130, Train mIoU: 0.9459, Validation Loss: 13.2565, Validation mIoU: 0.7970\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/200 Training: 100%|██████████| 221/221 [07:10<00:00,  1.95s/it, loss=92.0950] \n",
      "Epoch 27/200 Validation: 100%|██████████| 33/33 [00:43<00:00,  1.31s/it, loss=391.9355]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/200, Train Loss: 5.9057, Train mIoU: 0.9454, Validation Loss: 12.8361, Validation mIoU: 0.8126\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/200 Training: 100%|██████████| 221/221 [07:14<00:00,  1.97s/it, loss=107.4168]\n",
      "Epoch 28/200 Validation: 100%|██████████| 33/33 [00:42<00:00,  1.30s/it, loss=199.8778]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/200, Train Loss: 5.7176, Train mIoU: 0.9492, Validation Loss: 13.3272, Validation mIoU: 0.8082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/200 Training: 100%|██████████| 221/221 [07:14<00:00,  1.96s/it, loss=92.2878] \n",
      "Epoch 29/200 Validation: 100%|██████████| 33/33 [00:43<00:00,  1.32s/it, loss=159.5150]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/200, Train Loss: 5.7386, Train mIoU: 0.9483, Validation Loss: 12.7490, Validation mIoU: 0.7946\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/200 Training: 100%|██████████| 221/221 [07:15<00:00,  1.97s/it, loss=93.8445] \n",
      "Epoch 30/200 Validation: 100%|██████████| 33/33 [00:42<00:00,  1.29s/it, loss=161.5087]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/200, Train Loss: 5.7040, Train mIoU: 0.9445, Validation Loss: 13.0323, Validation mIoU: 0.8008\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/200 Training: 100%|██████████| 221/221 [07:09<00:00,  1.94s/it, loss=88.7041] \n",
      "Epoch 31/200 Validation: 100%|██████████| 33/33 [00:42<00:00,  1.28s/it, loss=168.6165]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/200, Train Loss: 5.5612, Train mIoU: 0.9444, Validation Loss: 13.1058, Validation mIoU: 0.8114\n",
      "Early stopping at epoch 30\n"
     ]
    }
   ],
   "source": [
    "# To avoid making stupid errors\n",
    "CURR_TASK = \"A\"\n",
    "\n",
    "# For storing the model\n",
    "best_val_metric = -np.inf\n",
    "\n",
    "# Move model to device\n",
    "model.to(device)\n",
    "counter=0\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    train_running_loss = 0.0\n",
    "    val_running_loss = 0.0\n",
    "\n",
    "    # Set up tqdm for the training loop\n",
    "    train_loader = tqdm(\n",
    "        dataloaders[CURR_TASK][\"train\"], desc=f\"Epoch {epoch + 1}/{NUM_EPOCHS} Training\"\n",
    "    )\n",
    "\n",
    "    for batch in train_loader:\n",
    "        # Move everything to the device\n",
    "        batch[\"pixel_values\"] = batch[\"pixel_values\"].to(device)\n",
    "        batch[\"pixel_mask\"] = batch[\"pixel_mask\"].to(device)\n",
    "        batch[\"mask_labels\"] = [entry.to(device) for entry in batch[\"mask_labels\"]]\n",
    "        batch[\"class_labels\"] = [entry.to(device) for entry in batch[\"class_labels\"]]\n",
    "       \n",
    "        # Compute output and loss\n",
    "        outputs = model(**batch)\n",
    "\n",
    "        loss = outputs.loss\n",
    "        # Compute gradient and perform step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Record losses\n",
    "        current_loss = loss.item() * batch[\"pixel_values\"].size(0)\n",
    "        train_running_loss += current_loss\n",
    "        train_loader.set_postfix(loss=f\"{current_loss:.4f}\")\n",
    "\n",
    "        # Extract and compute metrics\n",
    "        pred_maps, masks = m2f_extract_pred_maps_and_masks(\n",
    "            batch, outputs, m2f_preprocessor_A\n",
    "        )\n",
    "        metric.add_batch(references=masks, predictions=pred_maps)\n",
    "\n",
    "    # After compute the batches that were added are deleted\n",
    "    temp_metrics = metric.compute(\n",
    "        num_labels=NUM_CLASSES, ignore_index=BG_VALUE_255, reduce_labels=False\n",
    "    )\n",
    "    mean_train_iou=temp_metrics[\"mean_iou\"]\n",
    "        \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loader = tqdm(\n",
    "        dataloaders[CURR_TASK][\"val\"], desc=f\"Epoch {epoch + 1}/{NUM_EPOCHS} Validation\"\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            # Move everything to the device\n",
    "            batch[\"pixel_values\"] = batch[\"pixel_values\"].to(device)\n",
    "            batch[\"pixel_mask\"] = batch[\"pixel_mask\"].to(device)\n",
    "            batch[\"mask_labels\"] = [entry.to(device) for entry in batch[\"mask_labels\"]]\n",
    "            batch[\"class_labels\"] = [\n",
    "                entry.to(device) for entry in batch[\"class_labels\"]\n",
    "            ]\n",
    "            # Compute output and loss\n",
    "            outputs = model(**batch)\n",
    "\n",
    "            loss = outputs.loss\n",
    "            # Record losses\n",
    "            current_loss = loss.item() * batch[\"pixel_values\"].size(0)\n",
    "            val_running_loss += current_loss\n",
    "            val_loader.set_postfix(loss=f\"{current_loss:.4f}\")\n",
    "\n",
    "            # Extract and compute metrics\n",
    "            pred_maps, masks = m2f_extract_pred_maps_and_masks(\n",
    "                batch, outputs, m2f_preprocessor_A\n",
    "            )\n",
    "            metric.add_batch(references=masks, predictions=pred_maps)\n",
    "            \n",
    "\n",
    "    # After compute the batches that were added are deleted\n",
    "    mean_val_iou = metric.compute(\n",
    "        num_labels=NUM_CLASSES, ignore_index=BG_VALUE_255, reduce_labels=False\n",
    "    )[\"mean_iou\"]\n",
    "\n",
    "    epoch_train_loss = train_running_loss / len(dataloaders[CURR_TASK][\"train\"].dataset)\n",
    "    epoch_val_loss = val_running_loss / len(dataloaders[CURR_TASK][\"val\"].dataset)\n",
    "\n",
    "    writer.add_scalar(f\"Loss/train_{base_model_name}_{CURR_TASK}\", epoch_train_loss, epoch + 1)\n",
    "    writer.add_scalar(f\"Loss/val_{base_model_name}_{CURR_TASK}\", epoch_val_loss, epoch + 1)\n",
    "    writer.add_scalar(f\"mIoU/train_{base_model_name}_{CURR_TASK}\", mean_train_iou, epoch + 1)\n",
    "    writer.add_scalar(f\"mIoU/val_{base_model_name}_{CURR_TASK}\", mean_val_iou, epoch + 1)\n",
    "\n",
    "    wandb.log({\n",
    "        f\"Loss/train_{CURR_TASK}\": epoch_train_loss,\n",
    "        f\"Loss/val_{CURR_TASK}\": epoch_val_loss,\n",
    "        f\"mIoU/train_{CURR_TASK}\": mean_train_iou,\n",
    "        f\"mIoU/val_{CURR_TASK}\": mean_val_iou\n",
    "    })\n",
    "\n",
    "\n",
    "    tqdm.write(\n",
    "        f\"Epoch {epoch + 1}/{NUM_EPOCHS}, Train Loss: {epoch_train_loss:.4f}, Train mIoU: {mean_train_iou:.4f}, Validation Loss: {epoch_val_loss:.4f}, Validation mIoU: {mean_val_iou:.4f}\"\n",
    "    )\n",
    "    \n",
    "    if mean_val_iou > best_val_metric:\n",
    "        best_val_metric = mean_val_iou\n",
    "        model.save_pretrained(f\"{best_model_dir}{CURR_TASK}/\")\n",
    "        counter=0\n",
    "    else:\n",
    "        counter+=1\n",
    "        if counter == PATIENCE:\n",
    "            print(\"Early stopping at epoch\",epoch)\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Test results on A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Load best model and evaluate on test\n",
    "#model = Mask2FormerForUniversalSegmentation.from_pretrained(f\"{best_model_dir}{CURR_TASK}/\").to(device)\n",
    "\n",
    "# Load pretrained on Cadis from naive forgetting \n",
    "model = Mask2FormerForUniversalSegmentation.from_pretrained(f\"/notebooks/continual-learning/outputs/models/{base_model_name}/best_model/A\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test loop: 100%|██████████| 36/36 [00:58<00:00,  1.63s/it, loss=318.6559]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 14.8668, Test mIoU: 0.7716\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "test_running_loss = 0\n",
    "test_loader = tqdm(dataloaders[CURR_TASK][\"test\"], desc=\"Test loop\")\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        # Move everything to the device\n",
    "        batch[\"pixel_values\"] = batch[\"pixel_values\"].to(device)\n",
    "        batch[\"pixel_mask\"] = batch[\"pixel_mask\"].to(device)\n",
    "        batch[\"mask_labels\"] = [entry.to(device) for entry in batch[\"mask_labels\"]]\n",
    "        batch[\"class_labels\"] = [entry.to(device) for entry in batch[\"class_labels\"]]\n",
    "        # Compute output and loss\n",
    "        outputs = model(**batch)\n",
    "\n",
    "        loss = outputs.loss\n",
    "        # Record losses\n",
    "        current_loss = loss.item() * batch[\"pixel_values\"].size(0)\n",
    "        test_running_loss += current_loss\n",
    "        test_loader.set_postfix(loss=f\"{current_loss:.4f}\")\n",
    "\n",
    "        # Extract and compute metrics\n",
    "        pred_maps, masks = m2f_extract_pred_maps_and_masks(\n",
    "            batch, outputs, m2f_preprocessor_A\n",
    "        )\n",
    "        metric.add_batch(references=masks, predictions=pred_maps)\n",
    "        \n",
    "# After compute the batches that were added are deleted\n",
    "test_metrics_A = metric.compute(\n",
    "    num_labels=NUM_CLASSES, ignore_index=BG_VALUE_255, reduce_labels=False\n",
    ")\n",
    "mean_test_iou = test_metrics_A[\"mean_iou\"]\n",
    "final_test_loss = test_running_loss / len(dataloaders[CURR_TASK][\"test\"].dataset)\n",
    "wandb.log({\n",
    "    f\"Loss/test_{CURR_TASK}\": final_test_loss,\n",
    "    f\"mIoU/test_{CURR_TASK}\": mean_test_iou\n",
    "})\n",
    "print(f\"Test Loss: {final_test_loss:.4f}, Test mIoU: {mean_test_iou:.4f}\")\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'mean_iou': 0.7735610940366814,\n",
       " 'mean_accuracy': 0.8364536793631839,\n",
       " 'overall_accuracy': 0.9621300366527966,\n",
       " 'per_category_iou': array([0.96072231, 0.93562797, 0.64369086, 0.39402199, 0.4091044 ,\n",
       "        0.81320499, 0.81338487, 0.76786534, 0.89548438, 0.94261244,\n",
       "        0.93345249]),\n",
       " 'per_category_accuracy': array([0.97414412, 0.98914931, 0.81667993, 0.43158069, 0.42852069,\n",
       "        0.91832696, 0.89176254, 0.86807917, 0.94488664, 0.96806242,\n",
       "        0.969798  ])}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#previous run\n",
    "test_metrics_A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Now train on B and forget A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Training\n",
    "NUM_EPOCHS = 200\n",
    "LEARNING_RATE = 1e-4\n",
    "LR_MULTIPLIER = 0.1\n",
    "BACKBONE_LR = LEARNING_RATE * LR_MULTIPLIER\n",
    "WEIGHT_DECAY = 0.05\n",
    "PATIENCE=15\n",
    "encoder_params = [\n",
    "    param\n",
    "    for name, param in model.named_parameters()\n",
    "    if name.startswith(\"model.pixel_level_module.encoder\")\n",
    "]\n",
    "decoder_params = [\n",
    "    param\n",
    "    for name, param in model.named_parameters()\n",
    "    if name.startswith(\"model.pixel_level_module.decoder\")\n",
    "]\n",
    "transformer_params = [\n",
    "    param\n",
    "    for name, param in model.named_parameters()\n",
    "    if name.startswith(\"model.transformer_module\")\n",
    "]\n",
    "optimizer = optim.AdamW(\n",
    "    [\n",
    "        {\"params\": encoder_params, \"lr\": BACKBONE_LR},\n",
    "        {\"params\": decoder_params},\n",
    "        {\"params\": transformer_params},\n",
    "    ],\n",
    "    lr=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    ")\n",
    "\n",
    "scheduler = optim.lr_scheduler.PolynomialLR(\n",
    "    optimizer, total_iters=NUM_EPOCHS, power=0.9\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# WandB for team usage !!!!\n",
    "\n",
    "wandb.login() # use this one if a different person is going to run the notebook\n",
    "#wandb.login(relogin=False) # if the same person in the last run is going to run the notebook again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(\n",
    "    project=\"M2F_original\",\n",
    "    config={\n",
    "        \"learning_rate\": LEARNING_RATE,\n",
    "        \"learning_rate_multiplier\": LR_MULTIPLIER,\n",
    "        \"backbone_learning_rate\": BACKBONE_LR,\n",
    "        \"learning_rate_scheduler\": scheduler.__class__.__name__,\n",
    "        \"optimizer\": optimizer.__class__.__name__,\n",
    "        \"backbone\": SWIN_BACKBONE,\n",
    "        \"m2f_preprocessor\": m2f_preprocessor_B.__dict__,\n",
    "        \"m2f_model_config\": model.config\n",
    "    },\n",
    "    name=\"M2F-Swin-Tiny-Replay-All\",\n",
    "    notes=\"M2F with tiny Swin backbone pretrained on ImageNet-1K. \\\n",
    "        Scenario: Pretrained on A, Train on A + B naive finetuning (replay all), Test forgetting on A\"\n",
    ")\n",
    "\n",
    "# Tensorboard logging\n",
    "writer = SummaryWriter(log_dir=out_dir + \"runs\")\n",
    "\n",
    "# Model checkpointing\n",
    "model_name = \"m2f_swin_backbone_replay_all\"\n",
    "model_dir = out_dir + \"models/\"\n",
    "if not os.path.exists(model_dir):\n",
    "    print(\"Store weights in: \", model_dir)\n",
    "    os.makedirs(model_dir)\n",
    "\n",
    "best_model_dir = model_dir + f\"{model_name}/best_model/\"\n",
    "if not os.path.exists(best_model_dir):\n",
    "    print(\"Store best model weights in: \", best_model_dir)\n",
    "    os.makedirs(best_model_dir)\n",
    "final_model_dir = model_dir + f\"{model_name}/final_model/\"\n",
    "if not os.path.exists(final_model_dir):\n",
    "    print(\"Store final model weights in: \", final_model_dir)\n",
    "    os.makedirs(final_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['outputs/models/m2f_swin_backbone_replay_all/preprocessor_config.json']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the preprocessor\n",
    "m2f_preprocessor_B.save_pretrained(model_dir + model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/200 Training: 100%|██████████| 334/334 [10:53<00:00,  1.96s/it, loss=159.9668]\n",
      "Epoch 1/200 Validation: 100%|██████████| 14/14 [00:22<00:00,  1.62s/it, loss=174.6148]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200, Train Loss: 12.2269, Train mIoU: 0.7891, Validation Loss: 12.5346, Validation mIoU: 0.5552\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/200 Training: 100%|██████████| 334/334 [11:06<00:00,  1.99s/it, loss=148.3775]\n",
      "Epoch 2/200 Validation: 100%|██████████| 14/14 [00:19<00:00,  1.43s/it, loss=151.7886]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/200, Train Loss: 9.4902, Train mIoU: 0.8563, Validation Loss: 10.8903, Validation mIoU: 0.6870\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/200 Training: 100%|██████████| 334/334 [10:49<00:00,  1.94s/it, loss=136.3703]\n",
      "Epoch 3/200 Validation: 100%|██████████| 14/14 [00:20<00:00,  1.45s/it, loss=148.0401]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/200, Train Loss: 8.7525, Train mIoU: 0.8820, Validation Loss: 9.7021, Validation mIoU: 0.7947\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/200 Training: 100%|██████████| 334/334 [10:55<00:00,  1.96s/it, loss=180.6022]\n",
      "Epoch 4/200 Validation: 100%|██████████| 14/14 [00:18<00:00,  1.35s/it, loss=156.6559]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/200, Train Loss: 8.3104, Train mIoU: 0.8840, Validation Loss: 9.5877, Validation mIoU: 0.7631\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/200 Training: 100%|██████████| 334/334 [10:57<00:00,  1.97s/it, loss=123.5997]\n",
      "Epoch 5/200 Validation: 100%|██████████| 14/14 [00:22<00:00,  1.58s/it, loss=169.5353]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/200, Train Loss: 7.8368, Train mIoU: 0.9126, Validation Loss: 9.2520, Validation mIoU: 0.7334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/200 Training: 100%|██████████| 334/334 [10:52<00:00,  1.96s/it, loss=136.4776]\n",
      "Epoch 6/200 Validation: 100%|██████████| 14/14 [00:20<00:00,  1.49s/it, loss=112.9756]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/200, Train Loss: 7.5730, Train mIoU: 0.9168, Validation Loss: 8.5893, Validation mIoU: 0.7919\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/200 Training: 100%|██████████| 334/334 [10:42<00:00,  1.92s/it, loss=125.7431]\n",
      "Epoch 7/200 Validation: 100%|██████████| 14/14 [00:21<00:00,  1.54s/it, loss=120.9915]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/200, Train Loss: 7.2968, Train mIoU: 0.9233, Validation Loss: 8.3832, Validation mIoU: 0.7757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/200 Training: 100%|██████████| 334/334 [10:55<00:00,  1.96s/it, loss=102.1801]\n",
      "Epoch 8/200 Validation: 100%|██████████| 14/14 [00:19<00:00,  1.41s/it, loss=107.4177]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/200, Train Loss: 6.9509, Train mIoU: 0.9302, Validation Loss: 8.3375, Validation mIoU: 0.8082\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/200 Training: 100%|██████████| 334/334 [10:52<00:00,  1.96s/it, loss=85.7844] \n",
      "Epoch 9/200 Validation: 100%|██████████| 14/14 [00:19<00:00,  1.42s/it, loss=107.7070]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/200, Train Loss: 6.7584, Train mIoU: 0.9341, Validation Loss: 7.7436, Validation mIoU: 0.8446\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/200 Training: 100%|██████████| 334/334 [10:55<00:00,  1.96s/it, loss=115.0919]\n",
      "Epoch 10/200 Validation: 100%|██████████| 14/14 [00:18<00:00,  1.35s/it, loss=124.5227]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/200, Train Loss: 6.5894, Train mIoU: 0.9409, Validation Loss: 7.9311, Validation mIoU: 0.8064\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/200 Training: 100%|██████████| 334/334 [10:45<00:00,  1.93s/it, loss=116.3852]\n",
      "Epoch 11/200 Validation: 100%|██████████| 14/14 [00:19<00:00,  1.36s/it, loss=109.3057]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/200, Train Loss: 6.4495, Train mIoU: 0.9401, Validation Loss: 8.6182, Validation mIoU: 0.7538\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/200 Training: 100%|██████████| 334/334 [10:56<00:00,  1.97s/it, loss=97.6583] \n",
      "Epoch 12/200 Validation: 100%|██████████| 14/14 [00:20<00:00,  1.44s/it, loss=189.7944]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/200, Train Loss: 6.3265, Train mIoU: 0.9402, Validation Loss: 8.0202, Validation mIoU: 0.8305\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/200 Training: 100%|██████████| 334/334 [10:54<00:00,  1.96s/it, loss=83.9409] \n",
      "Epoch 13/200 Validation: 100%|██████████| 14/14 [00:19<00:00,  1.40s/it, loss=166.2131]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/200, Train Loss: 6.0628, Train mIoU: 0.9468, Validation Loss: 7.8697, Validation mIoU: 0.8194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/200 Training: 100%|██████████| 334/334 [10:48<00:00,  1.94s/it, loss=97.9259] \n",
      "Epoch 14/200 Validation: 100%|██████████| 14/14 [00:23<00:00,  1.71s/it, loss=105.7941]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/200, Train Loss: 6.0379, Train mIoU: 0.9405, Validation Loss: 7.9397, Validation mIoU: 0.7945\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/200 Training: 100%|██████████| 334/334 [10:51<00:00,  1.95s/it, loss=88.8487] \n",
      "Epoch 15/200 Validation: 100%|██████████| 14/14 [00:20<00:00,  1.45s/it, loss=146.2966]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/200, Train Loss: 5.9496, Train mIoU: 0.9445, Validation Loss: 7.8810, Validation mIoU: 0.8511\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/200 Training: 100%|██████████| 334/334 [10:51<00:00,  1.95s/it, loss=88.0879] \n",
      "Epoch 16/200 Validation: 100%|██████████| 14/14 [00:18<00:00,  1.35s/it, loss=116.7256]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/200, Train Loss: 6.0797, Train mIoU: 0.9330, Validation Loss: 8.2957, Validation mIoU: 0.7580\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/200 Training: 100%|██████████| 334/334 [10:51<00:00,  1.95s/it, loss=80.7473] \n",
      "Epoch 17/200 Validation: 100%|██████████| 14/14 [00:19<00:00,  1.41s/it, loss=104.4139]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/200, Train Loss: 5.8240, Train mIoU: 0.9459, Validation Loss: 7.7586, Validation mIoU: 0.7633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/200 Training: 100%|██████████| 334/334 [10:49<00:00,  1.95s/it, loss=91.8050] \n",
      "Epoch 18/200 Validation: 100%|██████████| 14/14 [00:19<00:00,  1.37s/it, loss=132.3295]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/200, Train Loss: 5.6472, Train mIoU: 0.9504, Validation Loss: 8.3063, Validation mIoU: 0.8227\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/200 Training: 100%|██████████| 334/334 [10:52<00:00,  1.95s/it, loss=83.2500] \n",
      "Epoch 19/200 Validation: 100%|██████████| 14/14 [00:19<00:00,  1.37s/it, loss=131.8750]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/200, Train Loss: 5.5010, Train mIoU: 0.9553, Validation Loss: 7.5506, Validation mIoU: 0.8362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/200 Training: 100%|██████████| 334/334 [10:55<00:00,  1.96s/it, loss=95.5137] \n",
      "Epoch 20/200 Validation: 100%|██████████| 14/14 [00:22<00:00,  1.58s/it, loss=99.1227] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/200, Train Loss: 5.4049, Train mIoU: 0.9542, Validation Loss: 7.5598, Validation mIoU: 0.8622\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/200 Training: 100%|██████████| 334/334 [10:46<00:00,  1.94s/it, loss=86.5527] \n",
      "Epoch 21/200 Validation: 100%|██████████| 14/14 [00:19<00:00,  1.38s/it, loss=114.3295]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/200, Train Loss: 5.3167, Train mIoU: 0.9564, Validation Loss: 7.7998, Validation mIoU: 0.8265\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/200 Training: 100%|██████████| 334/334 [10:53<00:00,  1.96s/it, loss=93.7683] \n",
      "Epoch 22/200 Validation: 100%|██████████| 14/14 [00:21<00:00,  1.50s/it, loss=115.7745]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/200, Train Loss: 5.3428, Train mIoU: 0.9555, Validation Loss: 8.0463, Validation mIoU: 0.8111\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/200 Training: 100%|██████████| 334/334 [10:56<00:00,  1.97s/it, loss=79.1328] \n",
      "Epoch 23/200 Validation: 100%|██████████| 14/14 [00:21<00:00,  1.52s/it, loss=109.4543]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/200, Train Loss: 5.3521, Train mIoU: 0.9469, Validation Loss: 8.3293, Validation mIoU: 0.7692\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/200 Training: 100%|██████████| 334/334 [10:50<00:00,  1.95s/it, loss=71.5348] \n",
      "Epoch 24/200 Validation: 100%|██████████| 14/14 [00:21<00:00,  1.56s/it, loss=143.2441]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/200, Train Loss: 5.4763, Train mIoU: 0.9423, Validation Loss: 7.7064, Validation mIoU: 0.8193\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/200 Training: 100%|██████████| 334/334 [10:51<00:00,  1.95s/it, loss=87.3346] \n",
      "Epoch 25/200 Validation: 100%|██████████| 14/14 [00:21<00:00,  1.51s/it, loss=126.5389]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/200, Train Loss: 5.2609, Train mIoU: 0.9572, Validation Loss: 8.1271, Validation mIoU: 0.8347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/200 Training: 100%|██████████| 334/334 [10:50<00:00,  1.95s/it, loss=81.2089] \n",
      "Epoch 26/200 Validation: 100%|██████████| 14/14 [00:20<00:00,  1.45s/it, loss=122.8888]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/200, Train Loss: 5.0475, Train mIoU: 0.9564, Validation Loss: 8.0226, Validation mIoU: 0.8325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/200 Training: 100%|██████████| 334/334 [10:53<00:00,  1.96s/it, loss=79.4973] \n",
      "Epoch 27/200 Validation: 100%|██████████| 14/14 [00:22<00:00,  1.58s/it, loss=97.8096] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/200, Train Loss: 4.8845, Train mIoU: 0.9636, Validation Loss: 7.3859, Validation mIoU: 0.7768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/200 Training: 100%|██████████| 334/334 [10:54<00:00,  1.96s/it, loss=81.7715] \n",
      "Epoch 28/200 Validation: 100%|██████████| 14/14 [00:19<00:00,  1.38s/it, loss=111.1705]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/200, Train Loss: 4.8563, Train mIoU: 0.9610, Validation Loss: 7.8150, Validation mIoU: 0.8133\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/200 Training: 100%|██████████| 334/334 [10:51<00:00,  1.95s/it, loss=84.7437] \n",
      "Epoch 29/200 Validation: 100%|██████████| 14/14 [00:20<00:00,  1.44s/it, loss=101.6447]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/200, Train Loss: 4.7794, Train mIoU: 0.9607, Validation Loss: 7.5795, Validation mIoU: 0.8144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/200 Training: 100%|██████████| 334/334 [10:54<00:00,  1.96s/it, loss=75.1428] \n",
      "Epoch 30/200 Validation: 100%|██████████| 14/14 [00:19<00:00,  1.41s/it, loss=124.0426]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/200, Train Loss: 4.7023, Train mIoU: 0.9586, Validation Loss: 8.0124, Validation mIoU: 0.8068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/200 Training: 100%|██████████| 334/334 [10:50<00:00,  1.95s/it, loss=69.8799] \n",
      "Epoch 31/200 Validation: 100%|██████████| 14/14 [00:22<00:00,  1.61s/it, loss=136.3914]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/200, Train Loss: 4.6417, Train mIoU: 0.9657, Validation Loss: 8.0394, Validation mIoU: 0.7760\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/200 Training: 100%|██████████| 334/334 [10:52<00:00,  1.95s/it, loss=85.4359] \n",
      "Epoch 32/200 Validation: 100%|██████████| 14/14 [00:21<00:00,  1.55s/it, loss=146.0571]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/200, Train Loss: 4.6763, Train mIoU: 0.9625, Validation Loss: 7.9591, Validation mIoU: 0.7853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/200 Training:  87%|████████▋ | 290/334 [09:30<01:28,  2.00s/it, loss=75.1221] "
     ]
    }
   ],
   "source": [
    "# To avoid making stupid errors\n",
    "CURR_TASK = \"B\"\n",
    "model_path_second = f\"{best_model_dir}A+{CURR_TASK}/\"\n",
    "\n",
    "\n",
    "# For storing the model\n",
    "best_val_metric = -np.inf\n",
    "\n",
    "# Move model to device\n",
    "model.to(device)\n",
    "counter=0\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    train_running_loss = 0.0\n",
    "    val_running_loss = 0.0\n",
    "\n",
    "    # Set up tqdm for the training loop\n",
    "    train_loader = tqdm(\n",
    "        dataloaders[CURR_TASK][\"train\"], desc=f\"Epoch {epoch + 1}/{NUM_EPOCHS} Training\"\n",
    "    )\n",
    "\n",
    "    for batch in train_loader:\n",
    "        # Move everything to the device\n",
    "        batch[\"pixel_values\"] = batch[\"pixel_values\"].to(device)\n",
    "        batch[\"pixel_mask\"] = batch[\"pixel_mask\"].to(device)\n",
    "        batch[\"mask_labels\"] = [entry.to(device) for entry in batch[\"mask_labels\"]]\n",
    "        batch[\"class_labels\"] = [entry.to(device) for entry in batch[\"class_labels\"]]\n",
    "\n",
    "        # Compute output and loss\n",
    "        outputs = model(**batch)\n",
    "\n",
    "        loss = outputs.loss\n",
    "\n",
    "        # Compute gradient and perform step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Record losses\n",
    "        current_loss = loss.item() * batch[\"pixel_values\"].size(0)\n",
    "        train_running_loss += current_loss\n",
    "        train_loader.set_postfix(loss=f\"{current_loss:.4f}\")\n",
    "\n",
    "        # Extract and compute metrics\n",
    "        pred_maps, masks = m2f_extract_pred_maps_and_masks(\n",
    "            batch, outputs, m2f_preprocessor_B\n",
    "        )\n",
    "        metric.add_batch(references=masks, predictions=pred_maps)\n",
    "        \n",
    "\n",
    "    # After compute the batches that were added are deleted\n",
    "    mean_train_iou = metric.compute(\n",
    "        num_labels=NUM_CLASSES, ignore_index=BG_VALUE_255, reduce_labels=False\n",
    "    )[\"mean_iou\"]\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loader = tqdm(\n",
    "        dataloaders[CURR_TASK][\"val\"], desc=f\"Epoch {epoch + 1}/{NUM_EPOCHS} Validation\"\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            # Move everything to the device\n",
    "            batch[\"pixel_values\"] = batch[\"pixel_values\"].to(device)\n",
    "            batch[\"pixel_mask\"] = batch[\"pixel_mask\"].to(device)\n",
    "            batch[\"mask_labels\"] = [entry.to(device) for entry in batch[\"mask_labels\"]]\n",
    "            batch[\"class_labels\"] = [\n",
    "                entry.to(device) for entry in batch[\"class_labels\"]\n",
    "            ]\n",
    "            # Compute output and loss\n",
    "            outputs = model(**batch)\n",
    "\n",
    "            loss = outputs.loss\n",
    "            # Record losses\n",
    "            current_loss = loss.item() * batch[\"pixel_values\"].size(0)\n",
    "            val_running_loss += current_loss\n",
    "            val_loader.set_postfix(loss=f\"{current_loss:.4f}\")\n",
    "\n",
    "            # Extract and compute metrics\n",
    "            pred_maps, masks = m2f_extract_pred_maps_and_masks(\n",
    "                batch, outputs, m2f_preprocessor_B\n",
    "            )\n",
    "            metric.add_batch(references=masks, predictions=pred_maps)\n",
    "            \n",
    "\n",
    "    # After compute the batches that were added are deleted\n",
    "    mean_val_iou = metric.compute(\n",
    "        num_labels=NUM_CLASSES, ignore_index=BG_VALUE_255, reduce_labels=False\n",
    "    )[\"mean_iou\"]\n",
    "\n",
    "    epoch_train_loss = train_running_loss / len(dataloaders[CURR_TASK][\"train\"].dataset)\n",
    "    epoch_val_loss = val_running_loss / len(dataloaders[CURR_TASK][\"val\"].dataset)\n",
    "\n",
    "    writer.add_scalar(f\"Loss/train_{model_name}_{CURR_TASK}\", epoch_train_loss, epoch + 1)\n",
    "    writer.add_scalar(f\"Loss/val_{model_name}_{CURR_TASK}\", epoch_val_loss, epoch + 1)\n",
    "    writer.add_scalar(f\"mIoU/train_{model_name}_{CURR_TASK}\", mean_train_iou, epoch + 1)\n",
    "    writer.add_scalar(f\"mIoU/val_{model_name}_{CURR_TASK}\", mean_val_iou, epoch + 1)\n",
    "\n",
    "    wandb.log({\n",
    "        f\"Loss/train_A+{CURR_TASK}\": epoch_train_loss,\n",
    "        f\"Loss/val_A+{CURR_TASK}\": epoch_val_loss,\n",
    "        f\"mIoU/train_A+{CURR_TASK}\": mean_train_iou,\n",
    "        f\"mIoU/val_A+{CURR_TASK}\": mean_val_iou\n",
    "    })\n",
    "\n",
    "    tqdm.write(\n",
    "        f\"Epoch {epoch + 1}/{NUM_EPOCHS}, Train Loss: {epoch_train_loss:.4f}, Train mIoU: {mean_train_iou:.4f}, Validation Loss: {epoch_val_loss:.4f}, Validation mIoU: {mean_val_iou:.4f}\"\n",
    "    )\n",
    "    if mean_val_iou > best_val_metric:\n",
    "        best_val_metric = mean_val_iou\n",
    "        model.save_pretrained(model_path_second)\n",
    "        counter=0\n",
    "    else:\n",
    "        counter+=1\n",
    "        if counter == PATIENCE:\n",
    "            print(\"Early stopping at epoch\",epoch)\n",
    "            break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Test results on B first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model and evaluate on test\n",
    "model = Mask2FormerForUniversalSegmentation.from_pretrained(model_path_second).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test loop: 100%|██████████| 14/14 [00:22<00:00,  1.61s/it, loss=102.1515]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 7.7336, Test mIoU: 0.8278\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "test_running_loss = 0\n",
    "test_loader = tqdm(dataloaders[CURR_TASK][\"test\"], desc=\"Test loop\")\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        # Move everything to the device\n",
    "        batch[\"pixel_values\"] = batch[\"pixel_values\"].to(device)\n",
    "        batch[\"pixel_mask\"] = batch[\"pixel_mask\"].to(device)\n",
    "        batch[\"mask_labels\"] = [entry.to(device) for entry in batch[\"mask_labels\"]]\n",
    "        batch[\"class_labels\"] = [entry.to(device) for entry in batch[\"class_labels\"]]\n",
    "        # Compute output and loss\n",
    "        outputs = model(**batch)\n",
    "\n",
    "        loss = outputs.loss\n",
    "        # Record losses\n",
    "        current_loss = loss.item() * batch[\"pixel_values\"].size(0)\n",
    "        test_running_loss += current_loss\n",
    "        test_loader.set_postfix(loss=f\"{current_loss:.4f}\")\n",
    "\n",
    "        # Extract and compute metrics\n",
    "        pred_maps, masks = m2f_extract_pred_maps_and_masks(\n",
    "            batch, outputs, m2f_preprocessor_B\n",
    "        )\n",
    "        metric.add_batch(references=masks, predictions=pred_maps)\n",
    "\n",
    "\n",
    "# After compute the batches that were added are deleted\n",
    "test_metrics_B = metric.compute(\n",
    "    num_labels=NUM_CLASSES, ignore_index=BG_VALUE_255, reduce_labels=False\n",
    ")\n",
    "mean_test_iou = test_metrics_B[\"mean_iou\"]\n",
    "final_test_loss = test_running_loss / len(dataloaders[CURR_TASK][\"test\"].dataset)\n",
    "wandb.log({\n",
    "    f\"Loss/test_{CURR_TASK}\": final_test_loss,\n",
    "    f\"mIoU/test_{CURR_TASK}\": mean_test_iou\n",
    "})\n",
    "print(f\"Test Loss: {final_test_loss:.4f}, Test mIoU: {mean_test_iou:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Test results on A after training on B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test loop: 100%|██████████| 36/36 [00:53<00:00,  1.48s/it, loss=266.1530]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 17.5114, Test mIoU: 0.7854\n"
     ]
    }
   ],
   "source": [
    "# To avoid making stupid errors\n",
    "CURR_TASK = \"A\"\n",
    "\n",
    "model.eval()\n",
    "test_running_loss = 0\n",
    "test_loader = tqdm(dataloaders[CURR_TASK][\"test\"], desc=\"Test loop\")\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        # Move everything to the device\n",
    "        batch[\"pixel_values\"] = batch[\"pixel_values\"].to(device)\n",
    "        batch[\"pixel_mask\"] = batch[\"pixel_mask\"].to(device)\n",
    "        batch[\"mask_labels\"] = [entry.to(device) for entry in batch[\"mask_labels\"]]\n",
    "        batch[\"class_labels\"] = [entry.to(device) for entry in batch[\"class_labels\"]]\n",
    "        # Compute output and loss\n",
    "        outputs = model(**batch)\n",
    "\n",
    "        loss = outputs.loss\n",
    "        # Record losses\n",
    "        current_loss = loss.item() * batch[\"pixel_values\"].size(0)\n",
    "        test_running_loss += current_loss\n",
    "        test_loader.set_postfix(loss=f\"{current_loss:.4f}\")\n",
    "\n",
    "        # Extract and compute metrics\n",
    "        pred_maps, masks = m2f_extract_pred_maps_and_masks(\n",
    "            batch, outputs, m2f_preprocessor_A\n",
    "        )\n",
    "        metric.add_batch(references=masks, predictions=pred_maps)\n",
    "        \n",
    "# After compute the batches that were added are deleted\n",
    "test_metrics_forgetting_A = metric.compute(\n",
    "    num_labels=NUM_CLASSES, ignore_index=BG_VALUE_255, reduce_labels=False\n",
    ")\n",
    "mean_test_iou = test_metrics_forgetting_A[\"mean_iou\"]\n",
    "final_test_loss = test_running_loss / len(dataloaders[CURR_TASK][\"test\"].dataset)\n",
    "wandb.log({\n",
    "    f\"Loss/test_replay_all_{CURR_TASK}\": final_test_loss,\n",
    "    f\"mIoU/test_replay_all_{CURR_TASK}\": mean_test_iou\n",
    "})\n",
    "print(f\"Test Loss: {final_test_loss:.4f}, Test mIoU: {mean_test_iou:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** Overall mIoU ****\n",
      "mIoU on task A: 0.7715989272025321\n",
      "mIoU on task B: 0.8277712327043516\n",
      "mIoU on task A after training on B: 0.7854313993000791\n",
      "\n",
      "**** Per category mIoU ****\n",
      "Per category mIoU on task A: [0.96005866 0.93660036 0.63264134 0.41352694 0.39168077 0.81113885\n",
      " 0.80177793 0.7707886  0.89548438 0.94156179 0.93232858]\n",
      "Per category mIoU on task B: [0.96077659 0.93669937 0.64262966 0.59210555 0.44402038 0.81422925\n",
      " 0.75690367 0.75457962 0.88294103 0.93135118 0.92350911]\n",
      "Per category mIoU on task A after training on B: [0.96077659 0.93669937 0.64262966 0.59210555 0.44402038 0.81422925\n",
      " 0.75690367 0.75457962 0.88294103 0.93135118 0.92350911]\n",
      "\n",
      "**** Average learning accuracies ****\n",
      "Average learning acc.: 0.7996850799534418\n",
      "Per category Average learning acc.: [0.96041762 0.93664986 0.6376355  0.50281624 0.41785058 0.81268405\n",
      " 0.7793408  0.76268411 0.8892127  0.93645649 0.92791884]\n",
      "\n",
      "**** Forgetting ****\n",
      "Total forgetting: -0.01383247209754701\n",
      "Per category forgetting: [-7.17928432e-04 -9.90047526e-05 -9.98831850e-03 -1.78578610e-01\n",
      " -5.23396101e-02 -3.09039992e-03  4.48742539e-02  1.62089822e-02\n",
      "  1.25433499e-02  1.02106149e-02  8.81947724e-03]\n"
     ]
    }
   ],
   "source": [
    "# Collect overall mIoU\n",
    "mIoU_A = test_metrics_A[\"mean_iou\"]\n",
    "mIoU_forgetting_A = test_metrics_forgetting_A[\"mean_iou\"]\n",
    "mIoU_B = test_metrics_B[\"mean_iou\"]\n",
    "\n",
    "# Collect per category mIoU\n",
    "per_category_mIoU_A = np.array(test_metrics_A[\"per_category_iou\"])\n",
    "per_category_mIoU_forgetting_A = np.array(test_metrics_forgetting_A[\"per_category_iou\"])\n",
    "per_category_mIoU_B = np.array(test_metrics_forgetting_A[\"per_category_iou\"])\n",
    "\n",
    "# Average learning accuracies (mIoUs)\n",
    "avg_learning_acc = (mIoU_A + mIoU_B) / 2\n",
    "per_category_avg_learning_acc = (per_category_mIoU_A + per_category_mIoU_B) / 2\n",
    "\n",
    "# Forgetting\n",
    "total_forgetting = mIoU_A - mIoU_forgetting_A\n",
    "per_category_forgetting = (per_category_mIoU_A - per_category_mIoU_forgetting_A)\n",
    "\n",
    "# Export evaluation metrics to WandB\n",
    "wandb.log({\n",
    "    \"eval/avg_learning_acc\": avg_learning_acc,\n",
    "    \"eval/per_category_avg_learning_acc\": per_category_avg_learning_acc,\n",
    "    \"eval/total_forgetting\": total_forgetting,\n",
    "    \"eval/per_category_forgetting\": per_category_forgetting\n",
    "})\n",
    "print(\"**** Overall mIoU ****\")\n",
    "print(f\"mIoU on task A: {mIoU_A}\")\n",
    "print(f\"mIoU on task B: {mIoU_B}\")\n",
    "print(f\"mIoU on task A after training on B: {mIoU_forgetting_A}\")\n",
    "\n",
    "print(\"\\n**** Per category mIoU ****\")\n",
    "print(f\"Per category mIoU on task A: {per_category_mIoU_A}\")\n",
    "print(f\"Per category mIoU on task B: {per_category_mIoU_B}\")\n",
    "print(f\"Per category mIoU on task A after training on B: {per_category_mIoU_forgetting_A}\")\n",
    "\n",
    "print(\"\\n**** Average learning accuracies ****\")\n",
    "print(f\"Average learning acc.: {avg_learning_acc}\")\n",
    "print(f\"Per category Average learning acc.: {per_category_avg_learning_acc}\")\n",
    "\n",
    "print(\"\\n**** Forgetting ****\")\n",
    "print(f\"Total forgetting: {total_forgetting}\")\n",
    "print(f\"Per category forgetting: {per_category_forgetting}\")\n",
    "wandb.finish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
