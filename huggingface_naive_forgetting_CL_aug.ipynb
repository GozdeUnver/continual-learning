{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gozdeunver/Library/Caches/pypoetry/virtualenvs/continual-learning-UoKZrllX-py3.12/lib/python3.12/site-packages/threadpoolctl.py:1214: RuntimeWarning: \n",
      "Found Intel OpenMP ('libiomp') and LLVM OpenMP ('libomp') loaded at\n",
      "the same time. Both libraries are known to be incompatible and this\n",
      "can cause random crashes or deadlocks on Linux when loaded in the\n",
      "same Python program.\n",
      "Using threadpoolctl may cause crashes or deadlocks. For more\n",
      "information and possible workarounds, please see\n",
      "    https://github.com/joblib/threadpoolctl/blob/master/multiple_openmp.md\n",
      "\n",
      "  warnings.warn(msg, RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "from utils.common import (\n",
    "    m2f_dataset_collate,\n",
    "    m2f_extract_pred_maps_and_masks,\n",
    "    get_perpixel_features,\n",
    "    set_seed,\n",
    "    pixel_mean_std,\n",
    "    CADIS_PIXEL_MEAN,\n",
    "    CADIS_PIXEL_STD,\n",
    "    CAT1K_PIXEL_MEAN,\n",
    "    CAT1K_PIXEL_STD,\n",
    "    WEIGHTS_CADIS_TRAIN\n",
    ")\n",
    "from utils.dataset_utils import (\n",
    "    get_cadisv2_dataset,\n",
    "    get_cataract1k_dataset,\n",
    "    ZEISS_CATEGORIES,\n",
    ")\n",
    "from utils.augmentations import train_transforms_color_jitter\n",
    "from utils.medical_datasets import Mask2FormerDataset\n",
    "from transformers import (\n",
    "    Mask2FormerForUniversalSegmentation,\n",
    "    SwinModel,\n",
    "    SwinConfig,\n",
    "    Mask2FormerConfig,\n",
    "    AutoImageProcessor,\n",
    "    Mask2FormerImageProcessor,\n",
    ")\n",
    "from torch.utils.data import DataLoader\n",
    "import evaluate\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "import wandb\n",
    "from copy import deepcopy\n",
    "import shutil\n",
    "import random\n",
    "from utils.wandb_utils import log_table_of_images\n",
    "from utils.losses import PixelContrastLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gozdeunver/Library/Caches/pypoetry/virtualenvs/continual-learning-UoKZrllX-py3.12/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not Matched: hidden_states_norms.stage1.weight != layernorm.weight\n",
      "Not Matched: hidden_states_norms.stage1.bias != layernorm.bias\n"
     ]
    }
   ],
   "source": [
    "NUM_CLASSES = len(ZEISS_CATEGORIES) - 3  + 1 # Remove class incremental and add background !!!\n",
    "SWIN_BACKBONE = \"microsoft/swin-tiny-patch4-window7-224\"#\"microsoft/swin-large-patch4-window12-384\"\n",
    "\n",
    "# Download pretrained swin model\n",
    "swin_model = SwinModel.from_pretrained(\n",
    "    SWIN_BACKBONE, out_features=[\"stage1\", \"stage2\", \"stage3\", \"stage4\"]\n",
    ")\n",
    "swin_config = SwinConfig.from_pretrained(\n",
    "    SWIN_BACKBONE, out_features=[\"stage1\", \"stage2\", \"stage3\", \"stage4\"]\n",
    ")\n",
    "\n",
    "# Create Mask2Former configuration based on Swin's configuration\n",
    "mask2former_config = Mask2FormerConfig(\n",
    "    backbone_config=swin_config, num_labels=NUM_CLASSES #, ignore_value=BG_VALUE\n",
    ")\n",
    "\n",
    "# Create the Mask2Former model with this configuration\n",
    "model = Mask2FormerForUniversalSegmentation(mask2former_config)\n",
    "\n",
    "# Reuse pretrained parameters\n",
    "for swin_param, m2f_param in zip(\n",
    "    swin_model.named_parameters(),\n",
    "    model.model.pixel_level_module.encoder.named_parameters(),\n",
    "):\n",
    "    m2f_param_name = f\"model.pixel_level_module.encoder.{m2f_param[0]}\"\n",
    "\n",
    "    if swin_param[0] == m2f_param[0]:\n",
    "        model.state_dict()[m2f_param_name].copy_(swin_param[1])\n",
    "        continue\n",
    "\n",
    "    print(f\"Not Matched: {m2f_param[0]} != {swin_param[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/gozdeunver/Library/Caches/pypoetry/virtualenvs/continual-learning-UoKZrllX-py3.12/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A': {'train': <torch.utils.data.dataloader.DataLoader object at 0x165fc1df0>, 'val': <torch.utils.data.dataloader.DataLoader object at 0x165fc1e20>, 'test': <torch.utils.data.dataloader.DataLoader object at 0x165fc1520>}, 'B': {'train': <torch.utils.data.dataloader.DataLoader object at 0x165fc0b00>, 'val': <torch.utils.data.dataloader.DataLoader object at 0x165fc15b0>, 'test': <torch.utils.data.dataloader.DataLoader object at 0x165fc22a0>}}\n"
     ]
    }
   ],
   "source": [
    "# Helper function to load datasets\n",
    "def load_dataset(dataset_getter, data_path, domain_incremental):\n",
    "    return dataset_getter(data_path, domain_incremental=domain_incremental)\n",
    "\n",
    "\n",
    "# Helper function to create dataloaders for a dataset\n",
    "def create_dataloaders(\n",
    "    dataset, batch_size, shuffle, num_workers, drop_last, pin_memory, collate_fn\n",
    "):\n",
    "    batch_size_val=16\n",
    "    batch_size_test=16\n",
    "\n",
    "    return {\n",
    "        \"train\": DataLoader(\n",
    "            dataset[\"train\"],\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle,\n",
    "            num_workers=num_workers,\n",
    "            drop_last=drop_last,\n",
    "            pin_memory=pin_memory,\n",
    "            collate_fn=collate_fn,\n",
    "        ),\n",
    "        \"val\": DataLoader(\n",
    "            dataset[\"val\"],\n",
    "            batch_size=batch_size_val,\n",
    "            shuffle=shuffle,\n",
    "            num_workers=num_workers,\n",
    "            drop_last=drop_last,\n",
    "            pin_memory=pin_memory,\n",
    "            collate_fn=collate_fn,\n",
    "        ),\n",
    "        \"test\": DataLoader(\n",
    "            dataset[\"test\"],\n",
    "            batch_size=batch_size_test,\n",
    "            shuffle=False,\n",
    "            num_workers=num_workers,\n",
    "            drop_last=False,\n",
    "            pin_memory=pin_memory,\n",
    "            collate_fn=collate_fn,\n",
    "        ),\n",
    "    }\n",
    "\n",
    "\n",
    "# Load datasets\n",
    "datasets = {\n",
    "    \"A\": load_dataset(get_cadisv2_dataset, \"../../storage/data/CaDISv2\", True),\n",
    "    \"B\": load_dataset(get_cataract1k_dataset, \"../../storage/data/cataract-1k\", True),\n",
    "}\n",
    "\n",
    "\n",
    "pixel_mean_A=np.array(CADIS_PIXEL_MEAN)\n",
    "pixel_std_A=np.array(CADIS_PIXEL_STD)\n",
    "pixel_mean_B=np.array(CAT1K_PIXEL_MEAN)\n",
    "pixel_std_B=np.array(CAT1K_PIXEL_STD)\n",
    "\n",
    "weights_A=WEIGHTS_CADIS_TRAIN # Class weights used for weightes contrastive loss\n",
    "\n",
    "# Define preprocessor\n",
    "swin_processor = AutoImageProcessor.from_pretrained(SWIN_BACKBONE)\n",
    "m2f_preprocessor_A = Mask2FormerImageProcessor(\n",
    "    reduce_labels=False,\n",
    "    ignore_index=255,\n",
    "    do_resize=False,\n",
    "    do_rescale=False,\n",
    "    do_normalize=True,\n",
    "    image_std=pixel_std_A,\n",
    "    image_mean=pixel_mean_A,\n",
    ")\n",
    "\n",
    "m2f_preprocessor_B = Mask2FormerImageProcessor(\n",
    "    reduce_labels=False,\n",
    "    ignore_index=255,\n",
    "    do_resize=False,\n",
    "    do_rescale=False,\n",
    "    do_normalize=True,\n",
    "    image_std=pixel_std_B,\n",
    "    image_mean=pixel_mean_B,\n",
    ")\n",
    "# Create Mask2Former Datasets\n",
    "\n",
    "m2f_datasets = {\n",
    "    \"A\": {\n",
    "        \"train\": Mask2FormerDataset(datasets[\"A\"][0], m2f_preprocessor_A, transform=train_transforms_color_jitter),\n",
    "        \"val\": Mask2FormerDataset(datasets[\"A\"][1], m2f_preprocessor_A),\n",
    "        \"test\": Mask2FormerDataset(datasets[\"A\"][2], m2f_preprocessor_A),\n",
    "    },\n",
    "    \"B\": {\n",
    "        \"train\": Mask2FormerDataset(datasets[\"B\"][0], m2f_preprocessor_B, transform=train_transforms_color_jitter),\n",
    "        \"val\": Mask2FormerDataset(datasets[\"B\"][1], m2f_preprocessor_B),\n",
    "        \"test\": Mask2FormerDataset(datasets[\"B\"][2], m2f_preprocessor_B),\n",
    "    },\n",
    "}\n",
    "\n",
    "# DataLoader parameters\n",
    "N_WORKERS = 4\n",
    "BATCH_SIZE = 16\n",
    "SHUFFLE = True\n",
    "DROP_LAST = True\n",
    "\n",
    "dataloader_params = {\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"shuffle\": SHUFFLE,\n",
    "    \"num_workers\": N_WORKERS,\n",
    "    \"drop_last\": DROP_LAST,\n",
    "    \"pin_memory\": True,\n",
    "    \"collate_fn\": m2f_dataset_collate,\n",
    "}\n",
    "\n",
    "# Create DataLoaders\n",
    "dataloaders = {\n",
    "    key: create_dataloaders(m2f_datasets[key], **dataloader_params)\n",
    "    for key in m2f_datasets\n",
    "}\n",
    "\n",
    "print(dataloaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA is available, otherwise use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "BG_VALUE_255=255\n",
    "base_run_name=\"M2F-Swin-Tiny-Train_Cadis_Contrastive_Loss_Aug\"\n",
    "project_name = \"M2F_latest\"\n",
    "user_or_team = \"continual-learning-tum\"\n",
    "new_run_name=\"M2F-Swin-Tiny-Train_Naive-Forgetting-CL-Aug\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-e92f1dae8f8b562\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-e92f1dae8f8b562\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tensorboard setup\n",
    "out_dir=\"outputs/\"\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "if not os.path.exists(out_dir+\"runs\"):\n",
    "    os.makedirs(out_dir+\"runs\")\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir outputs/runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "!CUDA_LAUNCH_BLOCKING=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorboard logging\n",
    "writer = SummaryWriter(log_dir=out_dir + \"runs\")\n",
    "\n",
    "# Model checkpointing\n",
    "model_dir = out_dir + \"models/\"\n",
    "if not os.path.exists(model_dir):\n",
    "    print(\"Store weights in: \", model_dir)\n",
    "    os.makedirs(model_dir)\n",
    "\n",
    "best_model_dir = model_dir + f\"{base_run_name}/best_model/\"\n",
    "if not os.path.exists(best_model_dir):\n",
    "    print(\"Store best model weights in: \", best_model_dir)\n",
    "    os.makedirs(best_model_dir)\n",
    "final_model_dir = model_dir + f\"{base_run_name}/final_model/\"\n",
    "if not os.path.exists(final_model_dir):\n",
    "    print(\"Store final model weights in: \", final_model_dir)\n",
    "    os.makedirs(final_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mge85ket\u001b[0m (\u001b[33mcontinual-learning-tum\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# WandB for team usage !!!!\n",
    "\n",
    "wandb.login() # use this one if a different person is going to run the notebook\n",
    "#wandb.login(relogin=False) # if the same person in the last run is going to run the notebook again\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "NUM_EPOCHS = 200\n",
    "LEARNING_RATE = 1e-4\n",
    "LR_MULTIPLIER = 0.1\n",
    "BACKBONE_LR = LEARNING_RATE * LR_MULTIPLIER\n",
    "WEIGHT_DECAY = 0.05\n",
    "PATIENCE=15\n",
    "\n",
    "# Important to ALWAYS set the full_batch_sampling=True for A training (no replay)\n",
    "\n",
    "use_prototypes=False # Change it to True if prototypes will be used as anchors and change the loss function as below: \n",
    "#contrastive_loss=PixelContrastLoss(full_batch_sampling=True, num_classes=NUM_CLASSES, num_prototypes_per_class=5, in_channels=256)\n",
    "\n",
    "#contrastive_loss = PixelContrastLoss(full_batch_sampling=True, weights=weights_A) # if weighted contrastive loss needs to be used\n",
    "contrastive_loss=PixelContrastLoss(full_batch_sampling=True) # vanilla CL\n",
    "CONTRASTIVE_LOSS_LAMBDA=1 # Increase the lambda when necessary (probably while using weighted contrastive loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"mean_iou\")\n",
    "encoder_params = [\n",
    "    param\n",
    "    for name, param in model.named_parameters()\n",
    "    if name.startswith(\"model.pixel_level_module.encoder\")\n",
    "]\n",
    "decoder_params = [\n",
    "    param\n",
    "    for name, param in model.named_parameters()\n",
    "    if name.startswith(\"model.pixel_level_module.decoder\")\n",
    "]\n",
    "transformer_params = [\n",
    "    param\n",
    "    for name, param in model.named_parameters()\n",
    "    if name.startswith(\"model.transformer_module\")\n",
    "]\n",
    "class_prediction_params=[\n",
    "    param\n",
    "    for name, param in model.named_parameters() \n",
    "    if not name.startswith(\"model.pixel_level_module.encoder\") and not name.startswith(\"model.transformer_module\") and not name.startswith(\"model.pixel_level_module.decoder\")\n",
    "]\n",
    "optimizer = optim.AdamW(\n",
    "    [\n",
    "        {\"params\": encoder_params, \"lr\": BACKBONE_LR},\n",
    "        {\"params\": decoder_params},\n",
    "        {\"params\": transformer_params},\n",
    "        {\"params\": class_prediction_params}\n",
    "    ],\n",
    "    lr=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    ")\n",
    "\n",
    "scheduler = optim.lr_scheduler.PolynomialLR(\n",
    "    optimizer, total_iters=NUM_EPOCHS, power=0.9\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(\n",
    "    project=project_name,\n",
    "    config={\n",
    "        \"learning_rate\": LEARNING_RATE,\n",
    "        \"learning_rate_multiplier\": LR_MULTIPLIER,\n",
    "        \"backbone_learning_rate\": BACKBONE_LR,\n",
    "        \"learning_rate_scheduler\": scheduler.__class__.__name__,\n",
    "        \"optimizer\": optimizer.__class__.__name__,\n",
    "        \"backbone\": SWIN_BACKBONE,\n",
    "        \"m2f_preprocessor\": m2f_preprocessor_A.__dict__,\n",
    "        \"m2f_model_config\": model.config\n",
    "    },\n",
    "    name=new_run_name,\n",
    "    notes=\"M2F with tiny Swin backbone pretrained on ImageNet-1K. \\\n",
    "        Scenario: Train on A, Test on A\"\n",
    ")\n",
    "print(\"wandb run id:\",wandb.run.id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact best_model_M2F-Swin-Tiny-Train_Cadis_Contrastive_Loss_Aug:latest, 181.32MB. 1 files... \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n",
      "Done. 0:0:8.5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Mask2FormerForUniversalSegmentation(\n",
       "  (model): Mask2FormerModel(\n",
       "    (pixel_level_module): Mask2FormerPixelLevelModule(\n",
       "      (encoder): SwinBackbone(\n",
       "        (embeddings): SwinEmbeddings(\n",
       "          (patch_embeddings): SwinPatchEmbeddings(\n",
       "            (projection): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))\n",
       "          )\n",
       "          (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (encoder): SwinEncoder(\n",
       "          (layers): ModuleList(\n",
       "            (0): SwinStage(\n",
       "              (blocks): ModuleList(\n",
       "                (0-1): 2 x SwinLayer(\n",
       "                  (layernorm_before): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "                  (attention): SwinAttention(\n",
       "                    (self): SwinSelfAttention(\n",
       "                      (query): Linear(in_features=96, out_features=96, bias=True)\n",
       "                      (key): Linear(in_features=96, out_features=96, bias=True)\n",
       "                      (value): Linear(in_features=96, out_features=96, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (output): SwinSelfOutput(\n",
       "                      (dense): Linear(in_features=96, out_features=96, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                  (drop_path): SwinDropPath(p=0.1)\n",
       "                  (layernorm_after): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "                  (intermediate): SwinIntermediate(\n",
       "                    (dense): Linear(in_features=96, out_features=384, bias=True)\n",
       "                    (intermediate_act_fn): GELUActivation()\n",
       "                  )\n",
       "                  (output): SwinOutput(\n",
       "                    (dense): Linear(in_features=384, out_features=96, bias=True)\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (downsample): SwinPatchMerging(\n",
       "                (reduction): Linear(in_features=384, out_features=192, bias=False)\n",
       "                (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "            (1): SwinStage(\n",
       "              (blocks): ModuleList(\n",
       "                (0-1): 2 x SwinLayer(\n",
       "                  (layernorm_before): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "                  (attention): SwinAttention(\n",
       "                    (self): SwinSelfAttention(\n",
       "                      (query): Linear(in_features=192, out_features=192, bias=True)\n",
       "                      (key): Linear(in_features=192, out_features=192, bias=True)\n",
       "                      (value): Linear(in_features=192, out_features=192, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (output): SwinSelfOutput(\n",
       "                      (dense): Linear(in_features=192, out_features=192, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                  (drop_path): SwinDropPath(p=0.1)\n",
       "                  (layernorm_after): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "                  (intermediate): SwinIntermediate(\n",
       "                    (dense): Linear(in_features=192, out_features=768, bias=True)\n",
       "                    (intermediate_act_fn): GELUActivation()\n",
       "                  )\n",
       "                  (output): SwinOutput(\n",
       "                    (dense): Linear(in_features=768, out_features=192, bias=True)\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (downsample): SwinPatchMerging(\n",
       "                (reduction): Linear(in_features=768, out_features=384, bias=False)\n",
       "                (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "            (2): SwinStage(\n",
       "              (blocks): ModuleList(\n",
       "                (0-5): 6 x SwinLayer(\n",
       "                  (layernorm_before): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "                  (attention): SwinAttention(\n",
       "                    (self): SwinSelfAttention(\n",
       "                      (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "                      (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "                      (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (output): SwinSelfOutput(\n",
       "                      (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                  (drop_path): SwinDropPath(p=0.1)\n",
       "                  (layernorm_after): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "                  (intermediate): SwinIntermediate(\n",
       "                    (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                    (intermediate_act_fn): GELUActivation()\n",
       "                  )\n",
       "                  (output): SwinOutput(\n",
       "                    (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (downsample): SwinPatchMerging(\n",
       "                (reduction): Linear(in_features=1536, out_features=768, bias=False)\n",
       "                (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "            (3): SwinStage(\n",
       "              (blocks): ModuleList(\n",
       "                (0-1): 2 x SwinLayer(\n",
       "                  (layernorm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                  (attention): SwinAttention(\n",
       "                    (self): SwinSelfAttention(\n",
       "                      (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                      (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                      (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (output): SwinSelfOutput(\n",
       "                      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                  (drop_path): SwinDropPath(p=0.1)\n",
       "                  (layernorm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                  (intermediate): SwinIntermediate(\n",
       "                    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                    (intermediate_act_fn): GELUActivation()\n",
       "                  )\n",
       "                  (output): SwinOutput(\n",
       "                    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (hidden_states_norms): ModuleDict(\n",
       "          (stage1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "          (stage2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (stage3): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (stage4): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (decoder): Mask2FormerPixelDecoder(\n",
       "        (position_embedding): Mask2FormerSinePositionEmbedding()\n",
       "        (input_projections): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): Conv2d(192, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "          )\n",
       "        )\n",
       "        (encoder): Mask2FormerPixelDecoderEncoderOnly(\n",
       "          (layers): ModuleList(\n",
       "            (0-5): 6 x Mask2FormerPixelDecoderEncoderLayer(\n",
       "              (self_attn): Mask2FormerPixelDecoderEncoderMultiscaleDeformableAttention(\n",
       "                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)\n",
       "                (attention_weights): Linear(in_features=256, out_features=96, bias=True)\n",
       "                (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              )\n",
       "              (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "              (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "              (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "              (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (mask_projection): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (adapter_1): Sequential(\n",
       "          (0): Conv2d(96, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "        )\n",
       "        (layer_1): Sequential(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (transformer_module): Mask2FormerTransformerModule(\n",
       "      (position_embedder): Mask2FormerSinePositionEmbedding()\n",
       "      (queries_embedder): Embedding(100, 256)\n",
       "      (queries_features): Embedding(100, 256)\n",
       "      (decoder): Mask2FormerMaskedAttentionDecoder(\n",
       "        (layers): ModuleList(\n",
       "          (0-8): 9 x Mask2FormerMaskedAttentionDecoderLayer(\n",
       "            (self_attn): Mask2FormerAttention(\n",
       "              (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            )\n",
       "            (activation_fn): ReLU()\n",
       "            (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (cross_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "            )\n",
       "            (cross_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "            (fc2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "            (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (layernorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (mask_predictor): Mask2FormerMaskPredictor(\n",
       "          (mask_embedder): Mask2FormerMLPPredictionHead(\n",
       "            (0): Mask2FormerPredictionBlock(\n",
       "              (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (1): ReLU()\n",
       "            )\n",
       "            (1): Mask2FormerPredictionBlock(\n",
       "              (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (1): ReLU()\n",
       "            )\n",
       "            (2): Mask2FormerPredictionBlock(\n",
       "              (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (1): Identity()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (level_embed): Embedding(3, 256)\n",
       "    )\n",
       "  )\n",
       "  (class_predictor): Linear(in_features=256, out_features=13, bias=True)\n",
       "  (criterion): Mask2FormerLoss(\n",
       "    (matcher): Mask2FormerHungarianMatcher()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load best model and evaluate on test\n",
    "# Construct the artifact path\n",
    "artifact_path = f\"{user_or_team}/{project_name}/best_model_{base_run_name}:latest\"\n",
    "\n",
    "# Load from W&B\n",
    "api = wandb.Api()\n",
    "artifact=api.artifact(artifact_path)\n",
    "artifact_dir=artifact.download()\n",
    "model_state_dict_path = os.path.join(artifact_dir, f\"best_model_{base_run_name}.pth\" )\n",
    "model_state_dict = torch.load(model_state_dict_path,map_location=device)\n",
    "model = Mask2FormerForUniversalSegmentation(mask2former_config)\n",
    "model.load_state_dict(model_state_dict)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "test_running_loss = 0\n",
    "CURR_TASK=\"A\"\n",
    "test_loader = tqdm(dataloaders[CURR_TASK][\"test\"], desc=\"Test loop\")\n",
    "\n",
    "BATCH_INDEX = 0\n",
    "table = wandb.Table(columns=[\"ID\", \"Image\"])\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        # Move everything to the device\n",
    "        batch[\"pixel_values\"] = batch[\"pixel_values\"].to(device)\n",
    "        batch[\"pixel_mask\"] = batch[\"pixel_mask\"].to(device)\n",
    "        batch[\"mask_labels\"] = [entry.to(device) for entry in batch[\"mask_labels\"]]\n",
    "        batch[\"class_labels\"] = [entry.to(device) for entry in batch[\"class_labels\"]]\n",
    "        # Compute output and loss\n",
    "        outputs = model(**batch)\n",
    "\n",
    "        loss = outputs.loss\n",
    "        # Record losses\n",
    "        current_loss = loss.item() * batch[\"pixel_values\"].size(0)\n",
    "        test_running_loss += current_loss\n",
    "        test_loader.set_postfix(loss=f\"{current_loss:.4f}\")\n",
    "\n",
    "        # Extract and compute metrics\n",
    "        pred_maps, masks = m2f_extract_pred_maps_and_masks(\n",
    "            batch, outputs, m2f_preprocessor_A\n",
    "        )\n",
    "        metric.add_batch(references=masks, predictions=pred_maps)\n",
    "        if BATCH_INDEX <5:\n",
    "            # Visualize\n",
    "            log_table_of_images(\n",
    "                table, # common table for all batches\n",
    "                batch[\"pixel_values\"],\n",
    "                pixel_mean_A, # remove normalization\n",
    "                pixel_std_A, # remove normalization\n",
    "                pred_maps,\n",
    "                masks,\n",
    "                BATCH_INDEX, # correct indexing in table\n",
    "            )\n",
    "            BATCH_INDEX += 1\n",
    "# Log table\n",
    "wandb.log({f\"{CURR_TASK}_TEST_AFTER_TRAINING_A\": table})\n",
    "\n",
    "# After compute the batches that were added are deleted\n",
    "test_metrics_A = metric.compute(\n",
    "    num_labels=NUM_CLASSES, ignore_index=BG_VALUE_255, reduce_labels=False\n",
    ")\n",
    "mean_test_iou = test_metrics_A[\"mean_iou\"]\n",
    "final_test_loss = test_running_loss / len(dataloaders[CURR_TASK][\"test\"].dataset)\n",
    "wandb.log({\n",
    "    f\"Loss/test_{CURR_TASK}\": final_test_loss,\n",
    "    f\"mIoU/test_{CURR_TASK}\": mean_test_iou\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "test_running_loss = 0\n",
    "CURR_TASK=\"B\"\n",
    "test_loader = tqdm(dataloaders[CURR_TASK][\"test\"], desc=\"Test loop\")\n",
    "\n",
    "BATCH_INDEX = 0\n",
    "table = wandb.Table(columns=[\"ID\", \"Image\"])\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        # Move everything to the device\n",
    "        batch[\"pixel_values\"] = batch[\"pixel_values\"].to(device)\n",
    "        batch[\"pixel_mask\"] = batch[\"pixel_mask\"].to(device)\n",
    "        batch[\"mask_labels\"] = [entry.to(device) for entry in batch[\"mask_labels\"]]\n",
    "        batch[\"class_labels\"] = [entry.to(device) for entry in batch[\"class_labels\"]]\n",
    "        # Compute output and loss\n",
    "        outputs = model(**batch)\n",
    "\n",
    "        loss = outputs.loss\n",
    "        # Record losses\n",
    "        current_loss = loss.item() * batch[\"pixel_values\"].size(0)\n",
    "        test_running_loss += current_loss\n",
    "        test_loader.set_postfix(loss=f\"{current_loss:.4f}\")\n",
    "\n",
    "        # Extract and compute metrics\n",
    "        pred_maps, masks = m2f_extract_pred_maps_and_masks(\n",
    "            batch, outputs, m2f_preprocessor_B\n",
    "        )\n",
    "        metric.add_batch(references=masks, predictions=pred_maps)\n",
    "        if BATCH_INDEX <5:\n",
    "            # Visualize\n",
    "            log_table_of_images(\n",
    "                table, # common table for all batches\n",
    "                batch[\"pixel_values\"],\n",
    "                np.array(CAT1K_PIXEL_MEAN), # remove normalization\n",
    "                np.array(CAT1K_PIXEL_STD), # remove normalization\n",
    "                pred_maps,\n",
    "                masks,\n",
    "                BATCH_INDEX, # correct indexing in table\n",
    "            )\n",
    "            BATCH_INDEX += 1\n",
    "\n",
    "# Log table\n",
    "wandb.log({f\"{CURR_TASK}_TEST_AFTER_TRAINING_A\": table})\n",
    "\n",
    "# After compute the batches that were added are deleted\n",
    "test_metrics_B_before = metric.compute(\n",
    "    num_labels=NUM_CLASSES, ignore_index=BG_VALUE_255, reduce_labels=False\n",
    ")\n",
    "mean_test_iou = test_metrics_B_before[\"mean_iou\"]\n",
    "final_test_loss = test_running_loss / len(dataloaders[CURR_TASK][\"test\"].dataset)\n",
    "wandb.log({\n",
    "    f\"Loss/test_{CURR_TASK}\": final_test_loss,\n",
    "    f\"mIoU/test_{CURR_TASK}\": mean_test_iou\n",
    "})\n",
    "print(f\"Test Loss: {final_test_loss:.4f}, Test mIoU: {mean_test_iou:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train on B now with CL only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"mean_iou\")\n",
    "encoder_params = [\n",
    "    param\n",
    "    for name, param in model.named_parameters()\n",
    "    if name.startswith(\"model.pixel_level_module.encoder\")\n",
    "]\n",
    "decoder_params = [\n",
    "    param\n",
    "    for name, param in model.named_parameters()\n",
    "    if name.startswith(\"model.pixel_level_module.decoder\")\n",
    "]\n",
    "transformer_params = [\n",
    "    param\n",
    "    for name, param in model.named_parameters()\n",
    "    if name.startswith(\"model.transformer_module\")\n",
    "]\n",
    "class_prediction_params=[\n",
    "    param\n",
    "    for name, param in model.named_parameters() \n",
    "    if not name.startswith(\"model.pixel_level_module.encoder\") and not name.startswith(\"model.transformer_module\") and not name.startswith(\"model.pixel_level_module.decoder\")\n",
    "]\n",
    "optimizer = optim.AdamW(\n",
    "    [\n",
    "        {\"params\": encoder_params, \"lr\": BACKBONE_LR},\n",
    "        {\"params\": decoder_params},\n",
    "        {\"params\": transformer_params},\n",
    "        {\"params\": class_prediction_params}\n",
    "    ],\n",
    "    lr=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    ")\n",
    "\n",
    "scheduler = optim.lr_scheduler.PolynomialLR(\n",
    "    optimizer, total_iters=NUM_EPOCHS, power=0.9\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To avoid making stupid errors\n",
    "CURR_TASK = \"B\"\n",
    "if use_prototypes:\n",
    "    os.makedirs(f\"{best_model_dir}{CURR_TASK}_prototypes/\",exist_ok=True)\n",
    "\n",
    "# For storing the model\n",
    "best_val_metric = -np.inf\n",
    "best_model_weights=None # best model weights are stored here\n",
    "best_prototypes=None\n",
    "\n",
    "# Move model to device\n",
    "model.to(device)\n",
    "counter=0\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    train_running_loss = 0.0\n",
    "    val_running_loss = 0.0\n",
    "\n",
    "    # Set up tqdm for the training loop\n",
    "    train_loader = tqdm(\n",
    "        dataloaders[CURR_TASK][\"train\"], desc=f\"Epoch {epoch + 1}/{NUM_EPOCHS} Training\"\n",
    "    )\n",
    "\n",
    "    for batch in train_loader:\n",
    "        # Move everything to the device\n",
    "        batch[\"pixel_values\"] = batch[\"pixel_values\"].to(device)\n",
    "        batch[\"pixel_mask\"] = batch[\"pixel_mask\"].to(device)\n",
    "        batch[\"mask_labels\"] = [entry.to(device) for entry in batch[\"mask_labels\"]]\n",
    "        batch[\"class_labels\"] = [entry.to(device) for entry in batch[\"class_labels\"]]\n",
    "       \n",
    "        outputs = model(**batch,output_hidden_states=True)\n",
    "        # Extract and compute metrics\n",
    "        pred_maps, masks = m2f_extract_pred_maps_and_masks(\n",
    "            batch, outputs, m2f_preprocessor_A\n",
    "        )\n",
    "        metric.add_batch(references=masks, predictions=pred_maps)\n",
    "\n",
    "        feats=get_perpixel_features(outputs.pixel_decoder_hidden_states,outputs.pixel_decoder_last_hidden_state).to(device)\n",
    "        contrastive_loss_output=contrastive_loss(feats,labels=torch.stack(masks,dim=0).to(device),predict=torch.stack(pred_maps,dim=0).to(device))\n",
    "        \n",
    "        loss = outputs.loss + CONTRASTIVE_LOSS_LAMBDA * contrastive_loss_output\n",
    "        \n",
    "        # Compute gradient and perform step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Record losses\n",
    "        current_loss = loss.item() * batch[\"pixel_values\"].size(0)\n",
    "        train_running_loss += current_loss\n",
    "        train_loader.set_postfix(loss=f\"{current_loss:.4f}\")\n",
    "        \n",
    "\n",
    "        \n",
    "    \n",
    "    # After compute the batches that were added are deleted\n",
    "    mean_train_iou = metric.compute(\n",
    "        num_labels=NUM_CLASSES, ignore_index=BG_VALUE_255, reduce_labels=False\n",
    "    )[\"mean_iou\"]\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loader = tqdm(\n",
    "        dataloaders[CURR_TASK][\"val\"], desc=f\"Epoch {epoch + 1}/{NUM_EPOCHS} Validation\"\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            # Move everything to the device\n",
    "            batch[\"pixel_values\"] = batch[\"pixel_values\"].to(device)\n",
    "            batch[\"pixel_mask\"] = batch[\"pixel_mask\"].to(device)\n",
    "            batch[\"mask_labels\"] = [entry.to(device) for entry in batch[\"mask_labels\"]]\n",
    "            batch[\"class_labels\"] = [\n",
    "                entry.to(device) for entry in batch[\"class_labels\"]\n",
    "            ]\n",
    "            # Compute output and loss\n",
    "            outputs = model(**batch)\n",
    "\n",
    "            loss = outputs.loss\n",
    "            # Record losses\n",
    "            current_loss = loss.item() * batch[\"pixel_values\"].size(0)\n",
    "            val_running_loss += current_loss\n",
    "            val_loader.set_postfix(loss=f\"{current_loss:.4f}\")\n",
    "\n",
    "            # Extract and compute metrics\n",
    "            pred_maps, masks = m2f_extract_pred_maps_and_masks(\n",
    "                batch, outputs, m2f_preprocessor_A\n",
    "            )\n",
    "            metric.add_batch(references=masks, predictions=pred_maps)\n",
    "            \n",
    "\n",
    "    # After compute the batches that were added are deleted\n",
    "    mean_val_iou = metric.compute(\n",
    "        num_labels=NUM_CLASSES, ignore_index=BG_VALUE_255, reduce_labels=False\n",
    "    )[\"mean_iou\"]\n",
    "\n",
    "    epoch_train_loss = train_running_loss / len(dataloaders[CURR_TASK][\"train\"].dataset)\n",
    "    epoch_val_loss = val_running_loss / len(dataloaders[CURR_TASK][\"val\"].dataset)\n",
    "\n",
    "    writer.add_scalar(f\"Loss/train_{new_run_name}_{CURR_TASK}\", epoch_train_loss, epoch + 1)\n",
    "    writer.add_scalar(f\"Loss/val_{new_run_name}_{CURR_TASK}\", epoch_val_loss, epoch + 1)\n",
    "    writer.add_scalar(f\"mIoU/train_{new_run_name}_{CURR_TASK}\", mean_train_iou, epoch + 1)\n",
    "    writer.add_scalar(f\"mIoU/val_{new_run_name}_{CURR_TASK}\", mean_val_iou, epoch + 1)\n",
    "\n",
    "    wandb.log({\n",
    "        f\"Loss/train_replay_A_{CURR_TASK}\": epoch_train_loss,\n",
    "        f\"Loss/val_replay_A_{CURR_TASK}\": epoch_val_loss,\n",
    "        f\"mIoU/train_replay_A_{CURR_TASK}\": mean_train_iou,\n",
    "        f\"mIoU/val_replay_A_{CURR_TASK}\": mean_val_iou\n",
    "    })\n",
    "\n",
    "\n",
    "    tqdm.write(\n",
    "        f\"Epoch {epoch + 1}/{NUM_EPOCHS}, Train Loss: {epoch_train_loss:.4f}, Train mIoU: {mean_train_iou:.4f}, Validation Loss: {epoch_val_loss:.4f}, Validation mIoU: {mean_val_iou:.4f}\"\n",
    "    )\n",
    "    \n",
    "    if mean_val_iou > best_val_metric:\n",
    "        best_val_metric = mean_val_iou\n",
    "        model.save_pretrained(f\"{best_model_dir}{CURR_TASK}/\")\n",
    "        best_model_weights = deepcopy(model.state_dict())\n",
    "        counter=0\n",
    "        if use_prototypes:\n",
    "            best_prototypes=deepcopy(contrastive_loss.prototypes)\n",
    "            torch.save(best_prototypes, f\"{best_model_dir}{CURR_TASK}_prototypes/prototypes.pth\")\n",
    "        \n",
    "    else:\n",
    "        counter+=1\n",
    "        if counter == PATIENCE:\n",
    "            print(\"Early stopping at epoch\",epoch)\n",
    "            break\n",
    "            \n",
    "os.makedirs(f\"{best_model_dir}{CURR_TASK}/\",exist_ok=True)\n",
    "artifact = wandb.Artifact(f\"best_model_{new_run_name}\", type=\"model\")\n",
    "trained_model_path=f\"{best_model_dir}{CURR_TASK}/best_model_{new_run_name}.pth\"\n",
    "artifact.add_file(trained_model_path, torch.save(best_model_weights, trained_model_path))\n",
    "\n",
    "if use_prototypes:\n",
    "    prototypes_path = f\"{best_model_dir}{CURR_TASK}/prototypes_{new_run_name}.pth\"\n",
    "    artifact.add_file(prototypes_path,torch.save(best_prototypes,prototypes_path))\n",
    "    \n",
    "wandb.run.log_artifact(artifact)\n",
    "\n",
    "if os.path.exists(trained_model_path):\n",
    "    os.remove(trained_model_path)\n",
    "\n",
    "if use_prototypes:\n",
    "    if os.path.exists(prototypes_path):\n",
    "        os.remove(prototypes_path)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = Mask2FormerForUniversalSegmentation.from_pretrained(f\"{best_model_dir}{CURR_TASK}/\").to(device)\n",
    "\n",
    "# Load best model and evaluate on test\n",
    "# Construct the artifact path\n",
    "artifact_path = f\"{user_or_team}/{project_name}/best_model_{new_run_name}:latest\"\n",
    "\n",
    "# Load from W&B\n",
    "api = wandb.Api()\n",
    "artifact=api.artifact(artifact_path)\n",
    "artifact_dir=artifact.download()\n",
    "model_state_dict_path = os.path.join(artifact_dir, f\"best_model_{new_run_name}.pth\" )\n",
    "model_state_dict = torch.load(model_state_dict_path,map_location=device)\n",
    "model = Mask2FormerForUniversalSegmentation(mask2former_config)\n",
    "model.load_state_dict(model_state_dict)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test on B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "test_running_loss = 0\n",
    "CURR_TASK=\"B\"\n",
    "test_loader = tqdm(dataloaders[CURR_TASK][\"test\"], desc=\"Test loop\")\n",
    "BATCH_INDEX = 0\n",
    "table = wandb.Table(columns=[\"ID\", \"Image\"])\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        # Move everything to the device\n",
    "        batch[\"pixel_values\"] = batch[\"pixel_values\"].to(device)\n",
    "        batch[\"pixel_mask\"] = batch[\"pixel_mask\"].to(device)\n",
    "        batch[\"mask_labels\"] = [entry.to(device) for entry in batch[\"mask_labels\"]]\n",
    "        batch[\"class_labels\"] = [entry.to(device) for entry in batch[\"class_labels\"]]\n",
    "        # Compute output and loss\n",
    "        outputs = model(**batch)\n",
    "\n",
    "        loss = outputs.loss\n",
    "        # Record losses\n",
    "        current_loss = loss.item() * batch[\"pixel_values\"].size(0)\n",
    "        test_running_loss += current_loss\n",
    "        test_loader.set_postfix(loss=f\"{current_loss:.4f}\")\n",
    "\n",
    "        # Extract and compute metrics\n",
    "        pred_maps, masks = m2f_extract_pred_maps_and_masks(\n",
    "            batch, outputs, m2f_preprocessor_B\n",
    "        )\n",
    "        metric.add_batch(references=masks, predictions=pred_maps)\n",
    "        if BATCH_INDEX <5:\n",
    "            # Visualize\n",
    "            log_table_of_images(\n",
    "                table, # common table for all batches\n",
    "                batch[\"pixel_values\"],\n",
    "                pixel_mean_B, # remove normalization\n",
    "                pixel_std_B, # remove normalization\n",
    "                pred_maps,\n",
    "                masks,\n",
    "                BATCH_INDEX, # correct indexing in table\n",
    "            )\n",
    "            BATCH_INDEX += 1\n",
    "# Log table\n",
    "wandb.log({f\"{CURR_TASK}_TEST_AFTER_TRAINING_B\": table})\n",
    "\n",
    "# After compute the batches that were added are deleted\n",
    "test_metrics_B = metric.compute(\n",
    "    num_labels=NUM_CLASSES, ignore_index=BG_VALUE_255, reduce_labels=False\n",
    ")\n",
    "mean_test_iou = test_metrics_B[\"mean_iou\"]\n",
    "final_test_loss = test_running_loss / len(dataloaders[CURR_TASK][\"test\"].dataset)\n",
    "wandb.log({\n",
    "    f\"Loss/test_{CURR_TASK}\": final_test_loss,\n",
    "    f\"mIoU/test_{CURR_TASK}\": mean_test_iou\n",
    "})\n",
    "print(f\"Test Loss: {final_test_loss:.4f}, Test mIoU: {mean_test_iou:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test on A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To avoid making stupid errors\n",
    "CURR_TASK = \"A\"\n",
    "\n",
    "model.eval()\n",
    "test_running_loss = 0\n",
    "test_loader = tqdm(dataloaders[CURR_TASK][\"test\"], desc=\"Test loop\")\n",
    "BATCH_INDEX = 0\n",
    "table = wandb.Table(columns=[\"ID\", \"Image\"])\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        # Move everything to the device\n",
    "        batch[\"pixel_values\"] = batch[\"pixel_values\"].to(device)\n",
    "        batch[\"pixel_mask\"] = batch[\"pixel_mask\"].to(device)\n",
    "        batch[\"mask_labels\"] = [entry.to(device) for entry in batch[\"mask_labels\"]]\n",
    "        batch[\"class_labels\"] = [entry.to(device) for entry in batch[\"class_labels\"]]\n",
    "        # Compute output and loss\n",
    "        outputs = model(**batch)\n",
    "\n",
    "        loss = outputs.loss\n",
    "        # Record losses\n",
    "        current_loss = loss.item() * batch[\"pixel_values\"].size(0)\n",
    "        test_running_loss += current_loss\n",
    "        test_loader.set_postfix(loss=f\"{current_loss:.4f}\")\n",
    "\n",
    "        # Extract and compute metrics\n",
    "        pred_maps, masks = m2f_extract_pred_maps_and_masks(\n",
    "            batch, outputs, m2f_preprocessor_A\n",
    "        )\n",
    "        metric.add_batch(references=masks, predictions=pred_maps)\n",
    "        if BATCH_INDEX <5:\n",
    "            # Visualize\n",
    "            log_table_of_images(\n",
    "                table, # common table for all batches\n",
    "                batch[\"pixel_values\"],\n",
    "                pixel_mean_A, # remove normalization\n",
    "                pixel_std_A, # remove normalization\n",
    "                pred_maps,\n",
    "                masks,\n",
    "                BATCH_INDEX, # correct indexing in table\n",
    "            )\n",
    "            BATCH_INDEX += 1\n",
    "# Log table\n",
    "wandb.log({f\"{CURR_TASK}_TEST_AFTER_TRAINING_B\": table})\n",
    "\n",
    "# After compute the batches that were added are deleted\n",
    "test_metrics_forgetting_A = metric.compute(\n",
    "    num_labels=NUM_CLASSES, ignore_index=BG_VALUE_255, reduce_labels=False\n",
    ")\n",
    "mean_test_iou = test_metrics_forgetting_A[\"mean_iou\"]\n",
    "final_test_loss = test_running_loss / len(dataloaders[CURR_TASK][\"test\"].dataset)\n",
    "wandb.log({\n",
    "    f\"Loss/test_naive_forgetting_{CURR_TASK}\": final_test_loss,\n",
    "    f\"mIoU/test_naive_forgetting_{CURR_TASK}\": mean_test_iou\n",
    "})\n",
    "print(f\"Test Loss: {final_test_loss:.4f}, Test mIoU: {mean_test_iou:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect overall mIoU\n",
    "mIoU_A_before = test_metrics_A[\"mean_iou\"]\n",
    "mIoU_B_before=test_metrics_B_before[\"mean_iou\"]\n",
    "mIoU_forgetting_A = test_metrics_forgetting_A[\"mean_iou\"]\n",
    "mIoU_B = test_metrics_B[\"mean_iou\"]\n",
    "\n",
    "# Collect per category mIoU\n",
    "per_category_mIoU_A_before = np.array(test_metrics_A[\"per_category_iou\"])\n",
    "per_category_mIoU_A = np.array(test_metrics_forgetting_A[\"per_category_iou\"])\n",
    "per_category_mIoU_B = np.array(test_metrics_B[\"per_category_iou\"])\n",
    "per_category_mIoU_B_before=np.array(test_metrics_B_before[\"per_category_iou\"])\n",
    "\n",
    "# Average learning accuracies (mIoUs)\n",
    "avg_learning_acc = (mIoU_A_before + mIoU_B) / 2\n",
    "per_category_avg_learning_acc = (per_category_mIoU_A_before + per_category_mIoU_B) / 2\n",
    "\n",
    "# Forgetting\n",
    "total_forgetting = mIoU_A_before - mIoU_forgetting_A\n",
    "per_category_forgetting = (per_category_mIoU_A_before - per_category_mIoU_A)\n",
    "\n",
    "# Export evaluation metrics to WandB\n",
    "wandb.log({\n",
    "    \"eval/avg_learning_acc\": avg_learning_acc,\n",
    "    \"eval/total_forgetting\": total_forgetting,\n",
    "})\n",
    "\n",
    "columns=[\"categories\",\"per_category_mIoU_A_before\",\"per_category_mIoU_B_before\",\n",
    "         \"per_category_mIoU_B\", \"per_category_mIoU_A\",\n",
    "         \"per_category_avg_learning_acc\",\"per_category_forgetting\"]\n",
    "data=[]\n",
    "\n",
    "data.append([\"background\",per_category_mIoU_A_before[0],\n",
    "                 per_category_mIoU_B_before[0],\n",
    "                 per_category_mIoU_B[0],\n",
    "                per_category_mIoU_A[0],per_category_avg_learning_acc[0],\n",
    "                per_category_forgetting[0]])\n",
    "\n",
    "for cat_id in range(1,12):\n",
    "    data.append([ZEISS_CATEGORIES[cat_id],per_category_mIoU_A_before[cat_id],\n",
    "                 per_category_mIoU_B_before[cat_id],\n",
    "                 per_category_mIoU_B[cat_id],\n",
    "                per_category_mIoU_A[cat_id],per_category_avg_learning_acc[cat_id],\n",
    "                per_category_forgetting[cat_id]])\n",
    "    \n",
    "    \n",
    "table = wandb.Table(columns=columns, data=data)\n",
    "wandb.log({\"per_category_metrics_table\": table})\n",
    "\n",
    "print(\"**** Overall mIoU ****\")\n",
    "print(f\"mIoU on task A before training on B: {mIoU_A_before}\")\n",
    "print(f\"mIoU on task B before training on B: {mIoU_B_before}\")\n",
    "print(\"\\n\")\n",
    "print(f\"mIoU on task B after training on B: {mIoU_B}\")\n",
    "print(f\"mIoU on task A after training on B: {mIoU_forgetting_A}\")\n",
    "\n",
    "print(\"\\n**** Per category mIoU ****\")\n",
    "print(f\"Per category mIoU on task A before training on B: {per_category_mIoU_A_before}\")\n",
    "print(f\"Per category mIoU on task B before training on B: {per_category_mIoU_B_before}\")\n",
    "print(\"\\n\")\n",
    "print(f\"Per category mIoU on task B after training on B: {per_category_mIoU_B}\")\n",
    "print(f\"Per category mIoU on task A after training on B: {per_category_mIoU_A}\")\n",
    "\n",
    "print(\"\\n**** Average learning accuracies ****\")\n",
    "print(f\"Average learning acc.: {avg_learning_acc}\")\n",
    "print(f\"Per category Average learning acc.: {per_category_avg_learning_acc}\")\n",
    "\n",
    "print(\"\\n**** Forgetting ****\")\n",
    "print(f\"Total forgetting: {total_forgetting}\")\n",
    "print(f\"Per category forgetting: {per_category_forgetting}\")\n",
    "wandb.finish()\n",
    "\n",
    "if os.path.exists(\"artifacts/\"):\n",
    "    shutil.rmtree(\"artifacts/\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "continual-learning-UoKZrllX-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
