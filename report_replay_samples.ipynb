{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from utils.common import (\n",
    "    m2f_dataset_collate,\n",
    "    m2f_extract_pred_maps_and_masks,\n",
    "    set_seed,\n",
    "    pixel_mean_std,\n",
    "    CADIS_PIXEL_MEAN,\n",
    "    CADIS_PIXEL_STD,\n",
    "    CAT1K_PIXEL_MEAN,\n",
    "    CAT1K_PIXEL_STD\n",
    ")\n",
    "from utils.dataset_utils import (\n",
    "    get_cadisv2_dataset,\n",
    "    get_cataract1k_dataset,\n",
    "    ZEISS_CATEGORIES,\n",
    ")\n",
    "from utils.medical_datasets import Mask2FormerDataset\n",
    "from transformers import (\n",
    "    Mask2FormerForUniversalSegmentation,\n",
    "    SwinModel,\n",
    "    SwinConfig,\n",
    "    Mask2FormerConfig,\n",
    "    AutoImageProcessor,\n",
    "    Mask2FormerImageProcessor,\n",
    ")\n",
    "from torch.utils.data import DataLoader\n",
    "import evaluate\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "import wandb\n",
    "from sklearn.cluster import KMeans\n",
    "import umap\n",
    "from scipy.spatial.distance import cdist\n",
    "from copy import deepcopy\n",
    "import shutil\n",
    "import random\n",
    "from utils.wandb_utils import log_table_of_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "NUM_CLASSES = len(ZEISS_CATEGORIES) - 3  + 1 # Remove class incremental and add background !!!\n",
    "SWIN_BACKBONE = \"microsoft/swin-tiny-patch4-window7-224\"#\"microsoft/swin-large-patch4-window12-384\"\n",
    "\n",
    "# Download pretrained swin model\n",
    "swin_model = SwinModel.from_pretrained(\n",
    "    SWIN_BACKBONE, out_features=[\"stage1\", \"stage2\", \"stage3\", \"stage4\"]\n",
    ")\n",
    "swin_config = SwinConfig.from_pretrained(\n",
    "    SWIN_BACKBONE, out_features=[\"stage1\", \"stage2\", \"stage3\", \"stage4\"]\n",
    ")\n",
    "\n",
    "# Create Mask2Former configuration based on Swin's configuration\n",
    "mask2former_config = Mask2FormerConfig(\n",
    "    backbone_config=swin_config, num_labels=NUM_CLASSES #, ignore_value=BG_VALUE\n",
    ")\n",
    "\n",
    "# Create the Mask2Former model with this configuration\n",
    "model = Mask2FormerForUniversalSegmentation(mask2former_config)\n",
    "\n",
    "# Reuse pretrained parameters\n",
    "for swin_param, m2f_param in zip(\n",
    "    swin_model.named_parameters(),\n",
    "    model.model.pixel_level_module.encoder.named_parameters(),\n",
    "):\n",
    "    m2f_param_name = f\"model.pixel_level_module.encoder.{m2f_param[0]}\"\n",
    "\n",
    "    if swin_param[0] == m2f_param[0]:\n",
    "        model.state_dict()[m2f_param_name].copy_(swin_param[1])\n",
    "        continue\n",
    "\n",
    "    print(f\"Not Matched: {m2f_param[0]} != {swin_param[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Helper function to load datasets\n",
    "def load_dataset(dataset_getter, data_path, domain_incremental):\n",
    "    return dataset_getter(data_path, domain_incremental=domain_incremental)\n",
    "\n",
    "\n",
    "# Helper function to create dataloaders for a dataset\n",
    "def create_dataloaders(\n",
    "    dataset, batch_size, shuffle, num_workers, drop_last, pin_memory, collate_fn\n",
    "):\n",
    "    return {\n",
    "        \"train\": DataLoader(\n",
    "            dataset[\"train\"],\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle,\n",
    "            num_workers=num_workers,\n",
    "            drop_last=drop_last,\n",
    "            pin_memory=pin_memory,\n",
    "            collate_fn=collate_fn,\n",
    "        ),\n",
    "        \"val\": DataLoader(\n",
    "            dataset[\"val\"],\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle,\n",
    "            num_workers=num_workers,\n",
    "            drop_last=drop_last,\n",
    "            pin_memory=pin_memory,\n",
    "            collate_fn=collate_fn,\n",
    "        ),\n",
    "        \"test\": DataLoader(\n",
    "            dataset[\"test\"],\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=num_workers,\n",
    "            drop_last=False,\n",
    "            pin_memory=pin_memory,\n",
    "            collate_fn=collate_fn,\n",
    "        ),\n",
    "    }\n",
    "\n",
    "\n",
    "# Load datasets\n",
    "datasets = {\n",
    "    \"A\": load_dataset(get_cadisv2_dataset, \"../../storage/data/CaDISv2\", True),\n",
    "    \"B\": load_dataset(get_cataract1k_dataset, \"../../storage/data/cataract-1k\", True),\n",
    "}\n",
    "\n",
    "# pixel_mean_A,pixel_std_A=pixel_mean_std(datasets[\"A\"][0])\n",
    "# print(\"pixel mean of A\",pixel_mean_A,\"pixel std:\",pixel_std_A)\n",
    "\n",
    "pixel_mean_A=np.array(CADIS_PIXEL_MEAN)\n",
    "pixel_std_A=np.array(CADIS_PIXEL_STD)\n",
    "pixel_mean_B=np.array(CAT1K_PIXEL_MEAN)\n",
    "pixel_std_B=np.array(CAT1K_PIXEL_STD)\n",
    "\n",
    "\n",
    "# Calculate the byte size of one sample (image + mask)\n",
    "def calculate_sample_size(image, mask):\n",
    "    image_size = image.numel() * image.element_size()  # Number of elements * bytes per element (for RGB)\n",
    "    \n",
    "    # if mask sizes also need to be taken into account, uncomment the below 2 lines!!\n",
    "    #mask_size = mask.numel() * mask.element_size()  \n",
    "    #return image_size + mask_size\n",
    "    \n",
    "    return image_size\n",
    "\n",
    "\n",
    "# Function to sample without replacement until target size is reached\n",
    "def sample_until_target_size(dataset, target_size_bytes):\n",
    "    sampled_indices = []\n",
    "    cumulative_size = 0\n",
    "\n",
    "    indices = list(range(len(dataset)))\n",
    "    random.shuffle(indices)\n",
    "\n",
    "    for idx in indices:\n",
    "        image, mask = dataset[idx]\n",
    "        sample_size = calculate_sample_size(image, mask)\n",
    "        if cumulative_size + sample_size <= target_size_bytes:\n",
    "            sampled_indices.append(idx)\n",
    "            cumulative_size += sample_size\n",
    "        else:\n",
    "            break\n",
    "    print(\"cumulative size:\",cumulative_size)\n",
    "    return sampled_indices\n",
    "\n",
    "\n",
    "# Target size in bytes (32MB)\n",
    "target_size_bytes = 32 * 1024 * 1024\n",
    "\n",
    "# Get the sampled indices\n",
    "sampled_indices = sample_until_target_size(datasets[\"A\"][0], target_size_bytes)\n",
    "N=len(sampled_indices) # will be used in the secod part of sampling\n",
    "# subset_A = torch.utils.data.Subset(datasets[\"A\"][0], sampled_indices)\n",
    "# new_train = torch.utils.data.ConcatDataset([subset_A, datasets[\"B\"][0]])\n",
    "\n",
    "# pixel_mean_B,pixel_std_B=pixel_mean_std(new_train)\n",
    "# print(\"pixel mean of B\",pixel_mean_B,\"pixel std:\",pixel_std_B)\n",
    "\n",
    "\n",
    "# datasets[\"B\"] = (new_train, datasets[\"B\"][1], datasets[\"B\"][2])\n",
    "\n",
    "# set_seed(42) # seed everything\n",
    "\n",
    "# Define preprocessor\n",
    "swin_processor = AutoImageProcessor.from_pretrained(SWIN_BACKBONE)\n",
    "m2f_preprocessor_A = Mask2FormerImageProcessor(\n",
    "    reduce_labels=False,\n",
    "    ignore_index=255,\n",
    "    do_resize=False,\n",
    "    do_rescale=False,\n",
    "    do_normalize=True,\n",
    "    image_std=pixel_std_A,\n",
    "    image_mean=pixel_mean_A,\n",
    ")\n",
    "\n",
    "m2f_preprocessor_B = Mask2FormerImageProcessor(\n",
    "    reduce_labels=False,\n",
    "    ignore_index=255,\n",
    "    do_resize=False,\n",
    "    do_rescale=False,\n",
    "    do_normalize=True,\n",
    "    image_std=pixel_std_B,\n",
    "    image_mean=pixel_mean_B,\n",
    ")\n",
    "# Create Mask2Former Datasets\n",
    "\n",
    "m2f_datasets = {\n",
    "    \"A\": {\n",
    "        \"train\": Mask2FormerDataset(datasets[\"A\"][0], m2f_preprocessor_A),\n",
    "        \"val\": Mask2FormerDataset(datasets[\"A\"][1], m2f_preprocessor_A),\n",
    "        \"test\": Mask2FormerDataset(datasets[\"A\"][2], m2f_preprocessor_A),\n",
    "    },\n",
    "    \"B\": {\n",
    "        \"train\": Mask2FormerDataset(datasets[\"B\"][0], m2f_preprocessor_B),\n",
    "        \"val\": Mask2FormerDataset(datasets[\"B\"][1], m2f_preprocessor_B),\n",
    "        \"test\": Mask2FormerDataset(datasets[\"B\"][2], m2f_preprocessor_B),\n",
    "    },\n",
    "}\n",
    "\n",
    "# DataLoader parameters\n",
    "N_WORKERS = 4\n",
    "BATCH_SIZE = 16\n",
    "SHUFFLE = True\n",
    "DROP_LAST = True\n",
    "\n",
    "dataloader_params = {\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"shuffle\": SHUFFLE,\n",
    "    \"num_workers\": N_WORKERS,\n",
    "    \"drop_last\": DROP_LAST,\n",
    "    \"pin_memory\": True,\n",
    "    \"collate_fn\": m2f_dataset_collate,\n",
    "}\n",
    "\n",
    "# Create DataLoaders\n",
    "dataloaders = {\n",
    "    key: create_dataloaders(m2f_datasets[key], **dataloader_params)\n",
    "    for key in m2f_datasets\n",
    "}\n",
    "\n",
    "print(dataloaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m2f_preprocessor_A.reduce_labels, m2f_preprocessor_A.ignore_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Check if CUDA is available, otherwise use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "BG_VALUE_255=255\n",
    "base_run_name=\"M2F-Swin-Tiny-Train_Cadis\"\n",
    "new_run_name=\"Replay-Samples-Visualization\"\n",
    "project_name = \"M2F_latest\"\n",
    "user_or_team = \"continual-learning-tum\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Tensorboard setup\n",
    "out_dir=\"outputs/\"\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "if not os.path.exists(out_dir+\"runs\"):\n",
    "    os.makedirs(out_dir+\"runs\")\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir outputs/runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!CUDA_LAUNCH_BLOCKING=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorboard logging\n",
    "writer = SummaryWriter(log_dir=out_dir + \"runs\")\n",
    "\n",
    "# Model checkpointing\n",
    "model_dir = out_dir + \"models/\"\n",
    "if not os.path.exists(model_dir):\n",
    "    print(\"Store weights in: \", model_dir)\n",
    "    os.makedirs(model_dir)\n",
    "\n",
    "best_model_dir = model_dir + f\"{base_run_name}/best_model/\"\n",
    "if not os.path.exists(best_model_dir):\n",
    "    print(\"Store best model weights in: \", best_model_dir)\n",
    "    os.makedirs(best_model_dir)\n",
    "final_model_dir = model_dir + f\"{base_run_name}/final_model/\"\n",
    "if not os.path.exists(final_model_dir):\n",
    "    print(\"Store final model weights in: \", final_model_dir)\n",
    "    os.makedirs(final_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WandB for team usage !!!!\n",
    "\n",
    "wandb.login() # use this one if a different person is going to run the notebook\n",
    "#wandb.login(relogin=False) # if the same person in the last run is going to run the notebook again\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First train on dataset A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.init(\n",
    "    project=project_name,\n",
    "    name=new_run_name,\n",
    "    notes=\"Visualizing samples from different replay methods\"\n",
    ")\n",
    "print(\"wandb run id:\",wandb.run.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model and evaluate on test\n",
    "\n",
    "#model = Mask2FormerForUniversalSegmentation.from_pretrained(f\"{best_model_dir}{CURR_TASK}/\").to(device)\n",
    "\n",
    "# Construct the artifact path\n",
    "artifact_path = f\"{user_or_team}/{project_name}/best_model_{base_run_name}:latest\"\n",
    "\n",
    "# Load from W&B\n",
    "api = wandb.Api()\n",
    "artifact=api.artifact(artifact_path)\n",
    "model_dir=artifact.download()\n",
    "model_state_dict_path = os.path.join(model_dir, f\"best_model_{base_run_name}.pth\" )\n",
    "model_state_dict = torch.load(model_state_dict_path)\n",
    "model = Mask2FormerForUniversalSegmentation(mask2former_config)\n",
    "model.load_state_dict(model_state_dict)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mean Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "losses = []\n",
    "encoder_samples = []\n",
    "\n",
    "# Collect losses and samples\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    for sample in tqdm(m2f_datasets[\"A\"][\"train\"]):\n",
    "        sample[\"pixel_values\"] = sample[\"pixel_values\"].to(device)\n",
    "        sample[\"pixel_mask\"] = sample[\"pixel_mask\"].to(device)\n",
    "        sample[\"mask_labels\"] = [entry.to(device) for entry in sample[\"mask_labels\"]]\n",
    "        sample[\"class_labels\"] = [entry.to(device) for entry in sample[\"class_labels\"]]\n",
    "        outputs = model(**sample)\n",
    "        losses.append(outputs.loss.item())\n",
    "        encoder_samples.append(outputs.encoder_last_hidden_state.cpu())\n",
    "\n",
    "losses_np = np.array(losses)\n",
    "\n",
    "# Sample images with mean loss\n",
    "mean_loss = np.mean(losses_np)\n",
    "differences = np.abs(losses_np - mean_loss)\n",
    "closest_indices = np.argsort(differences)[:N] # N was calculated above\n",
    "\n",
    "# Create a subset of B using the mean loss sampled indices\n",
    "subset_A = [m2f_datasets[\"A\"][\"train\"][i] for i in closest_indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "table = wandb.Table(columns=[\"ID\", \"Image\"])\n",
    "model.eval()\n",
    "for i, batch in tqdm(enumerate(subset_A)):\n",
    "    batch[\"pixel_values\"] = batch[\"pixel_values\"].to(device)\n",
    "    batch[\"pixel_mask\"] = batch[\"pixel_mask\"].to(device)\n",
    "    batch[\"mask_labels\"] = [entry.to(device) for entry in batch[\"mask_labels\"]]\n",
    "    batch[\"class_labels\"] = [entry.to(device) for entry in batch[\"class_labels\"]]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "    \n",
    "    pred_maps, masks = m2f_extract_pred_maps_and_masks(\n",
    "        batch, outputs, m2f_preprocessor_A\n",
    "    )\n",
    "\n",
    "    log_table_of_images(\n",
    "        table,\n",
    "        batch[\"pixel_values\"],\n",
    "        pixel_mean_A,\n",
    "        pixel_std_A,\n",
    "        pred_maps,\n",
    "        masks,\n",
    "        i\n",
    "    )\n",
    "\n",
    "wandb.log({\"Mean_Loss_Samples\": table})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Min Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample images with min loss\n",
    "closest_indices = np.argsort(losses_np)[:N] # N was calculated above\n",
    "\n",
    "# Create a subset of B using the mean loss sampled indices\n",
    "subset_A = [m2f_datasets[\"A\"][\"train\"][i] for i in closest_indices]\n",
    "\n",
    "table = wandb.Table(columns=[\"ID\", \"Image\"])\n",
    "model.eval()\n",
    "for i, batch in tqdm(enumerate(subset_A)):\n",
    "    batch[\"pixel_values\"] = batch[\"pixel_values\"].to(device)\n",
    "    batch[\"pixel_mask\"] = batch[\"pixel_mask\"].to(device)\n",
    "    batch[\"mask_labels\"] = [entry.to(device) for entry in batch[\"mask_labels\"]]\n",
    "    batch[\"class_labels\"] = [entry.to(device) for entry in batch[\"class_labels\"]]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "    \n",
    "    pred_maps, masks = m2f_extract_pred_maps_and_masks(\n",
    "        batch, outputs, m2f_preprocessor_A\n",
    "    )\n",
    "\n",
    "    log_table_of_images(\n",
    "        table,\n",
    "        batch[\"pixel_values\"],\n",
    "        pixel_mean_A,\n",
    "        pixel_std_A,\n",
    "        pred_maps,\n",
    "        masks,\n",
    "        i\n",
    "    )\n",
    "\n",
    "wandb.log({\"Min_Loss_Samples\": table})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Max Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample images with max loss\n",
    "closest_indices = np.argsort(losses_np)[-N:] # N was calculated above\n",
    "\n",
    "# Create a subset of B using the mean loss sampled indices\n",
    "subset_A = [m2f_datasets[\"A\"][\"train\"][i] for i in closest_indices]\n",
    "\n",
    "table = wandb.Table(columns=[\"ID\", \"Image\"])\n",
    "model.eval()\n",
    "for i, batch in tqdm(enumerate(subset_A)):\n",
    "    batch[\"pixel_values\"] = batch[\"pixel_values\"].to(device)\n",
    "    batch[\"pixel_mask\"] = batch[\"pixel_mask\"].to(device)\n",
    "    batch[\"mask_labels\"] = [entry.to(device) for entry in batch[\"mask_labels\"]]\n",
    "    batch[\"class_labels\"] = [entry.to(device) for entry in batch[\"class_labels\"]]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "    \n",
    "    pred_maps, masks = m2f_extract_pred_maps_and_masks(\n",
    "        batch, outputs, m2f_preprocessor_A\n",
    "    )\n",
    "\n",
    "    log_table_of_images(\n",
    "        table,\n",
    "        batch[\"pixel_values\"],\n",
    "        pixel_mean_A,\n",
    "        pixel_std_A,\n",
    "        pred_maps,\n",
    "        masks,\n",
    "        i\n",
    "    )\n",
    "\n",
    "wandb.log({\"Max_Loss_Samples\": table})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RSS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_samples_np = np.concatenate(encoder_samples)\n",
    "print(f\"Samples shape: {encoder_samples_np.shape}\")\n",
    "\n",
    "# ================== HYPERPARAMETERS ==================#\n",
    "# Number of UMAP components (not specified in the paper)\n",
    "N_COMPONENTS = 2\n",
    "\n",
    "# Number of clusters (M in the paper but not specified explicitly)\n",
    "N_CLUSTERS = 11\n",
    "\n",
    "# Number of closest samples per cluster (derived from N)\n",
    "N_PER_CLUSTER = int(N / N_CLUSTERS)\n",
    "# =====================================================#\n",
    "\n",
    "# Flatten the data for each sample\n",
    "n_samples, dim1, dim2, dim3 = encoder_samples_np.shape\n",
    "flattened_data = encoder_samples_np.reshape(n_samples, dim1 * dim2 * dim3)\n",
    "\n",
    "# Apply UMAP to reduce dimensionality\n",
    "umap_reducer = umap.UMAP(\n",
    "    n_components=N_COMPONENTS, random_state=42\n",
    ")  # You can adjust n_components as needed\n",
    "reduced_data = umap_reducer.fit_transform(flattened_data)\n",
    "\n",
    "# Perform clustering using KMeans\n",
    "kmeans = KMeans(n_clusters=N_CLUSTERS, random_state=42)\n",
    "cluster_labels = kmeans.fit_predict(reduced_data)\n",
    "\n",
    "# Calculate the distance of each sample to its assigned cluster centroid\n",
    "centroids = kmeans.cluster_centers_\n",
    "distances = cdist(reduced_data, centroids, \"euclidean\")\n",
    "sample_distances = distances[np.arange(n_samples), cluster_labels]\n",
    "\n",
    "closest_indices_per_cluster = []\n",
    "\n",
    "for cluster in range(N_CLUSTERS):\n",
    "    # Get indices of samples in the current cluster\n",
    "    cluster_indices = np.where(cluster_labels == cluster)[0]\n",
    "\n",
    "    # Get distances of these samples to the cluster centroid\n",
    "    cluster_distances = distances[cluster_indices, cluster]\n",
    "\n",
    "    # Find the indices of the N_per_cluster closest samples\n",
    "    closest_indices = cluster_indices[np.argsort(cluster_distances)[:N_PER_CLUSTER]]\n",
    "    closest_indices_per_cluster.extend(closest_indices)\n",
    "\n",
    "# Output the indices of the closest samples per cluster\n",
    "closest_indices_per_cluster = np.array(closest_indices_per_cluster)\n",
    "print(f\"Indicies for the sampled images: {closest_indices_per_cluster}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "scatter = plt.scatter(reduced_data[:, 0], reduced_data[:, 1], c=cluster_labels, cmap='viridis', s=50)\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1], c='red', marker='X', s=200, label='Centroids')\n",
    "plt.colorbar(scatter, label='Cluster Label')\n",
    "plt.xlabel('UMAP Dimension 1')\n",
    "plt.ylabel('UMAP Dimension 2')\n",
    "plt.title('UMAP Reduction with KMeans Clustering')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "subset_A = [m2f_datasets[\"A\"][\"train\"][i] for i in closest_indices_per_cluster]\n",
    "\n",
    "table = wandb.Table(columns=[\"ID\", \"Image\"])\n",
    "model.eval()\n",
    "for i, batch in tqdm(enumerate(subset_A)):\n",
    "    batch[\"pixel_values\"] = batch[\"pixel_values\"].to(device)\n",
    "    batch[\"pixel_mask\"] = batch[\"pixel_mask\"].to(device)\n",
    "    batch[\"mask_labels\"] = [entry.to(device) for entry in batch[\"mask_labels\"]]\n",
    "    batch[\"class_labels\"] = [entry.to(device) for entry in batch[\"class_labels\"]]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**batch)\n",
    "    \n",
    "    pred_maps, masks = m2f_extract_pred_maps_and_masks(\n",
    "        batch, outputs, m2f_preprocessor_A\n",
    "    )\n",
    "\n",
    "    log_table_of_images(\n",
    "        table,\n",
    "        batch[\"pixel_values\"],\n",
    "        pixel_mean_A,\n",
    "        pixel_std_A,\n",
    "        pred_maps,\n",
    "        masks,\n",
    "        i\n",
    "    )\n",
    "\n",
    "wandb.log({\"RSS_Samples\": table})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
