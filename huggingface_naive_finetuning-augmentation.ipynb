{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/notebooks/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import os\n",
    "from utils.common import (\n",
    "    m2f_dataset_collate,\n",
    "    m2f_extract_pred_maps_and_masks,\n",
    "    set_seed,\n",
    "    pixel_mean_std,\n",
    ")\n",
    "from utils.dataset_utils import (\n",
    "    get_cadisv2_dataset,\n",
    "    get_cataract1k_dataset,\n",
    "    ZEISS_CATEGORIES,\n",
    ")\n",
    "from utils.medical_datasets import Mask2FormerDataset\n",
    "from transformers import (\n",
    "    Mask2FormerForUniversalSegmentation,\n",
    "    SwinModel,\n",
    "    SwinConfig,\n",
    "    Mask2FormerConfig,\n",
    "    AutoImageProcessor,\n",
    "    Mask2FormerImageProcessor\n",
    ")\n",
    "from torch.utils.data import DataLoader\n",
    "import evaluate\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "from dotenv import load_dotenv\n",
    "import wandb\n",
    "from utils.augmentations import (train_transforms_noise,\n",
    "                                 train_transforms_noise_no_distortion,\n",
    "                                 train_transforms_blur)\n",
    "from copy import deepcopy\n",
    "import shutil\n",
    "from utils.wandb_utils import log_table_of_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set as 42\n"
     ]
    }
   ],
   "source": [
    "set_seed(42) # seed everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/notebooks/venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not Matched: hidden_states_norms.stage1.weight != layernorm.weight\n",
      "Not Matched: hidden_states_norms.stage1.bias != layernorm.bias\n"
     ]
    }
   ],
   "source": [
    "NUM_CLASSES = len(ZEISS_CATEGORIES) - 3 + 1 # Remove class incremental add background !!!\n",
    "SWIN_BACKBONE = \"microsoft/swin-tiny-patch4-window7-224\"#\"microsoft/swin-large-patch4-window12-384\"\n",
    "\n",
    "# Download pretrained swin model\n",
    "swin_model = SwinModel.from_pretrained(\n",
    "    SWIN_BACKBONE, out_features=[\"stage1\", \"stage2\", \"stage3\", \"stage4\"]\n",
    ")\n",
    "swin_config = SwinConfig.from_pretrained(\n",
    "    SWIN_BACKBONE, out_features=[\"stage1\", \"stage2\", \"stage3\", \"stage4\"]\n",
    ")\n",
    "\n",
    "# Create Mask2Former configuration based on Swin's configuration\n",
    "mask2former_config = Mask2FormerConfig(\n",
    "    backbone_config=swin_config, num_labels=NUM_CLASSES #, ignore_value=BG_VALUE\n",
    ")\n",
    "\n",
    "# Create the Mask2Former model with this configuration\n",
    "model = Mask2FormerForUniversalSegmentation(mask2former_config)\n",
    "\n",
    "# Reuse pretrained parameters\n",
    "for swin_param, m2f_param in zip(\n",
    "    swin_model.named_parameters(),\n",
    "    model.model.pixel_level_module.encoder.named_parameters(),\n",
    "):\n",
    "    m2f_param_name = f\"model.pixel_level_module.encoder.{m2f_param[0]}\"\n",
    "\n",
    "    if swin_param[0] == m2f_param[0]:\n",
    "        model.state_dict()[m2f_param_name].copy_(swin_param[1])\n",
    "        continue\n",
    "\n",
    "    print(f\"Not Matched: {m2f_param[0]} != {swin_param[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset A pixel mean: [0.57365126 0.34606295 0.19539679] pixel_std: [0.15933991 0.15584118 0.10485045]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/notebooks/venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset B pixel mean: [0.48640466 0.32646684 0.20089334] pixel_std: [0.24972845 0.19719756 0.15521158]\n",
      "{'A': {'train': <torch.utils.data.dataloader.DataLoader object at 0x7f4286157c50>, 'val': <torch.utils.data.dataloader.DataLoader object at 0x7f428b129130>, 'test': <torch.utils.data.dataloader.DataLoader object at 0x7f428b4a5f70>}, 'B': {'train': <torch.utils.data.dataloader.DataLoader object at 0x7f4286157800>, 'val': <torch.utils.data.dataloader.DataLoader object at 0x7f42861579b0>, 'test': <torch.utils.data.dataloader.DataLoader object at 0x7f4286157a10>}}\n"
     ]
    }
   ],
   "source": [
    "# Helper function to load datasets\n",
    "def load_dataset(dataset_getter, data_path, domain_incremental):\n",
    "    return dataset_getter(data_path, domain_incremental=domain_incremental)\n",
    "\n",
    "\n",
    "# Helper function to create dataloaders for a dataset\n",
    "def create_dataloaders(\n",
    "    dataset, batch_size, shuffle, num_workers, drop_last, pin_memory, collate_fn\n",
    "):\n",
    "    return {\n",
    "        \"train\": DataLoader(\n",
    "            dataset[\"train\"],\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle,\n",
    "            num_workers=num_workers,\n",
    "            drop_last=drop_last,\n",
    "            pin_memory=pin_memory,\n",
    "            collate_fn=collate_fn,\n",
    "        ),\n",
    "        \"val\": DataLoader(\n",
    "            dataset[\"val\"],\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle,\n",
    "            num_workers=num_workers,\n",
    "            drop_last=drop_last,\n",
    "            pin_memory=pin_memory,\n",
    "            collate_fn=collate_fn,\n",
    "        ),\n",
    "        \"test\": DataLoader(\n",
    "            dataset[\"test\"],\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=num_workers,\n",
    "            drop_last=False,\n",
    "            pin_memory=pin_memory,\n",
    "            collate_fn=collate_fn,\n",
    "        ),\n",
    "    }\n",
    "\n",
    "\n",
    "# Load datasets\n",
    "datasets = {\n",
    "    \"A\": load_dataset(get_cadisv2_dataset, \"../../storage/data/CaDISv2\", True),\n",
    "    \"B\": load_dataset(get_cataract1k_dataset, \"../../storage/data/cataract-1k\", True),\n",
    "}\n",
    "pixel_mean_A,pixel_std_A=pixel_mean_std(datasets[\"A\"][0])\n",
    "print(\"dataset A pixel mean:\",pixel_mean_A,\"pixel_std:\",pixel_std_A)\n",
    "\n",
    "\n",
    "# This time define the B train dataset such that it replays all the training samples from A\n",
    "new_train = torch.utils.data.ConcatDataset([datasets[\"A\"][0], datasets[\"B\"][0]])\n",
    "\n",
    "pixel_mean_B,pixel_std_B=pixel_mean_std(new_train)\n",
    "print(\"dataset B pixel mean:\",pixel_mean_B,\"pixel_std:\",pixel_std_B)\n",
    "\n",
    "\n",
    "datasets[\"B\"] = (new_train, datasets[\"B\"][1], datasets[\"B\"][2])\n",
    "\n",
    "# Define preprocessor\n",
    "swin_processor = AutoImageProcessor.from_pretrained(SWIN_BACKBONE)\n",
    "m2f_preprocessor_A = Mask2FormerImageProcessor(\n",
    "    reduce_labels=False,\n",
    "    ignore_index=255,\n",
    "    do_resize=False,\n",
    "    do_rescale=False,\n",
    "    do_normalize=True,\n",
    "    image_std=pixel_std_A,\n",
    "    image_mean=pixel_mean_A,\n",
    ")\n",
    "\n",
    "m2f_preprocessor_B = Mask2FormerImageProcessor(\n",
    "    reduce_labels=False,\n",
    "    ignore_index=255,\n",
    "    do_resize=False,\n",
    "    do_rescale=False,\n",
    "    do_normalize=True,\n",
    "    image_std=pixel_std_B,\n",
    "    image_mean=pixel_mean_B,\n",
    ")\n",
    "\n",
    "\n",
    "# Create Mask2Former Datasets\n",
    "\n",
    "m2f_datasets = {\n",
    "    \"A\": {\n",
    "        \"train\": Mask2FormerDataset(datasets[\"A\"][0], m2f_preprocessor_A, transform=train_transforms_blur),\n",
    "        \"val\": Mask2FormerDataset(datasets[\"A\"][1], m2f_preprocessor_A),\n",
    "        \"test\": Mask2FormerDataset(datasets[\"A\"][2], m2f_preprocessor_A),\n",
    "    },\n",
    "    \"B\": {\n",
    "        \"train\": Mask2FormerDataset(datasets[\"B\"][0], m2f_preprocessor_B, transform=train_transforms_blur),\n",
    "        \"val\": Mask2FormerDataset(datasets[\"B\"][1], m2f_preprocessor_B),\n",
    "        \"test\": Mask2FormerDataset(datasets[\"B\"][2], m2f_preprocessor_B),\n",
    "    },\n",
    "}\n",
    "\n",
    "\n",
    "# DataLoader parameters\n",
    "N_WORKERS = 4\n",
    "BATCH_SIZE = 16\n",
    "SHUFFLE = True\n",
    "DROP_LAST = True\n",
    "\n",
    "dataloader_params = {\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"shuffle\": SHUFFLE,\n",
    "    \"num_workers\": N_WORKERS,\n",
    "    \"drop_last\": DROP_LAST,\n",
    "    \"pin_memory\": True,\n",
    "    \"collate_fn\": m2f_dataset_collate,\n",
    "}\n",
    "\n",
    "# Create DataLoaders\n",
    "dataloaders = {\n",
    "    key: create_dataloaders(m2f_datasets[key], **dataloader_params)\n",
    "    for key in m2f_datasets\n",
    "}\n",
    "\n",
    "print(dataloaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(False, 255)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m2f_preprocessor_A.reduce_labels, m2f_preprocessor_A.ignore_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA is available, otherwise use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "BG_VALUE_255=255\n",
    "base_run_name=\"M2F-Swin-Tiny-Train_Cadis_AugBlur\"\n",
    "new_run_name=\"M2F-Swin-Tiny-Replay-All_AugBlur\"\n",
    "project_name = \"M2F_latest_aug\"\n",
    "user_or_team = \"continual-learning-tum\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-1c80317fa3b1799d\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-1c80317fa3b1799d\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tensorboard setup\n",
    "out_dir=\"outputs_aug/\"\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "if not os.path.exists(out_dir+\"runs\"):\n",
    "    os.makedirs(out_dir+\"runs\")\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir outputs/runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!CUDA_LAUNCH_BLOCKING=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorboard logging\n",
    "writer = SummaryWriter(log_dir=out_dir + \"runs\")\n",
    "\n",
    "# Model checkpointing\n",
    "model_dir = out_dir + \"models/\"\n",
    "if not os.path.exists(model_dir):\n",
    "    print(\"Store weights in: \", model_dir)\n",
    "    os.makedirs(model_dir)\n",
    "\n",
    "best_model_dir = model_dir + f\"{base_run_name}/best_model/\"\n",
    "if not os.path.exists(best_model_dir):\n",
    "    print(\"Store best model weights in: \", best_model_dir)\n",
    "    os.makedirs(best_model_dir)\n",
    "final_model_dir = model_dir + f\"{base_run_name}/final_model/\"\n",
    "if not os.path.exists(final_model_dir):\n",
    "    print(\"Store final model weights in: \", final_model_dir)\n",
    "    os.makedirs(final_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mge85ket\u001b[0m (\u001b[33mcontinual-learning-tum\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# WandB for team usage !!!!\n",
    "\n",
    "wandb.login() # use this one if a different person is going to run the notebook\n",
    "#wandb.login(relogin=False) # if the same person in the last run is going to run the notebook again\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# First train on dataset A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Training\n",
    "NUM_EPOCHS = 200\n",
    "LEARNING_RATE = 1e-4\n",
    "LR_MULTIPLIER = 0.1\n",
    "BACKBONE_LR = LEARNING_RATE * LR_MULTIPLIER\n",
    "WEIGHT_DECAY = 0.05\n",
    "PATIENCE=15\n",
    "metric = evaluate.load(\"mean_iou\") # mIoU will be used to pick the best performing model using val set\n",
    "encoder_params = [\n",
    "    param\n",
    "    for name, param in model.named_parameters()\n",
    "    if name.startswith(\"model.pixel_level_module.encoder\")\n",
    "]\n",
    "decoder_params = [\n",
    "    param\n",
    "    for name, param in model.named_parameters()\n",
    "    if name.startswith(\"model.pixel_level_module.decoder\")\n",
    "]\n",
    "transformer_params = [\n",
    "    param\n",
    "    for name, param in model.named_parameters()\n",
    "    if name.startswith(\"model.transformer_module\")\n",
    "]\n",
    "class_prediction_params=[\n",
    "    param\n",
    "    for name, param in model.named_parameters() \n",
    "    if not name.startswith(\"model.pixel_level_module.encoder\") and not name.startswith(\"model.transformer_module\") and not name.startswith(\"model.pixel_level_module.decoder\")\n",
    "]\n",
    "optimizer = optim.AdamW(\n",
    "    [\n",
    "        {\"params\": encoder_params, \"lr\": BACKBONE_LR},\n",
    "        {\"params\": decoder_params},\n",
    "        {\"params\": transformer_params},\n",
    "        {\"params\": class_prediction_params}\n",
    "    ],\n",
    "    lr=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    ")\n",
    "\n",
    "scheduler = optim.lr_scheduler.PolynomialLR(\n",
    "    optimizer, total_iters=NUM_EPOCHS, power=0.9\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mge85ket\u001b[0m (\u001b[33mcontinual-learning-tum\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# WandB for team usage !!!!\n",
    "\n",
    "wandb.login() # use this one if a different person is going to run the notebook\n",
    "#wandb.login(relogin=False) # if the same person in the last run is going to run the notebook again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/notebooks/continual-learning/wandb/run-20240604_160547-oyvhaqf5</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/continual-learning-tum/M2F_latest_aug/runs/oyvhaqf5' target=\"_blank\">M2F-Swin-Tiny-Train_Cadis_AugBlur</a></strong> to <a href='https://wandb.ai/continual-learning-tum/M2F_latest_aug' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/continual-learning-tum/M2F_latest_aug' target=\"_blank\">https://wandb.ai/continual-learning-tum/M2F_latest_aug</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/continual-learning-tum/M2F_latest_aug/runs/oyvhaqf5' target=\"_blank\">https://wandb.ai/continual-learning-tum/M2F_latest_aug/runs/oyvhaqf5</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wandb run id: oyvhaqf5\n"
     ]
    }
   ],
   "source": [
    "wandb.init(\n",
    "    project=project_name,\n",
    "    config={\n",
    "        \"learning_rate\": LEARNING_RATE,\n",
    "        \"learning_rate_multiplier\": LR_MULTIPLIER,\n",
    "        \"backbone_learning_rate\": BACKBONE_LR,\n",
    "        \"learning_rate_scheduler\": scheduler.__class__.__name__,\n",
    "        \"optimizer\": optimizer.__class__.__name__,\n",
    "        \"backbone\": SWIN_BACKBONE,\n",
    "        \"m2f_preprocessor\": m2f_preprocessor_A.__dict__,\n",
    "        \"m2f_model_config\": model.config\n",
    "    },\n",
    "    name=base_run_name,\n",
    "    notes=\"M2F with tiny Swin backbone pretrained on ImageNet-1K. \\\n",
    "        Scenario: Train on A, Test on A\"\n",
    ")\n",
    "print(\"wandb run id:\",wandb.run.id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['outputs_aug/models/M2F-Swin-Tiny-Train_Cadis_AugBlur/preprocessor_config.json']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the preprocessor\n",
    "m2f_preprocessor_A.save_pretrained(model_dir + base_run_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/200 Training: 100%|██████████| 221/221 [05:32<00:00,  1.50s/it, loss=704.7999] \n",
      "Epoch 1/200 Validation: 100%|██████████| 33/33 [00:39<00:00,  1.19s/it, loss=765.5924]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200, Train Loss: 67.3616, Train mIoU: 0.1051, Validation Loss: 50.5375, Validation mIoU: 0.1664\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/200 Training: 100%|██████████| 221/221 [05:28<00:00,  1.49s/it, loss=462.6539]\n",
      "Epoch 2/200 Validation: 100%|██████████| 33/33 [00:39<00:00,  1.19s/it, loss=548.7513]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/200, Train Loss: 34.7298, Train mIoU: 0.2194, Validation Loss: 32.8301, Validation mIoU: 0.2439\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/200 Training: 100%|██████████| 221/221 [05:01<00:00,  1.37s/it, loss=404.5788]\n",
      "Epoch 3/200 Validation: 100%|██████████| 33/33 [00:37<00:00,  1.13s/it, loss=434.6761]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/200, Train Loss: 28.7677, Train mIoU: 0.3186, Validation Loss: 27.6610, Validation mIoU: 0.3750\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/200 Training: 100%|██████████| 221/221 [04:45<00:00,  1.29s/it, loss=343.0320]\n",
      "Epoch 4/200 Validation: 100%|██████████| 33/33 [00:35<00:00,  1.07s/it, loss=322.5956]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/200, Train Loss: 25.5613, Train mIoU: 0.4206, Validation Loss: 24.7237, Validation mIoU: 0.4505\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/200 Training: 100%|██████████| 221/221 [04:58<00:00,  1.35s/it, loss=366.2369]\n",
      "Epoch 5/200 Validation: 100%|██████████| 33/33 [00:34<00:00,  1.05s/it, loss=370.9257]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/200, Train Loss: 22.9067, Train mIoU: 0.4816, Validation Loss: 21.9428, Validation mIoU: 0.5173\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/200 Training: 100%|██████████| 221/221 [05:15<00:00,  1.43s/it, loss=343.9352]\n",
      "Epoch 6/200 Validation: 100%|██████████| 33/33 [00:33<00:00,  1.02s/it, loss=392.7356]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/200, Train Loss: 21.1706, Train mIoU: 0.5291, Validation Loss: 21.9580, Validation mIoU: 0.5172\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/200 Training: 100%|██████████| 221/221 [05:10<00:00,  1.41s/it, loss=330.4022]\n",
      "Epoch 7/200 Validation: 100%|██████████| 33/33 [00:30<00:00,  1.10it/s, loss=287.5240]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/200, Train Loss: 19.6055, Train mIoU: 0.5596, Validation Loss: 20.7170, Validation mIoU: 0.5328\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/200 Training: 100%|██████████| 221/221 [04:57<00:00,  1.35s/it, loss=263.0647]\n",
      "Epoch 8/200 Validation: 100%|██████████| 33/33 [00:29<00:00,  1.10it/s, loss=275.9343]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/200, Train Loss: 18.7459, Train mIoU: 0.5736, Validation Loss: 19.8039, Validation mIoU: 0.5534\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/200 Training: 100%|██████████| 221/221 [04:47<00:00,  1.30s/it, loss=307.2384]\n",
      "Epoch 9/200 Validation: 100%|██████████| 33/33 [00:29<00:00,  1.11it/s, loss=301.8675]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/200, Train Loss: 17.6494, Train mIoU: 0.5986, Validation Loss: 19.7995, Validation mIoU: 0.5849\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/200 Training: 100%|██████████| 221/221 [04:52<00:00,  1.32s/it, loss=269.2785]\n",
      "Epoch 10/200 Validation: 100%|██████████| 33/33 [00:35<00:00,  1.07s/it, loss=263.1896]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/200, Train Loss: 16.8887, Train mIoU: 0.6150, Validation Loss: 20.2517, Validation mIoU: 0.5386\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/200 Training: 100%|██████████| 221/221 [05:00<00:00,  1.36s/it, loss=257.3792]\n",
      "Epoch 11/200 Validation: 100%|██████████| 33/33 [00:29<00:00,  1.12it/s, loss=299.0481]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/200, Train Loss: 16.0872, Train mIoU: 0.6367, Validation Loss: 18.9255, Validation mIoU: 0.5439\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/200 Training: 100%|██████████| 221/221 [04:53<00:00,  1.33s/it, loss=219.8190]\n",
      "Epoch 12/200 Validation: 100%|██████████| 33/33 [00:30<00:00,  1.08it/s, loss=355.9763]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/200, Train Loss: 15.6515, Train mIoU: 0.6476, Validation Loss: 18.0997, Validation mIoU: 0.6326\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/200 Training: 100%|██████████| 221/221 [04:50<00:00,  1.32s/it, loss=243.6143]\n",
      "Epoch 13/200 Validation: 100%|██████████| 33/33 [00:34<00:00,  1.04s/it, loss=332.8428]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/200, Train Loss: 15.1475, Train mIoU: 0.6593, Validation Loss: 18.7931, Validation mIoU: 0.6116\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/200 Training: 100%|██████████| 221/221 [04:58<00:00,  1.35s/it, loss=226.5961]\n",
      "Epoch 14/200 Validation: 100%|██████████| 33/33 [00:32<00:00,  1.03it/s, loss=341.9105]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/200, Train Loss: 14.6902, Train mIoU: 0.6649, Validation Loss: 19.9125, Validation mIoU: 0.6209\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/200 Training: 100%|██████████| 221/221 [05:07<00:00,  1.39s/it, loss=273.7238]\n",
      "Epoch 15/200 Validation: 100%|██████████| 33/33 [00:35<00:00,  1.06s/it, loss=276.5111]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/200, Train Loss: 14.0534, Train mIoU: 0.6831, Validation Loss: 19.2335, Validation mIoU: 0.5780\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/200 Training: 100%|██████████| 221/221 [04:49<00:00,  1.31s/it, loss=229.4294]\n",
      "Epoch 16/200 Validation: 100%|██████████| 33/33 [00:32<00:00,  1.01it/s, loss=334.8688]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/200, Train Loss: 13.7361, Train mIoU: 0.6979, Validation Loss: 18.4203, Validation mIoU: 0.5739\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/200 Training: 100%|██████████| 221/221 [05:02<00:00,  1.37s/it, loss=224.8513]\n",
      "Epoch 17/200 Validation: 100%|██████████| 33/33 [00:34<00:00,  1.04s/it, loss=452.4117]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/200, Train Loss: 13.2985, Train mIoU: 0.7035, Validation Loss: 20.0991, Validation mIoU: 0.5710\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/200 Training: 100%|██████████| 221/221 [04:52<00:00,  1.33s/it, loss=216.9509]\n",
      "Epoch 18/200 Validation: 100%|██████████| 33/33 [00:33<00:00,  1.01s/it, loss=261.4957]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/200, Train Loss: 13.1116, Train mIoU: 0.7123, Validation Loss: 19.2272, Validation mIoU: 0.6088\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/200 Training: 100%|██████████| 221/221 [04:59<00:00,  1.36s/it, loss=195.7676]\n",
      "Epoch 19/200 Validation: 100%|██████████| 33/33 [00:30<00:00,  1.08it/s, loss=304.0920]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/200, Train Loss: 12.7216, Train mIoU: 0.7166, Validation Loss: 19.8469, Validation mIoU: 0.5650\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/200 Training: 100%|██████████| 221/221 [04:52<00:00,  1.32s/it, loss=206.4666]\n",
      "Epoch 20/200 Validation: 100%|██████████| 33/33 [00:33<00:00,  1.03s/it, loss=313.8513]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/200, Train Loss: 12.4162, Train mIoU: 0.7261, Validation Loss: 18.7710, Validation mIoU: 0.6316\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/200 Training: 100%|██████████| 221/221 [04:44<00:00,  1.29s/it, loss=170.1489]\n",
      "Epoch 21/200 Validation: 100%|██████████| 33/33 [00:29<00:00,  1.11it/s, loss=308.4799]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/200, Train Loss: 12.1035, Train mIoU: 0.7384, Validation Loss: 18.8181, Validation mIoU: 0.5872\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/200 Training: 100%|██████████| 221/221 [05:00<00:00,  1.36s/it, loss=161.1959]\n",
      "Epoch 22/200 Validation: 100%|██████████| 33/33 [00:30<00:00,  1.07it/s, loss=242.7206]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/200, Train Loss: 11.8035, Train mIoU: 0.7503, Validation Loss: 18.0007, Validation mIoU: 0.5843\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/200 Training: 100%|██████████| 221/221 [04:52<00:00,  1.32s/it, loss=177.0575]\n",
      "Epoch 23/200 Validation: 100%|██████████| 33/33 [00:32<00:00,  1.01it/s, loss=360.6679]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/200, Train Loss: 11.5736, Train mIoU: 0.7525, Validation Loss: 19.4243, Validation mIoU: 0.6029\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/200 Training: 100%|██████████| 221/221 [04:57<00:00,  1.35s/it, loss=190.5688]\n",
      "Epoch 24/200 Validation: 100%|██████████| 33/33 [00:31<00:00,  1.04it/s, loss=362.2864]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/200, Train Loss: 11.4478, Train mIoU: 0.7571, Validation Loss: 19.7169, Validation mIoU: 0.5727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/200 Training: 100%|██████████| 221/221 [04:42<00:00,  1.28s/it, loss=153.5436]\n",
      "Epoch 25/200 Validation: 100%|██████████| 33/33 [00:33<00:00,  1.02s/it, loss=270.3229]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/200, Train Loss: 11.0825, Train mIoU: 0.7657, Validation Loss: 19.0948, Validation mIoU: 0.5966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/200 Training: 100%|██████████| 221/221 [04:52<00:00,  1.32s/it, loss=162.4758]\n",
      "Epoch 26/200 Validation: 100%|██████████| 33/33 [00:28<00:00,  1.14it/s, loss=327.7599]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/200, Train Loss: 11.0449, Train mIoU: 0.7540, Validation Loss: 19.6666, Validation mIoU: 0.6107\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/200 Training: 100%|██████████| 221/221 [04:45<00:00,  1.29s/it, loss=184.5747]\n",
      "Epoch 27/200 Validation: 100%|██████████| 33/33 [00:34<00:00,  1.04s/it, loss=323.0818]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/200, Train Loss: 10.6907, Train mIoU: 0.7743, Validation Loss: 19.3291, Validation mIoU: 0.6421\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/200 Training: 100%|██████████| 221/221 [04:49<00:00,  1.31s/it, loss=149.5404]\n",
      "Epoch 28/200 Validation: 100%|██████████| 33/33 [00:35<00:00,  1.07s/it, loss=237.8805]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/200, Train Loss: 10.6781, Train mIoU: 0.7675, Validation Loss: 19.6313, Validation mIoU: 0.6002\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/200 Training: 100%|██████████| 221/221 [04:50<00:00,  1.32s/it, loss=161.0405]\n",
      "Epoch 29/200 Validation: 100%|██████████| 33/33 [00:29<00:00,  1.13it/s, loss=347.0563]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/200, Train Loss: 10.4069, Train mIoU: 0.7791, Validation Loss: 20.4786, Validation mIoU: 0.5710\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/200 Training: 100%|██████████| 221/221 [04:59<00:00,  1.35s/it, loss=158.3876]\n",
      "Epoch 30/200 Validation: 100%|██████████| 33/33 [00:30<00:00,  1.08it/s, loss=314.8263]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/200, Train Loss: 10.0468, Train mIoU: 0.7920, Validation Loss: 19.4172, Validation mIoU: 0.6075\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/200 Training: 100%|██████████| 221/221 [04:45<00:00,  1.29s/it, loss=155.5565]\n",
      "Epoch 31/200 Validation: 100%|██████████| 33/33 [00:29<00:00,  1.10it/s, loss=435.9793]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/200, Train Loss: 10.0474, Train mIoU: 0.7904, Validation Loss: 19.6269, Validation mIoU: 0.6249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/200 Training: 100%|██████████| 221/221 [04:47<00:00,  1.30s/it, loss=175.4153]\n",
      "Epoch 32/200 Validation: 100%|██████████| 33/33 [00:32<00:00,  1.00it/s, loss=286.8088]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/200, Train Loss: 9.7849, Train mIoU: 0.7974, Validation Loss: 19.8700, Validation mIoU: 0.5857\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/200 Training: 100%|██████████| 221/221 [04:57<00:00,  1.35s/it, loss=128.6396]\n",
      "Epoch 33/200 Validation: 100%|██████████| 33/33 [00:40<00:00,  1.21s/it, loss=256.4563]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/200, Train Loss: 9.7581, Train mIoU: 0.7957, Validation Loss: 18.7916, Validation mIoU: 0.6293\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 34/200 Training: 100%|██████████| 221/221 [04:44<00:00,  1.29s/it, loss=143.9128]\n",
      "Epoch 34/200 Validation: 100%|██████████| 33/33 [00:28<00:00,  1.17it/s, loss=376.7933]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 34/200, Train Loss: 9.5543, Train mIoU: 0.7994, Validation Loss: 19.7944, Validation mIoU: 0.5894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 35/200 Training: 100%|██████████| 221/221 [04:44<00:00,  1.29s/it, loss=125.1175]\n",
      "Epoch 35/200 Validation: 100%|██████████| 33/33 [00:31<00:00,  1.04it/s, loss=273.5570]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35/200, Train Loss: 9.4178, Train mIoU: 0.8082, Validation Loss: 19.7769, Validation mIoU: 0.6081\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 36/200 Training: 100%|██████████| 221/221 [04:49<00:00,  1.31s/it, loss=161.5038]\n",
      "Epoch 36/200 Validation: 100%|██████████| 33/33 [00:30<00:00,  1.07it/s, loss=359.7872]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 36/200, Train Loss: 9.2922, Train mIoU: 0.8015, Validation Loss: 22.6949, Validation mIoU: 0.5615\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 37/200 Training: 100%|██████████| 221/221 [04:54<00:00,  1.33s/it, loss=130.8067]\n",
      "Epoch 37/200 Validation: 100%|██████████| 33/33 [00:31<00:00,  1.06it/s, loss=365.8166]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 37/200, Train Loss: 9.1335, Train mIoU: 0.8076, Validation Loss: 20.8718, Validation mIoU: 0.6488\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 38/200 Training: 100%|██████████| 221/221 [04:50<00:00,  1.31s/it, loss=141.2131]\n",
      "Epoch 38/200 Validation: 100%|██████████| 33/33 [00:32<00:00,  1.03it/s, loss=390.0930]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 38/200, Train Loss: 9.0162, Train mIoU: 0.8118, Validation Loss: 21.1939, Validation mIoU: 0.6000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 39/200 Training: 100%|██████████| 221/221 [04:45<00:00,  1.29s/it, loss=135.0854]\n",
      "Epoch 39/200 Validation: 100%|██████████| 33/33 [00:31<00:00,  1.03it/s, loss=428.8681]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 39/200, Train Loss: 8.9392, Train mIoU: 0.8155, Validation Loss: 20.6204, Validation mIoU: 0.6078\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 40/200 Training: 100%|██████████| 221/221 [04:48<00:00,  1.31s/it, loss=122.1686]\n",
      "Epoch 40/200 Validation: 100%|██████████| 33/33 [00:26<00:00,  1.24it/s, loss=271.4616]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 40/200, Train Loss: 8.7189, Train mIoU: 0.8207, Validation Loss: 20.4847, Validation mIoU: 0.6100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 41/200 Training: 100%|██████████| 221/221 [04:51<00:00,  1.32s/it, loss=163.8227]\n",
      "Epoch 41/200 Validation: 100%|██████████| 33/33 [00:27<00:00,  1.18it/s, loss=300.2075]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 41/200, Train Loss: 8.6612, Train mIoU: 0.8215, Validation Loss: 21.4255, Validation mIoU: 0.5986\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 42/200 Training: 100%|██████████| 221/221 [05:12<00:00,  1.41s/it, loss=117.1915]\n",
      "Epoch 42/200 Validation: 100%|██████████| 33/33 [00:35<00:00,  1.07s/it, loss=401.2606]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 42/200, Train Loss: 8.4651, Train mIoU: 0.8250, Validation Loss: 21.2698, Validation mIoU: 0.6020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 43/200 Training: 100%|██████████| 221/221 [05:07<00:00,  1.39s/it, loss=123.3194]\n",
      "Epoch 43/200 Validation: 100%|██████████| 33/33 [00:31<00:00,  1.06it/s, loss=466.0887]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 43/200, Train Loss: 8.3502, Train mIoU: 0.8304, Validation Loss: 21.1995, Validation mIoU: 0.6113\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 44/200 Training: 100%|██████████| 221/221 [04:49<00:00,  1.31s/it, loss=124.5013]\n",
      "Epoch 44/200 Validation: 100%|██████████| 33/33 [00:34<00:00,  1.03s/it, loss=431.4523]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 44/200, Train Loss: 8.3658, Train mIoU: 0.8275, Validation Loss: 21.9675, Validation mIoU: 0.5996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 45/200 Training: 100%|██████████| 221/221 [04:42<00:00,  1.28s/it, loss=133.2351]\n",
      "Epoch 45/200 Validation: 100%|██████████| 33/33 [00:30<00:00,  1.09it/s, loss=336.2133]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 45/200, Train Loss: 8.2847, Train mIoU: 0.8331, Validation Loss: 21.9501, Validation mIoU: 0.5924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 46/200 Training: 100%|██████████| 221/221 [04:45<00:00,  1.29s/it, loss=154.6065]\n",
      "Epoch 46/200 Validation: 100%|██████████| 33/33 [00:33<00:00,  1.01s/it, loss=361.6513]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46/200, Train Loss: 8.3729, Train mIoU: 0.8306, Validation Loss: 21.1495, Validation mIoU: 0.6274\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 47/200 Training: 100%|██████████| 221/221 [04:51<00:00,  1.32s/it, loss=158.5703]\n",
      "Epoch 47/200 Validation: 100%|██████████| 33/33 [00:28<00:00,  1.17it/s, loss=298.2366]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 47/200, Train Loss: 8.1476, Train mIoU: 0.8338, Validation Loss: 22.2162, Validation mIoU: 0.6056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 48/200 Training: 100%|██████████| 221/221 [04:46<00:00,  1.30s/it, loss=120.2198]\n",
      "Epoch 48/200 Validation: 100%|██████████| 33/33 [00:33<00:00,  1.02s/it, loss=366.9691]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 48/200, Train Loss: 8.0291, Train mIoU: 0.8455, Validation Loss: 22.6583, Validation mIoU: 0.5821\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 49/200 Training: 100%|██████████| 221/221 [04:41<00:00,  1.27s/it, loss=117.8744]\n",
      "Epoch 49/200 Validation: 100%|██████████| 33/33 [00:32<00:00,  1.02it/s, loss=378.2524]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 49/200, Train Loss: 7.9203, Train mIoU: 0.8434, Validation Loss: 21.9283, Validation mIoU: 0.5882\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 50/200 Training: 100%|██████████| 221/221 [04:47<00:00,  1.30s/it, loss=142.8060]\n",
      "Epoch 50/200 Validation: 100%|██████████| 33/33 [00:27<00:00,  1.22it/s, loss=370.8741]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 50/200, Train Loss: 7.7965, Train mIoU: 0.8387, Validation Loss: 23.0639, Validation mIoU: 0.5808\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 51/200 Training: 100%|██████████| 221/221 [04:47<00:00,  1.30s/it, loss=126.2721]\n",
      "Epoch 51/200 Validation: 100%|██████████| 33/33 [00:27<00:00,  1.22it/s, loss=401.6805]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 51/200, Train Loss: 7.7423, Train mIoU: 0.8426, Validation Loss: 22.1923, Validation mIoU: 0.5605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 52/200 Training: 100%|██████████| 221/221 [04:48<00:00,  1.31s/it, loss=165.8905]\n",
      "Epoch 52/200 Validation: 100%|██████████| 33/33 [00:29<00:00,  1.12it/s, loss=312.8188]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 52/200, Train Loss: 7.7437, Train mIoU: 0.8392, Validation Loss: 20.9566, Validation mIoU: 0.6090\n",
      "Early stopping at epoch 51\n"
     ]
    }
   ],
   "source": [
    "# To avoid making stupid errors\n",
    "CURR_TASK = \"A\"\n",
    "\n",
    "# For storing the model\n",
    "best_val_metric = -np.inf\n",
    "best_model_weights=None # best model weights are stored here\n",
    "\n",
    "# Move model to device\n",
    "model.to(device)\n",
    "counter=0\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    train_running_loss = 0.0\n",
    "    val_running_loss = 0.0\n",
    "\n",
    "    # Set up tqdm for the training loop\n",
    "    train_loader = tqdm(\n",
    "        dataloaders[CURR_TASK][\"train\"], desc=f\"Epoch {epoch + 1}/{NUM_EPOCHS} Training\"\n",
    "    )\n",
    "\n",
    "    for batch in train_loader:\n",
    "        # Move everything to the device\n",
    "        batch[\"pixel_values\"] = batch[\"pixel_values\"].to(device)\n",
    "        batch[\"pixel_mask\"] = batch[\"pixel_mask\"].to(device)\n",
    "        batch[\"mask_labels\"] = [entry.to(device) for entry in batch[\"mask_labels\"]]\n",
    "        batch[\"class_labels\"] = [entry.to(device) for entry in batch[\"class_labels\"]]\n",
    "       \n",
    "        # Compute output and loss\n",
    "        outputs = model(**batch)\n",
    "\n",
    "        loss = outputs.loss\n",
    "        # Compute gradient and perform step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Record losses\n",
    "        current_loss = loss.item() * batch[\"pixel_values\"].size(0)\n",
    "        train_running_loss += current_loss\n",
    "        train_loader.set_postfix(loss=f\"{current_loss:.4f}\")\n",
    "\n",
    "        # Extract and compute metrics\n",
    "        pred_maps, masks = m2f_extract_pred_maps_and_masks(\n",
    "            batch, outputs, m2f_preprocessor_A\n",
    "        )\n",
    "        metric.add_batch(references=masks, predictions=pred_maps)\n",
    "\n",
    "    # After compute the batches that were added are deleted\n",
    "    temp_metrics = metric.compute(\n",
    "        num_labels=NUM_CLASSES, ignore_index=BG_VALUE_255, reduce_labels=False\n",
    "    )\n",
    "    mean_train_iou=temp_metrics[\"mean_iou\"]\n",
    "        \n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loader = tqdm(\n",
    "        dataloaders[CURR_TASK][\"val\"], desc=f\"Epoch {epoch + 1}/{NUM_EPOCHS} Validation\"\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            # Move everything to the device\n",
    "            batch[\"pixel_values\"] = batch[\"pixel_values\"].to(device)\n",
    "            batch[\"pixel_mask\"] = batch[\"pixel_mask\"].to(device)\n",
    "            batch[\"mask_labels\"] = [entry.to(device) for entry in batch[\"mask_labels\"]]\n",
    "            batch[\"class_labels\"] = [\n",
    "                entry.to(device) for entry in batch[\"class_labels\"]\n",
    "            ]\n",
    "            # Compute output and loss\n",
    "            outputs = model(**batch)\n",
    "\n",
    "            loss = outputs.loss\n",
    "            # Record losses\n",
    "            current_loss = loss.item() * batch[\"pixel_values\"].size(0)\n",
    "            val_running_loss += current_loss\n",
    "            val_loader.set_postfix(loss=f\"{current_loss:.4f}\")\n",
    "\n",
    "            # Extract and compute metrics\n",
    "            pred_maps, masks = m2f_extract_pred_maps_and_masks(\n",
    "                batch, outputs, m2f_preprocessor_A\n",
    "            )\n",
    "            metric.add_batch(references=masks, predictions=pred_maps)\n",
    "            \n",
    "\n",
    "    # After compute the batches that were added are deleted\n",
    "    mean_val_iou = metric.compute(\n",
    "        num_labels=NUM_CLASSES, ignore_index=BG_VALUE_255, reduce_labels=False\n",
    "    )[\"mean_iou\"]\n",
    "\n",
    "    epoch_train_loss = train_running_loss / len(dataloaders[CURR_TASK][\"train\"].dataset)\n",
    "    epoch_val_loss = val_running_loss / len(dataloaders[CURR_TASK][\"val\"].dataset)\n",
    "\n",
    "    writer.add_scalar(f\"Loss/train_{base_run_name}_{CURR_TASK}\", epoch_train_loss, epoch + 1)\n",
    "    writer.add_scalar(f\"Loss/val_{base_run_name}_{CURR_TASK}\", epoch_val_loss, epoch + 1)\n",
    "    writer.add_scalar(f\"mIoU/train_{base_run_name}_{CURR_TASK}\", mean_train_iou, epoch + 1)\n",
    "    writer.add_scalar(f\"mIoU/val_{base_run_name}_{CURR_TASK}\", mean_val_iou, epoch + 1)\n",
    "\n",
    "    wandb.log({\n",
    "        f\"Loss/train_{CURR_TASK}\": epoch_train_loss,\n",
    "        f\"Loss/val_{CURR_TASK}\": epoch_val_loss,\n",
    "        f\"mIoU/train_{CURR_TASK}\": mean_train_iou,\n",
    "        f\"mIoU/val_{CURR_TASK}\": mean_val_iou\n",
    "    })\n",
    "\n",
    "\n",
    "    tqdm.write(\n",
    "        f\"Epoch {epoch + 1}/{NUM_EPOCHS}, Train Loss: {epoch_train_loss:.4f}, Train mIoU: {mean_train_iou:.4f}, Validation Loss: {epoch_val_loss:.4f}, Validation mIoU: {mean_val_iou:.4f}\"\n",
    "    )\n",
    "    \n",
    "    if mean_val_iou > best_val_metric:\n",
    "        best_val_metric = mean_val_iou\n",
    "        #model.save_pretrained(f\"{best_model_dir}{CURR_TASK}/\")\n",
    "        best_model_weights = deepcopy(model.state_dict())\n",
    "        counter=0\n",
    "    else:\n",
    "        counter+=1\n",
    "        if counter == PATIENCE:\n",
    "            print(\"Early stopping at epoch\",epoch)\n",
    "            break\n",
    "            \n",
    "os.makedirs(f\"{best_model_dir}{CURR_TASK}/\",exist_ok=True)\n",
    "artifact = wandb.Artifact(f\"best_model_{base_run_name}\", type=\"model\")\n",
    "artifact.add_file(f\"{best_model_dir}{CURR_TASK}/best_model_{base_run_name}.pth\", torch.save(best_model_weights, f\"{best_model_dir}{CURR_TASK}/best_model_{base_run_name}.pth\"))\n",
    "wandb.run.log_artifact(artifact)\n",
    "\n",
    "if os.path.exists(model_dir + f\"{base_run_name}\"):\n",
    "    shutil.rmtree(model_dir + f\"{base_run_name}\")\n",
    "\n",
    "        \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Test results on A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact best_model_M2F-Swin-Tiny-Train_Cadis_AugBlur:latest, 181.31MB. 1 files... \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n",
      "Done. 0:0:0.7\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Mask2FormerForUniversalSegmentation(\n",
       "  (model): Mask2FormerModel(\n",
       "    (pixel_level_module): Mask2FormerPixelLevelModule(\n",
       "      (encoder): SwinBackbone(\n",
       "        (embeddings): SwinEmbeddings(\n",
       "          (patch_embeddings): SwinPatchEmbeddings(\n",
       "            (projection): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))\n",
       "          )\n",
       "          (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (encoder): SwinEncoder(\n",
       "          (layers): ModuleList(\n",
       "            (0): SwinStage(\n",
       "              (blocks): ModuleList(\n",
       "                (0-1): 2 x SwinLayer(\n",
       "                  (layernorm_before): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "                  (attention): SwinAttention(\n",
       "                    (self): SwinSelfAttention(\n",
       "                      (query): Linear(in_features=96, out_features=96, bias=True)\n",
       "                      (key): Linear(in_features=96, out_features=96, bias=True)\n",
       "                      (value): Linear(in_features=96, out_features=96, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (output): SwinSelfOutput(\n",
       "                      (dense): Linear(in_features=96, out_features=96, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                  (drop_path): SwinDropPath(p=0.1)\n",
       "                  (layernorm_after): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "                  (intermediate): SwinIntermediate(\n",
       "                    (dense): Linear(in_features=96, out_features=384, bias=True)\n",
       "                    (intermediate_act_fn): GELUActivation()\n",
       "                  )\n",
       "                  (output): SwinOutput(\n",
       "                    (dense): Linear(in_features=384, out_features=96, bias=True)\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (downsample): SwinPatchMerging(\n",
       "                (reduction): Linear(in_features=384, out_features=192, bias=False)\n",
       "                (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "            (1): SwinStage(\n",
       "              (blocks): ModuleList(\n",
       "                (0-1): 2 x SwinLayer(\n",
       "                  (layernorm_before): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "                  (attention): SwinAttention(\n",
       "                    (self): SwinSelfAttention(\n",
       "                      (query): Linear(in_features=192, out_features=192, bias=True)\n",
       "                      (key): Linear(in_features=192, out_features=192, bias=True)\n",
       "                      (value): Linear(in_features=192, out_features=192, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (output): SwinSelfOutput(\n",
       "                      (dense): Linear(in_features=192, out_features=192, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                  (drop_path): SwinDropPath(p=0.1)\n",
       "                  (layernorm_after): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "                  (intermediate): SwinIntermediate(\n",
       "                    (dense): Linear(in_features=192, out_features=768, bias=True)\n",
       "                    (intermediate_act_fn): GELUActivation()\n",
       "                  )\n",
       "                  (output): SwinOutput(\n",
       "                    (dense): Linear(in_features=768, out_features=192, bias=True)\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (downsample): SwinPatchMerging(\n",
       "                (reduction): Linear(in_features=768, out_features=384, bias=False)\n",
       "                (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "            (2): SwinStage(\n",
       "              (blocks): ModuleList(\n",
       "                (0-5): 6 x SwinLayer(\n",
       "                  (layernorm_before): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "                  (attention): SwinAttention(\n",
       "                    (self): SwinSelfAttention(\n",
       "                      (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "                      (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "                      (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (output): SwinSelfOutput(\n",
       "                      (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                  (drop_path): SwinDropPath(p=0.1)\n",
       "                  (layernorm_after): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "                  (intermediate): SwinIntermediate(\n",
       "                    (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                    (intermediate_act_fn): GELUActivation()\n",
       "                  )\n",
       "                  (output): SwinOutput(\n",
       "                    (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (downsample): SwinPatchMerging(\n",
       "                (reduction): Linear(in_features=1536, out_features=768, bias=False)\n",
       "                (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "            (3): SwinStage(\n",
       "              (blocks): ModuleList(\n",
       "                (0-1): 2 x SwinLayer(\n",
       "                  (layernorm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                  (attention): SwinAttention(\n",
       "                    (self): SwinSelfAttention(\n",
       "                      (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                      (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                      (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (output): SwinSelfOutput(\n",
       "                      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                  (drop_path): SwinDropPath(p=0.1)\n",
       "                  (layernorm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                  (intermediate): SwinIntermediate(\n",
       "                    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                    (intermediate_act_fn): GELUActivation()\n",
       "                  )\n",
       "                  (output): SwinOutput(\n",
       "                    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (hidden_states_norms): ModuleDict(\n",
       "          (stage1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "          (stage2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (stage3): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (stage4): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (decoder): Mask2FormerPixelDecoder(\n",
       "        (position_embedding): Mask2FormerSinePositionEmbedding()\n",
       "        (input_projections): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): Conv2d(192, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "          )\n",
       "        )\n",
       "        (encoder): Mask2FormerPixelDecoderEncoderOnly(\n",
       "          (layers): ModuleList(\n",
       "            (0-5): 6 x Mask2FormerPixelDecoderEncoderLayer(\n",
       "              (self_attn): Mask2FormerPixelDecoderEncoderMultiscaleDeformableAttention(\n",
       "                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)\n",
       "                (attention_weights): Linear(in_features=256, out_features=96, bias=True)\n",
       "                (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              )\n",
       "              (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "              (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "              (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "              (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (mask_projection): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (adapter_1): Sequential(\n",
       "          (0): Conv2d(96, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "        )\n",
       "        (layer_1): Sequential(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (transformer_module): Mask2FormerTransformerModule(\n",
       "      (position_embedder): Mask2FormerSinePositionEmbedding()\n",
       "      (queries_embedder): Embedding(100, 256)\n",
       "      (queries_features): Embedding(100, 256)\n",
       "      (decoder): Mask2FormerMaskedAttentionDecoder(\n",
       "        (layers): ModuleList(\n",
       "          (0-8): 9 x Mask2FormerMaskedAttentionDecoderLayer(\n",
       "            (self_attn): Mask2FormerAttention(\n",
       "              (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            )\n",
       "            (activation_fn): ReLU()\n",
       "            (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (cross_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "            )\n",
       "            (cross_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "            (fc2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "            (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (layernorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (mask_predictor): Mask2FormerMaskPredictor(\n",
       "          (mask_embedder): Mask2FormerMLPPredictionHead(\n",
       "            (0): Mask2FormerPredictionBlock(\n",
       "              (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (1): ReLU()\n",
       "            )\n",
       "            (1): Mask2FormerPredictionBlock(\n",
       "              (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (1): ReLU()\n",
       "            )\n",
       "            (2): Mask2FormerPredictionBlock(\n",
       "              (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (1): Identity()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (level_embed): Embedding(3, 256)\n",
       "    )\n",
       "  )\n",
       "  (class_predictor): Linear(in_features=256, out_features=13, bias=True)\n",
       "  (criterion): Mask2FormerLoss(\n",
       "    (matcher): Mask2FormerHungarianMatcher()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load best model and evaluate on test\n",
    "#model = Mask2FormerForUniversalSegmentation.from_pretrained(f\"{best_model_dir}{CURR_TASK}/\").to(device)\n",
    "\n",
    "# Load pretrained on Cadis from naive forgetting \n",
    "#model = Mask2FormerForUniversalSegmentation.from_pretrained(f\"/notebooks/continual-learning/outputs/models/{base_model_name}/best_model/A\").to(device)\n",
    "\n",
    "# Construct the artifact path\n",
    "artifact_path = f\"{user_or_team}/{project_name}/best_model_{base_run_name}:latest\"\n",
    "\n",
    "# Load from W&B\n",
    "api = wandb.Api()\n",
    "artifact=api.artifact(artifact_path)\n",
    "model_dir=artifact.download()\n",
    "model_state_dict_path = os.path.join(model_dir, f\"best_model_{base_run_name}.pth\" )\n",
    "model_state_dict = torch.load(model_state_dict_path)\n",
    "model = Mask2FormerForUniversalSegmentation(mask2former_config)\n",
    "model.load_state_dict(model_state_dict)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test loop: 100%|██████████| 37/37 [00:40<00:00,  1.10s/it, loss=362.5905]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 28.3386, Test mIoU: 0.6400\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "test_running_loss = 0\n",
    "CURR_TASK=\"A\"\n",
    "test_loader = tqdm(dataloaders[CURR_TASK][\"test\"], desc=\"Test loop\")\n",
    "\n",
    "BATCH_INDEX = 0\n",
    "table = wandb.Table(columns=[\"ID\", \"Image\"])\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        # Move everything to the device\n",
    "        batch[\"pixel_values\"] = batch[\"pixel_values\"].to(device)\n",
    "        batch[\"pixel_mask\"] = batch[\"pixel_mask\"].to(device)\n",
    "        batch[\"mask_labels\"] = [entry.to(device) for entry in batch[\"mask_labels\"]]\n",
    "        batch[\"class_labels\"] = [entry.to(device) for entry in batch[\"class_labels\"]]\n",
    "        # Compute output and loss\n",
    "        outputs = model(**batch)\n",
    "\n",
    "        loss = outputs.loss\n",
    "        # Record losses\n",
    "        current_loss = loss.item() * batch[\"pixel_values\"].size(0)\n",
    "        test_running_loss += current_loss\n",
    "        test_loader.set_postfix(loss=f\"{current_loss:.4f}\")\n",
    "\n",
    "        # Extract and compute metrics\n",
    "        pred_maps, masks = m2f_extract_pred_maps_and_masks(\n",
    "            batch, outputs, m2f_preprocessor_A\n",
    "        )\n",
    "        metric.add_batch(references=masks, predictions=pred_maps)\n",
    "        if BATCH_INDEX <5:\n",
    "            # Visualize\n",
    "            log_table_of_images(\n",
    "                table, # common table for all batches\n",
    "                batch[\"pixel_values\"],\n",
    "                pixel_mean_A, # remove normalization\n",
    "                pixel_std_A, # remove normalization\n",
    "                pred_maps,\n",
    "                masks,\n",
    "                BATCH_INDEX, # correct indexing in table\n",
    "            )\n",
    "            BATCH_INDEX += 1\n",
    "# Log table\n",
    "wandb.log({f\"{CURR_TASK}_TEST_AFTER_TRAINING_A\": table})\n",
    "\n",
    "# After compute the batches that were added are deleted\n",
    "test_metrics_A = metric.compute(\n",
    "    num_labels=NUM_CLASSES, ignore_index=BG_VALUE_255, reduce_labels=False\n",
    ")\n",
    "mean_test_iou = test_metrics_A[\"mean_iou\"]\n",
    "final_test_loss = test_running_loss / len(dataloaders[CURR_TASK][\"test\"].dataset)\n",
    "\"\"\"wandb.log({\n",
    "    f\"Loss/test_{CURR_TASK}\": final_test_loss,\n",
    "    f\"mIoU/test_{CURR_TASK}\": mean_test_iou\n",
    "})\"\"\"\n",
    "print(f\"Test Loss: {final_test_loss:.4f}, Test mIoU: {mean_test_iou:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test results on B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test loop: 100%|██████████| 15/15 [00:21<00:00,  1.43s/it, loss=152.9794]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 48.3620, Test mIoU: 0.2589\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "test_running_loss = 0\n",
    "CURR_TASK=\"B\"\n",
    "test_loader = tqdm(dataloaders[CURR_TASK][\"test\"], desc=\"Test loop\")\n",
    "\n",
    "BATCH_INDEX = 0\n",
    "table = wandb.Table(columns=[\"ID\", \"Image\"])\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        # Move everything to the device\n",
    "        batch[\"pixel_values\"] = batch[\"pixel_values\"].to(device)\n",
    "        batch[\"pixel_mask\"] = batch[\"pixel_mask\"].to(device)\n",
    "        batch[\"mask_labels\"] = [entry.to(device) for entry in batch[\"mask_labels\"]]\n",
    "        batch[\"class_labels\"] = [entry.to(device) for entry in batch[\"class_labels\"]]\n",
    "        # Compute output and loss\n",
    "        outputs = model(**batch)\n",
    "\n",
    "        loss = outputs.loss\n",
    "        # Record losses\n",
    "        current_loss = loss.item() * batch[\"pixel_values\"].size(0)\n",
    "        test_running_loss += current_loss\n",
    "        test_loader.set_postfix(loss=f\"{current_loss:.4f}\")\n",
    "\n",
    "        # Extract and compute metrics\n",
    "        pred_maps, masks = m2f_extract_pred_maps_and_masks(\n",
    "            batch, outputs, m2f_preprocessor_B\n",
    "        )\n",
    "        metric.add_batch(references=masks, predictions=pred_maps)\n",
    "        if BATCH_INDEX <5:\n",
    "            # Visualize\n",
    "            log_table_of_images(\n",
    "                table, # common table for all batches\n",
    "                batch[\"pixel_values\"],\n",
    "                pixel_mean_B, # remove normalization\n",
    "                pixel_std_B, # remove normalization\n",
    "                pred_maps,\n",
    "                masks,\n",
    "                BATCH_INDEX, # correct indexing in table\n",
    "            )\n",
    "            BATCH_INDEX += 1\n",
    "\n",
    "# Log table\n",
    "wandb.log({f\"{CURR_TASK}_TEST_AFTER_TRAINING_A\": table})\n",
    "\n",
    "# After compute the batches that were added are deleted\n",
    "test_metrics_B_before = metric.compute(\n",
    "    num_labels=NUM_CLASSES, ignore_index=BG_VALUE_255, reduce_labels=False\n",
    ")\n",
    "mean_test_iou = test_metrics_B_before[\"mean_iou\"]\n",
    "final_test_loss = test_running_loss / len(dataloaders[CURR_TASK][\"test\"].dataset)\n",
    "\"\"\"wandb.log({\n",
    "    f\"Loss/test_{CURR_TASK}\": final_test_loss,\n",
    "    f\"mIoU/test_{CURR_TASK}\": mean_test_iou\n",
    "})\"\"\"\n",
    "print(f\"Test Loss: {final_test_loss:.4f}, Test mIoU: {mean_test_iou:.4f}\")\n",
    "#wandb.finish()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Now train on B and forget A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Training\n",
    "NUM_EPOCHS = 200\n",
    "LEARNING_RATE = 1e-4\n",
    "LR_MULTIPLIER = 0.1\n",
    "BACKBONE_LR = LEARNING_RATE * LR_MULTIPLIER\n",
    "WEIGHT_DECAY = 0.05\n",
    "PATIENCE=15\n",
    "model.train()\n",
    "metric = evaluate.load(\"mean_iou\")\n",
    "encoder_params = [\n",
    "    param\n",
    "    for name, param in model.named_parameters()\n",
    "    if name.startswith(\"model.pixel_level_module.encoder\")\n",
    "]\n",
    "decoder_params = [\n",
    "    param\n",
    "    for name, param in model.named_parameters()\n",
    "    if name.startswith(\"model.pixel_level_module.decoder\")\n",
    "]\n",
    "transformer_params = [\n",
    "    param\n",
    "    for name, param in model.named_parameters()\n",
    "    if name.startswith(\"model.transformer_module\")\n",
    "]\n",
    "class_prediction_params=[\n",
    "    param\n",
    "    for name, param in model.named_parameters() \n",
    "    if not name.startswith(\"model.pixel_level_module.encoder\") and not name.startswith(\"model.transformer_module\") and not name.startswith(\"model.pixel_level_module.decoder\")\n",
    "]\n",
    "optimizer = optim.AdamW(\n",
    "    [\n",
    "        {\"params\": encoder_params, \"lr\": BACKBONE_LR},\n",
    "        {\"params\": decoder_params},\n",
    "        {\"params\": transformer_params},\n",
    "        {\"params\": class_prediction_params}\n",
    "    ],\n",
    "    lr=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    ")\n",
    "\n",
    "scheduler = optim.lr_scheduler.PolynomialLR(\n",
    "    optimizer, total_iters=NUM_EPOCHS, power=0.9\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WandB for team usage !!!!\n",
    "\n",
    "wandb.login() # use this one if a different person is going to run the notebook\n",
    "#wandb.login(relogin=False) # if the same person in the last run is going to run the notebook again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/notebooks/continual-learning/wandb/run-20240605_000446-8ewtur1y</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/continual-learning-tum/M2F_latest_aug/runs/8ewtur1y' target=\"_blank\">M2F-Swin-Tiny-Replay-All_AugBlur</a></strong> to <a href='https://wandb.ai/continual-learning-tum/M2F_latest_aug' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/continual-learning-tum/M2F_latest_aug' target=\"_blank\">https://wandb.ai/continual-learning-tum/M2F_latest_aug</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/continual-learning-tum/M2F_latest_aug/runs/8ewtur1y' target=\"_blank\">https://wandb.ai/continual-learning-tum/M2F_latest_aug/runs/8ewtur1y</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wandb run id: 8ewtur1y\n"
     ]
    }
   ],
   "source": [
    "wandb.init(\n",
    "    project=project_name,\n",
    "    config={\n",
    "        \"learning_rate\": LEARNING_RATE,\n",
    "        \"learning_rate_multiplier\": LR_MULTIPLIER,\n",
    "        \"backbone_learning_rate\": BACKBONE_LR,\n",
    "        \"learning_rate_scheduler\": scheduler.__class__.__name__,\n",
    "        \"optimizer\": optimizer.__class__.__name__,\n",
    "        \"backbone\": SWIN_BACKBONE,\n",
    "        \"m2f_preprocessor\": m2f_preprocessor_B.__dict__,\n",
    "        \"m2f_model_config\": model.config\n",
    "    },\n",
    "    name=new_run_name,\n",
    "    notes=\"M2F with tiny Swin backbone pretrained on ImageNet-1K. \\\n",
    "        Scenario: Pretrained on A, Train on A + B naive finetuning (replay all), Test forgetting on A\"\n",
    ")\n",
    "\n",
    "print(\"wandb run id:\",wandb.run.id)\n",
    "\n",
    "# Tensorboard logging\n",
    "writer = SummaryWriter(log_dir=out_dir + \"runs\")\n",
    "\n",
    "# Model checkpointing\n",
    "model_dir = out_dir + \"models/\"\n",
    "if not os.path.exists(model_dir):\n",
    "    print(\"Store weights in: \", model_dir)\n",
    "    os.makedirs(model_dir)\n",
    "\n",
    "best_model_dir = model_dir + f\"{new_run_name}/best_model/\"\n",
    "if not os.path.exists(best_model_dir):\n",
    "    print(\"Store best model weights in: \", best_model_dir)\n",
    "    os.makedirs(best_model_dir)\n",
    "final_model_dir = model_dir + f\"{new_run_name}/final_model/\"\n",
    "if not os.path.exists(final_model_dir):\n",
    "    print(\"Store final model weights in: \", final_model_dir)\n",
    "    os.makedirs(final_model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the preprocessor\n",
    "m2f_preprocessor_B.save_pretrained(model_dir + new_run_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/200 Training: 100%|██████████| 334/334 [07:27<00:00,  1.34s/it, loss=190.2361]\n",
      "Epoch 1/200 Validation: 100%|██████████| 14/14 [00:13<00:00,  1.01it/s, loss=272.9021]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200, Train Loss: 13.6459, Train mIoU: 0.6826, Validation Loss: 14.3665, Validation mIoU: 0.5382\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/200 Training:  80%|████████  | 268/334 [06:03<01:29,  1.36s/it, loss=199.1889]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "matrix contains invalid numeric entries",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[18], line 31\u001b[0m\n\u001b[1;32m     28\u001b[0m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass_labels\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m [entry\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;28;01mfor\u001b[39;00m entry \u001b[38;5;129;01min\u001b[39;00m batch[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclass_labels\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[1;32m     30\u001b[0m \u001b[38;5;66;03m# Compute output and loss\u001b[39;00m\n\u001b[0;32m---> 31\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     33\u001b[0m loss \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mloss\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Compute gradient and perform step\u001b[39;00m\n",
      "File \u001b[0;32m/notebooks/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/notebooks/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/notebooks/venv/lib/python3.12/site-packages/transformers/models/mask2former/modeling_mask2former.py:2521\u001b[0m, in \u001b[0;36mMask2FormerForUniversalSegmentation.forward\u001b[0;34m(self, pixel_values, mask_labels, class_labels, pixel_mask, output_hidden_states, output_auxiliary_logits, output_attentions, return_dict)\u001b[0m\n\u001b[1;32m   2518\u001b[0m auxiliary_logits \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_auxiliary_logits(class_queries_logits, masks_queries_logits)\n\u001b[1;32m   2520\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask_labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m class_labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 2521\u001b[0m     loss_dict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loss_dict\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2522\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmasks_queries_logits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmasks_queries_logits\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2523\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_queries_logits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_queries_logits\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2524\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmask_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2525\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2526\u001b[0m \u001b[43m        \u001b[49m\u001b[43mauxiliary_predictions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauxiliary_logits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2527\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2528\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_loss(loss_dict)\n\u001b[1;32m   2530\u001b[0m encoder_hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/notebooks/venv/lib/python3.12/site-packages/transformers/models/mask2former/modeling_mask2former.py:2342\u001b[0m, in \u001b[0;36mMask2FormerForUniversalSegmentation.get_loss_dict\u001b[0;34m(self, masks_queries_logits, class_queries_logits, mask_labels, class_labels, auxiliary_predictions)\u001b[0m\n\u001b[1;32m   2334\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_loss_dict\u001b[39m(\n\u001b[1;32m   2335\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   2336\u001b[0m     masks_queries_logits: Tensor,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2340\u001b[0m     auxiliary_predictions: Dict[\u001b[38;5;28mstr\u001b[39m, Tensor],\n\u001b[1;32m   2341\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dict[\u001b[38;5;28mstr\u001b[39m, Tensor]:\n\u001b[0;32m-> 2342\u001b[0m     loss_dict: Dict[\u001b[38;5;28mstr\u001b[39m, Tensor] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcriterion\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2343\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmasks_queries_logits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmasks_queries_logits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2344\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_queries_logits\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_queries_logits\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2345\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmask_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmask_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2346\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_labels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mclass_labels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2347\u001b[0m \u001b[43m        \u001b[49m\u001b[43mauxiliary_predictions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mauxiliary_predictions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2348\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2350\u001b[0m     \u001b[38;5;66;03m# weight each loss by `self.weight_dict[<LOSS_NAME>]` including auxiliary losses\u001b[39;00m\n\u001b[1;32m   2351\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, weight \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweight_dict\u001b[38;5;241m.\u001b[39mitems():\n",
      "File \u001b[0;32m/notebooks/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/notebooks/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/notebooks/venv/lib/python3.12/site-packages/transformers/models/mask2former/modeling_mask2former.py:768\u001b[0m, in \u001b[0;36mMask2FormerLoss.forward\u001b[0;34m(self, masks_queries_logits, class_queries_logits, mask_labels, class_labels, auxiliary_predictions)\u001b[0m\n\u001b[1;32m    740\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    741\u001b[0m \u001b[38;5;124;03mThis performs the loss computation.\u001b[39;00m\n\u001b[1;32m    742\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    764\u001b[0m \u001b[38;5;124;03m    losses for each auxiliary predictions.\u001b[39;00m\n\u001b[1;32m    765\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    767\u001b[0m \u001b[38;5;66;03m# retrieve the matching between the outputs of the last layer and the labels\u001b[39;00m\n\u001b[0;32m--> 768\u001b[0m indices \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatcher\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmasks_queries_logits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_queries_logits\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmask_labels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclass_labels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    769\u001b[0m \u001b[38;5;66;03m# compute the average number of target masks for normalization purposes\u001b[39;00m\n\u001b[1;32m    770\u001b[0m num_masks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_num_masks(class_labels, device\u001b[38;5;241m=\u001b[39mclass_labels[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdevice)\n",
      "File \u001b[0;32m/notebooks/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/notebooks/venv/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/notebooks/venv/lib/python3.12/site-packages/torch/utils/_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(func)\n\u001b[1;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdecorate_context\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    114\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/notebooks/venv/lib/python3.12/site-packages/transformers/models/mask2former/modeling_mask2former.py:480\u001b[0m, in \u001b[0;36mMask2FormerHungarianMatcher.forward\u001b[0;34m(self, masks_queries_logits, class_queries_logits, mask_labels, class_labels)\u001b[0m\n\u001b[1;32m    478\u001b[0m     cost_matrix \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmaximum(cost_matrix, torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1e10\u001b[39m))\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;66;03m# do the assigmented using the hungarian algorithm in scipy\u001b[39;00m\n\u001b[0;32m--> 480\u001b[0m     assigned_indices: Tuple[np\u001b[38;5;241m.\u001b[39marray] \u001b[38;5;241m=\u001b[39m \u001b[43mlinear_sum_assignment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcost_matrix\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    481\u001b[0m     indices\u001b[38;5;241m.\u001b[39mappend(assigned_indices)\n\u001b[1;32m    483\u001b[0m \u001b[38;5;66;03m# It could be stacked in one tensor\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: matrix contains invalid numeric entries"
     ]
    }
   ],
   "source": [
    "# To avoid making stupid errors\n",
    "CURR_TASK = \"B\"\n",
    "model_path_second = f\"{best_model_dir}A+{CURR_TASK}/\"\n",
    "\n",
    "\n",
    "# For storing the model\n",
    "best_val_metric = -np.inf\n",
    "best_model_weights=None # best model weights are stored here\n",
    "\n",
    "# Move model to device\n",
    "model.to(device)\n",
    "counter=0\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    model.train()\n",
    "    train_running_loss = 0.0\n",
    "    val_running_loss = 0.0\n",
    "\n",
    "    # Set up tqdm for the training loop\n",
    "    train_loader = tqdm(\n",
    "        dataloaders[CURR_TASK][\"train\"], desc=f\"Epoch {epoch + 1}/{NUM_EPOCHS} Training\"\n",
    "    )\n",
    "\n",
    "    for batch in train_loader:\n",
    "        # Move everything to the device\n",
    "        batch[\"pixel_values\"] = batch[\"pixel_values\"].to(device)\n",
    "        batch[\"pixel_mask\"] = batch[\"pixel_mask\"].to(device)\n",
    "        batch[\"mask_labels\"] = [entry.to(device) for entry in batch[\"mask_labels\"]]\n",
    "        batch[\"class_labels\"] = [entry.to(device) for entry in batch[\"class_labels\"]]\n",
    "\n",
    "        # Compute output and loss\n",
    "        outputs = model(**batch)\n",
    "\n",
    "        loss = outputs.loss\n",
    "\n",
    "        # Compute gradient and perform step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Record losses\n",
    "        current_loss = loss.item() * batch[\"pixel_values\"].size(0)\n",
    "        train_running_loss += current_loss\n",
    "        train_loader.set_postfix(loss=f\"{current_loss:.4f}\")\n",
    "\n",
    "        # Extract and compute metrics\n",
    "        pred_maps, masks = m2f_extract_pred_maps_and_masks(\n",
    "            batch, outputs, m2f_preprocessor_B\n",
    "        )\n",
    "        metric.add_batch(references=masks, predictions=pred_maps)\n",
    "        \n",
    "\n",
    "    # After compute the batches that were added are deleted\n",
    "    mean_train_iou = metric.compute(\n",
    "        num_labels=NUM_CLASSES, ignore_index=BG_VALUE_255, reduce_labels=False\n",
    "    )[\"mean_iou\"]\n",
    "\n",
    "    # Validation phase\n",
    "    model.eval()\n",
    "    val_loader = tqdm(\n",
    "        dataloaders[CURR_TASK][\"val\"], desc=f\"Epoch {epoch + 1}/{NUM_EPOCHS} Validation\"\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            # Move everything to the device\n",
    "            batch[\"pixel_values\"] = batch[\"pixel_values\"].to(device)\n",
    "            batch[\"pixel_mask\"] = batch[\"pixel_mask\"].to(device)\n",
    "            batch[\"mask_labels\"] = [entry.to(device) for entry in batch[\"mask_labels\"]]\n",
    "            batch[\"class_labels\"] = [\n",
    "                entry.to(device) for entry in batch[\"class_labels\"]\n",
    "            ]\n",
    "            # Compute output and loss\n",
    "            outputs = model(**batch)\n",
    "\n",
    "            loss = outputs.loss\n",
    "            # Record losses\n",
    "            current_loss = loss.item() * batch[\"pixel_values\"].size(0)\n",
    "            val_running_loss += current_loss\n",
    "            val_loader.set_postfix(loss=f\"{current_loss:.4f}\")\n",
    "\n",
    "            # Extract and compute metrics\n",
    "            pred_maps, masks = m2f_extract_pred_maps_and_masks(\n",
    "                batch, outputs, m2f_preprocessor_B\n",
    "            )\n",
    "            metric.add_batch(references=masks, predictions=pred_maps)\n",
    "            \n",
    "\n",
    "    # After compute the batches that were added are deleted\n",
    "    mean_val_iou = metric.compute(\n",
    "        num_labels=NUM_CLASSES, ignore_index=BG_VALUE_255, reduce_labels=False\n",
    "    )[\"mean_iou\"]\n",
    "\n",
    "    epoch_train_loss = train_running_loss / len(dataloaders[CURR_TASK][\"train\"].dataset)\n",
    "    epoch_val_loss = val_running_loss / len(dataloaders[CURR_TASK][\"val\"].dataset)\n",
    "\n",
    "    writer.add_scalar(f\"Loss/train_{new_run_name}_{CURR_TASK}\", epoch_train_loss, epoch + 1)\n",
    "    writer.add_scalar(f\"Loss/val_{new_run_name}_{CURR_TASK}\", epoch_val_loss, epoch + 1)\n",
    "    writer.add_scalar(f\"mIoU/train_{new_run_name}_{CURR_TASK}\", mean_train_iou, epoch + 1)\n",
    "    writer.add_scalar(f\"mIoU/val_{new_run_name}_{CURR_TASK}\", mean_val_iou, epoch + 1)\n",
    "\n",
    "    wandb.log({\n",
    "        f\"Loss/train_A+{CURR_TASK}\": epoch_train_loss,\n",
    "        f\"Loss/val_A+{CURR_TASK}\": epoch_val_loss,\n",
    "        f\"mIoU/train_A+{CURR_TASK}\": mean_train_iou,\n",
    "        f\"mIoU/val_A+{CURR_TASK}\": mean_val_iou\n",
    "    })\n",
    "\n",
    "    tqdm.write(\n",
    "        f\"Epoch {epoch + 1}/{NUM_EPOCHS}, Train Loss: {epoch_train_loss:.4f}, Train mIoU: {mean_train_iou:.4f}, Validation Loss: {epoch_val_loss:.4f}, Validation mIoU: {mean_val_iou:.4f}\"\n",
    "    )\n",
    "    if mean_val_iou > best_val_metric:\n",
    "        best_val_metric = mean_val_iou\n",
    "        #model.save_pretrained(model_path_second)\n",
    "        best_model_weights = deepcopy(model.state_dict())\n",
    "        counter=0\n",
    "    else:\n",
    "        counter+=1\n",
    "        if counter == PATIENCE:\n",
    "            print(\"Early stopping at epoch\",epoch)\n",
    "            break\n",
    "            \n",
    "\n",
    "os.makedirs(f\"{model_path_second}\",exist_ok=True)\n",
    "artifact = wandb.Artifact(f\"best_model_{new_run_name}\", type=\"model\")\n",
    "artifact.add_file(f\"{model_path_second}/best_model_{new_run_name}.pth\", torch.save(best_model_weights, f\"{model_path_second}/best_model_{new_run_name}.pth\"))\n",
    "wandb.run.log_artifact(artifact)\n",
    "\n",
    "if os.path.exists(model_dir + f\"{new_run_name}\"):\n",
    "    shutil.rmtree(model_dir + f\"{new_run_name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"training done\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Test results on B first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model and evaluate on test\n",
    "#model = Mask2FormerForUniversalSegmentation.from_pretrained(model_path_second).to(device)\n",
    "\n",
    "# Construct the artifact path\n",
    "artifact_path = f\"{user_or_team}/{project_name}/best_model_{new_run_name}:latest\"\n",
    "\n",
    "# Load from W&B\n",
    "api = wandb.Api()\n",
    "artifact=api.artifact(artifact_path)\n",
    "model_dir=artifact.download()\n",
    "model_state_dict_path = os.path.join(model_dir, f\"best_model_{new_run_name}.pth\" )\n",
    "model_state_dict = torch.load(model_state_dict_path)\n",
    "model = Mask2FormerForUniversalSegmentation(mask2former_config)\n",
    "model.load_state_dict(model_state_dict)\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "test_running_loss = 0\n",
    "CURR_TASK=\"B\"\n",
    "test_loader = tqdm(dataloaders[CURR_TASK][\"test\"], desc=\"Test loop\")\n",
    "BATCH_INDEX = 0\n",
    "table = wandb.Table(columns=[\"ID\", \"Image\"])\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        # Move everything to the device\n",
    "        batch[\"pixel_values\"] = batch[\"pixel_values\"].to(device)\n",
    "        batch[\"pixel_mask\"] = batch[\"pixel_mask\"].to(device)\n",
    "        batch[\"mask_labels\"] = [entry.to(device) for entry in batch[\"mask_labels\"]]\n",
    "        batch[\"class_labels\"] = [entry.to(device) for entry in batch[\"class_labels\"]]\n",
    "        # Compute output and loss\n",
    "        outputs = model(**batch)\n",
    "\n",
    "        loss = outputs.loss\n",
    "        # Record losses\n",
    "        current_loss = loss.item() * batch[\"pixel_values\"].size(0)\n",
    "        test_running_loss += current_loss\n",
    "        test_loader.set_postfix(loss=f\"{current_loss:.4f}\")\n",
    "\n",
    "        # Extract and compute metrics\n",
    "        pred_maps, masks = m2f_extract_pred_maps_and_masks(\n",
    "            batch, outputs, m2f_preprocessor_B\n",
    "        )\n",
    "        metric.add_batch(references=masks, predictions=pred_maps)\n",
    "        if BATCH_INDEX <5:\n",
    "            # Visualize\n",
    "            log_table_of_images(\n",
    "                table, # common table for all batches\n",
    "                batch[\"pixel_values\"],\n",
    "                pixel_mean_B, # remove normalization\n",
    "                pixel_std_B, # remove normalization\n",
    "                pred_maps,\n",
    "                masks,\n",
    "                BATCH_INDEX, # correct indexing in table\n",
    "            )\n",
    "            BATCH_INDEX += 1\n",
    "# Log table\n",
    "wandb.log({f\"{CURR_TASK}_TEST_AFTER_TRAINING_B\": table})\n",
    "\n",
    "# After compute the batches that were added are deleted\n",
    "test_metrics_B = metric.compute(\n",
    "    num_labels=NUM_CLASSES, ignore_index=BG_VALUE_255, reduce_labels=False\n",
    ")\n",
    "mean_test_iou = test_metrics_B[\"mean_iou\"]\n",
    "final_test_loss = test_running_loss / len(dataloaders[CURR_TASK][\"test\"].dataset)\n",
    "wandb.log({\n",
    "    f\"Loss/test_{CURR_TASK}\": final_test_loss,\n",
    "    f\"mIoU/test_{CURR_TASK}\": mean_test_iou\n",
    "})\n",
    "print(f\"Test Loss: {final_test_loss:.4f}, Test mIoU: {mean_test_iou:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Test results on A after training on B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To avoid making stupid errors\n",
    "CURR_TASK = \"A\"\n",
    "\n",
    "model.eval()\n",
    "test_running_loss = 0\n",
    "test_loader = tqdm(dataloaders[CURR_TASK][\"test\"], desc=\"Test loop\")\n",
    "BATCH_INDEX = 0\n",
    "table = wandb.Table(columns=[\"ID\", \"Image\"])\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        # Move everything to the device\n",
    "        batch[\"pixel_values\"] = batch[\"pixel_values\"].to(device)\n",
    "        batch[\"pixel_mask\"] = batch[\"pixel_mask\"].to(device)\n",
    "        batch[\"mask_labels\"] = [entry.to(device) for entry in batch[\"mask_labels\"]]\n",
    "        batch[\"class_labels\"] = [entry.to(device) for entry in batch[\"class_labels\"]]\n",
    "        # Compute output and loss\n",
    "        outputs = model(**batch)\n",
    "\n",
    "        loss = outputs.loss\n",
    "        # Record losses\n",
    "        current_loss = loss.item() * batch[\"pixel_values\"].size(0)\n",
    "        test_running_loss += current_loss\n",
    "        test_loader.set_postfix(loss=f\"{current_loss:.4f}\")\n",
    "\n",
    "        # Extract and compute metrics\n",
    "        pred_maps, masks = m2f_extract_pred_maps_and_masks(\n",
    "            batch, outputs, m2f_preprocessor_A\n",
    "        )\n",
    "        metric.add_batch(references=masks, predictions=pred_maps)\n",
    "        if BATCH_INDEX <5:\n",
    "            # Visualize\n",
    "            log_table_of_images(\n",
    "                table, # common table for all batches\n",
    "                batch[\"pixel_values\"],\n",
    "                pixel_mean_A, # remove normalization\n",
    "                pixel_std_A, # remove normalization\n",
    "                pred_maps,\n",
    "                masks,\n",
    "                BATCH_INDEX, # correct indexing in table\n",
    "            )\n",
    "            BATCH_INDEX += 1\n",
    "# Log table\n",
    "wandb.log({f\"{CURR_TASK}_TEST_AFTER_TRAINING_B\": table})\n",
    "\n",
    "# After compute the batches that were added are deleted\n",
    "test_metrics_forgetting_A = metric.compute(\n",
    "    num_labels=NUM_CLASSES, ignore_index=BG_VALUE_255, reduce_labels=False\n",
    ")\n",
    "mean_test_iou = test_metrics_forgetting_A[\"mean_iou\"]\n",
    "final_test_loss = test_running_loss / len(dataloaders[CURR_TASK][\"test\"].dataset)\n",
    "wandb.log({\n",
    "    f\"Loss/test_replay_all_{CURR_TASK}\": final_test_loss,\n",
    "    f\"mIoU/test_replay_all_{CURR_TASK}\": mean_test_iou\n",
    "})\n",
    "print(f\"Test Loss: {final_test_loss:.4f}, Test mIoU: {mean_test_iou:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Collect overall mIoU\n",
    "mIoU_A_before = test_metrics_A[\"mean_iou\"]\n",
    "mIoU_B_before=test_metrics_B_before[\"mean_iou\"]\n",
    "mIoU_forgetting_A = test_metrics_forgetting_A[\"mean_iou\"]\n",
    "mIoU_B = test_metrics_B[\"mean_iou\"]\n",
    "\n",
    "# Collect per category mIoU\n",
    "per_category_mIoU_A_before = np.array(test_metrics_A[\"per_category_iou\"])\n",
    "per_category_mIoU_A = np.array(test_metrics_forgetting_A[\"per_category_iou\"])\n",
    "per_category_mIoU_B = np.array(test_metrics_B[\"per_category_iou\"])\n",
    "per_category_mIoU_B_before=np.array(test_metrics_B_before[\"per_category_iou\"])\n",
    "\n",
    "# Average learning accuracies (mIoUs)\n",
    "avg_learning_acc = (mIoU_A_before + mIoU_B) / 2\n",
    "per_category_avg_learning_acc = (per_category_mIoU_A_before + per_category_mIoU_B) / 2\n",
    "\n",
    "# Forgetting\n",
    "total_forgetting = mIoU_A_before - mIoU_forgetting_A\n",
    "per_category_forgetting = (per_category_mIoU_A_before - per_category_mIoU_A)\n",
    "\n",
    "# Export evaluation metrics to WandB\n",
    "wandb.log({\n",
    "    \"eval/avg_learning_acc\": avg_learning_acc,\n",
    "    \"eval/total_forgetting\": total_forgetting,\n",
    "})\n",
    "\n",
    "columns=[\"categories\",\"per_category_mIoU_A_before\",\"per_category_mIoU_B_before\",\n",
    "         \"per_category_mIoU_B\", \"per_category_mIoU_A\",\n",
    "         \"per_category_avg_learning_acc\",\"per_category_forgetting\"]\n",
    "data=[]\n",
    "\n",
    "data.append([\"background\",per_category_mIoU_A_before[0],\n",
    "                 per_category_mIoU_B_before[0],\n",
    "                 per_category_mIoU_B[0],\n",
    "                per_category_mIoU_A[0],per_category_avg_learning_acc[0],\n",
    "                per_category_forgetting[0]])\n",
    "\n",
    "for cat_id in range(1,12):\n",
    "    data.append([ZEISS_CATEGORIES[cat_id],per_category_mIoU_A_before[cat_id],\n",
    "                 per_category_mIoU_B_before[cat_id],\n",
    "                 per_category_mIoU_B[cat_id],\n",
    "                per_category_mIoU_A[cat_id],per_category_avg_learning_acc[cat_id],\n",
    "                per_category_forgetting[cat_id]])\n",
    "    \n",
    "    \n",
    "table = wandb.Table(columns=columns, data=data)\n",
    "wandb.log({\"per_category_metrics_table\": table})\n",
    "\n",
    "print(\"**** Overall mIoU ****\")\n",
    "print(f\"mIoU on task A before training on B: {mIoU_A_before}\")\n",
    "print(f\"mIoU on task B before training on B: {mIoU_B_before}\")\n",
    "print(\"\\n\")\n",
    "print(f\"mIoU on task B after training on B: {mIoU_B}\")\n",
    "print(f\"mIoU on task A after training on B: {mIoU_forgetting_A}\")\n",
    "\n",
    "print(\"\\n**** Per category mIoU ****\")\n",
    "print(f\"Per category mIoU on task A before training on B: {per_category_mIoU_A_before}\")\n",
    "print(f\"Per category mIoU on task B before training on B: {per_category_mIoU_B_before}\")\n",
    "print(\"\\n\")\n",
    "print(f\"Per category mIoU on task B after training on B: {per_category_mIoU_B}\")\n",
    "print(f\"Per category mIoU on task A after training on B: {per_category_mIoU_A}\")\n",
    "\n",
    "print(\"\\n**** Average learning accuracies ****\")\n",
    "print(f\"Average learning acc.: {avg_learning_acc}\")\n",
    "print(f\"Per category Average learning acc.: {per_category_avg_learning_acc}\")\n",
    "\n",
    "print(\"\\n**** Forgetting ****\")\n",
    "print(f\"Total forgetting: {total_forgetting}\")\n",
    "print(f\"Per category forgetting: {per_category_forgetting}\")\n",
    "wandb.finish()\n",
    "\n",
    "if os.path.exists(\"artifacts/\"):\n",
    "    shutil.rmtree(\"artifacts/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
