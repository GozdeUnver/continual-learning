{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-20T09:29:17.481286Z",
     "iopub.status.busy": "2024-06-20T09:29:17.480924Z",
     "iopub.status.idle": "2024-06-20T09:29:17.505518Z",
     "shell.execute_reply": "2024-06-20T09:29:17.504457Z",
     "shell.execute_reply.started": "2024-06-20T09:29:17.481260Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-20T09:29:18.250010Z",
     "iopub.status.busy": "2024-06-20T09:29:18.249580Z",
     "iopub.status.idle": "2024-06-20T09:29:52.700931Z",
     "shell.execute_reply": "2024-06-20T09:29:52.699926Z",
     "shell.execute_reply.started": "2024-06-20T09:29:18.249978Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/notebooks/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "from utils.common import (\n",
    "    m2f_dataset_collate,\n",
    "    m2f_extract_pred_maps_and_masks,\n",
    "    set_seed,\n",
    "    CADIS_PIXEL_MEAN,\n",
    "    CADIS_PIXEL_STD,\n",
    "    CAT1K_PIXEL_MEAN,\n",
    "    CAT1K_PIXEL_STD,\n",
    ")\n",
    "from utils.kd import compute_kd_loss\n",
    "from utils.dataset_utils import (\n",
    "    get_cadisv2_dataset,\n",
    "    get_cataract1k_dataset,\n",
    "    ZEISS_CATEGORIES,\n",
    ")\n",
    "from utils.medical_datasets import Mask2FormerDataset\n",
    "from transformers import (\n",
    "    Mask2FormerForUniversalSegmentation,\n",
    "    SwinModel,\n",
    "    SwinConfig,\n",
    "    Mask2FormerConfig,\n",
    "    AutoImageProcessor,\n",
    "    Mask2FormerImageProcessor,\n",
    ")\n",
    "from torch.utils.data import DataLoader\n",
    "import evaluate\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import numpy as np\n",
    "import wandb\n",
    "from copy import deepcopy\n",
    "import shutil\n",
    "from utils.wandb_utils import log_table_of_images\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-20T09:29:52.702377Z",
     "iopub.status.busy": "2024-06-20T09:29:52.702240Z",
     "iopub.status.idle": "2024-06-20T09:29:52.804328Z",
     "shell.execute_reply": "2024-06-20T09:29:52.803224Z",
     "shell.execute_reply.started": "2024-06-20T09:29:52.702354Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set as 42\n"
     ]
    }
   ],
   "source": [
    "set_seed(42) # seed everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-20T09:29:52.805794Z",
     "iopub.status.busy": "2024-06-20T09:29:52.805544Z",
     "iopub.status.idle": "2024-06-20T09:29:57.816705Z",
     "shell.execute_reply": "2024-06-20T09:29:57.815738Z",
     "shell.execute_reply.started": "2024-06-20T09:29:52.805772Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/notebooks/venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Not Matched: hidden_states_norms.stage1.weight != layernorm.weight\n",
      "Not Matched: hidden_states_norms.stage1.bias != layernorm.bias\n"
     ]
    }
   ],
   "source": [
    "NUM_CLASSES = (\n",
    "    len(ZEISS_CATEGORIES) - 3 + 1\n",
    ")  # Remove class incremental and add background!!!\n",
    "SWIN_BACKBONE = \"microsoft/swin-tiny-patch4-window7-224\"  # \"microsoft/swin-large-patch4-window12-384\"\n",
    "\n",
    "# Download pretrained swin model\n",
    "swin_model = SwinModel.from_pretrained(\n",
    "    SWIN_BACKBONE, out_features=[\"stage1\", \"stage2\", \"stage3\", \"stage4\"]\n",
    ")\n",
    "swin_config = SwinConfig.from_pretrained(\n",
    "    SWIN_BACKBONE, out_features=[\"stage1\", \"stage2\", \"stage3\", \"stage4\"]\n",
    ")\n",
    "\n",
    "# Create Mask2Former configuration based on Swin's configuration\n",
    "mask2former_config = Mask2FormerConfig(\n",
    "    backbone_config=swin_config, num_labels=NUM_CLASSES  # , ignore_value=BG_VALUE\n",
    ")\n",
    "\n",
    "# Create the Mask2Former model with this configuration\n",
    "student = Mask2FormerForUniversalSegmentation(mask2former_config)\n",
    "\n",
    "# Reuse pretrained parameters\n",
    "for swin_param, m2f_param in zip(\n",
    "    swin_model.named_parameters(),\n",
    "    student.model.pixel_level_module.encoder.named_parameters(),\n",
    "):\n",
    "    m2f_param_name = f\"model.pixel_level_module.encoder.{m2f_param[0]}\"\n",
    "\n",
    "    if swin_param[0] == m2f_param[0]:\n",
    "        student.state_dict()[m2f_param_name].copy_(swin_param[1])\n",
    "        continue\n",
    "\n",
    "    print(f\"Not Matched: {m2f_param[0]} != {swin_param[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-20T09:29:57.818967Z",
     "iopub.status.busy": "2024-06-20T09:29:57.818714Z",
     "iopub.status.idle": "2024-06-20T09:29:58.557806Z",
     "shell.execute_reply": "2024-06-20T09:29:58.556973Z",
     "shell.execute_reply.started": "2024-06-20T09:29:57.818946Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/notebooks/venv/lib/python3.12/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Could not find image processor class in the image processor config or the model config. Loading based on pattern matching with the model's feature extractor configuration. Please open a PR/issue to update `preprocessor_config.json` to use `image_processor_type` instead of `feature_extractor_type`. This warning will be removed in v4.40.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A': {'train': <torch.utils.data.dataloader.DataLoader object at 0x7feb41fc6240>, 'val': <torch.utils.data.dataloader.DataLoader object at 0x7feb4289b080>, 'test': <torch.utils.data.dataloader.DataLoader object at 0x7feb41fc7a10>}, 'B': {'train': <torch.utils.data.dataloader.DataLoader object at 0x7feb41fc6330>, 'val': <torch.utils.data.dataloader.DataLoader object at 0x7feb41fc6210>, 'test': <torch.utils.data.dataloader.DataLoader object at 0x7feb41fc6450>}}\n"
     ]
    }
   ],
   "source": [
    "# Helper function to load datasets\n",
    "def load_dataset(dataset_getter, data_path, domain_incremental):\n",
    "    return dataset_getter(data_path, domain_incremental=domain_incremental)\n",
    "\n",
    "\n",
    "SWIN_BACKBONE = \"microsoft/swin-tiny-patch4-window7-224\"  # \"microsoft/swin-large-patch4-window12-384\"\n",
    "\n",
    "\n",
    "# Helper function to create dataloaders for a dataset\n",
    "def create_dataloaders(\n",
    "    dataset, batch_size, shuffle, num_workers, drop_last, pin_memory, collate_fn\n",
    "):\n",
    "    return {\n",
    "        \"train\": DataLoader(\n",
    "            dataset[\"train\"],\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle,\n",
    "            num_workers=num_workers,\n",
    "            drop_last=drop_last,\n",
    "            pin_memory=pin_memory,\n",
    "            collate_fn=collate_fn,\n",
    "        ),\n",
    "        \"val\": DataLoader(\n",
    "            dataset[\"val\"],\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle,\n",
    "            num_workers=num_workers,\n",
    "            drop_last=drop_last,\n",
    "            pin_memory=pin_memory,\n",
    "            collate_fn=collate_fn,\n",
    "        ),\n",
    "        \"test\": DataLoader(\n",
    "            dataset[\"test\"],\n",
    "            batch_size=batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=num_workers,\n",
    "            drop_last=False,\n",
    "            pin_memory=pin_memory,\n",
    "            collate_fn=collate_fn,\n",
    "        ),\n",
    "    }\n",
    "\n",
    "\n",
    "# Load datasets\n",
    "datasets = {\n",
    "    \"A\": load_dataset(get_cadisv2_dataset, \"../../storage/data/CaDISv2\", True),\n",
    "    \"B\": load_dataset(get_cataract1k_dataset, \"../../storage/data/cataract-1k\", True),\n",
    "}\n",
    "\n",
    "\n",
    "pixel_mean_A = np.array(CADIS_PIXEL_MEAN)\n",
    "pixel_std_A = np.array(CADIS_PIXEL_STD)\n",
    "pixel_mean_B = np.array(CAT1K_PIXEL_MEAN)\n",
    "pixel_std_B = np.array(CAT1K_PIXEL_STD)\n",
    "\n",
    "\n",
    "# Define preprocessor\n",
    "swin_processor = AutoImageProcessor.from_pretrained(SWIN_BACKBONE)\n",
    "m2f_preprocessor_A = Mask2FormerImageProcessor(\n",
    "    reduce_labels=False,\n",
    "    ignore_index=255,\n",
    "    do_resize=False,\n",
    "    do_rescale=False,\n",
    "    do_normalize=True,\n",
    "    image_std=pixel_std_A,\n",
    "    image_mean=pixel_mean_A,\n",
    ")\n",
    "\n",
    "m2f_preprocessor_B = Mask2FormerImageProcessor(\n",
    "    reduce_labels=False,\n",
    "    ignore_index=255,\n",
    "    do_resize=False,\n",
    "    do_rescale=False,\n",
    "    do_normalize=True,\n",
    "    image_std=pixel_std_B,\n",
    "    image_mean=pixel_mean_B,\n",
    ")\n",
    "\n",
    "# Create Mask2Former Datasets\n",
    "m2f_datasets = {\n",
    "    \"A\": {\n",
    "        \"train\": Mask2FormerDataset(datasets[\"A\"][0], m2f_preprocessor_A),\n",
    "        \"val\": Mask2FormerDataset(datasets[\"A\"][1], m2f_preprocessor_A),\n",
    "        \"test\": Mask2FormerDataset(datasets[\"A\"][2], m2f_preprocessor_A),\n",
    "    },\n",
    "    \"B\": {\n",
    "        \"train\": Mask2FormerDataset(datasets[\"B\"][0], m2f_preprocessor_B),\n",
    "        \"val\": Mask2FormerDataset(datasets[\"B\"][1], m2f_preprocessor_B),\n",
    "        \"test\": Mask2FormerDataset(datasets[\"B\"][2], m2f_preprocessor_B),\n",
    "    },\n",
    "}\n",
    "\n",
    "# DataLoader parameters\n",
    "N_WORKERS = 4\n",
    "BATCH_SIZE = 16\n",
    "SHUFFLE = True\n",
    "DROP_LAST = True\n",
    "\n",
    "dataloader_params = {\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"shuffle\": SHUFFLE,\n",
    "    \"num_workers\": N_WORKERS,\n",
    "    \"drop_last\": DROP_LAST,\n",
    "    \"pin_memory\": True,\n",
    "    \"collate_fn\": m2f_dataset_collate,\n",
    "}\n",
    "\n",
    "# Create DataLoaders\n",
    "dataloaders = {\n",
    "    key: create_dataloaders(m2f_datasets[key], **dataloader_params)\n",
    "    for key in m2f_datasets\n",
    "}\n",
    "\n",
    "print(dataloaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-20T09:29:58.559694Z",
     "iopub.status.busy": "2024-06-20T09:29:58.559245Z",
     "iopub.status.idle": "2024-06-20T09:30:03.097719Z",
     "shell.execute_reply": "2024-06-20T09:30:03.097056Z",
     "shell.execute_reply.started": "2024-06-20T09:29:58.559659Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Check if CUDA is available, otherwise use CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "BG_VALUE_255=255\n",
    "base_run_name=\"M2F-Swin-Tiny-Train_Cadis\"\n",
    "new_run_name=\"M2F-Swin-Tiny-KD-EMA-Mean-Loss-Batch-Replay\"\n",
    "project_name = \"M2F_latest\"\n",
    "user_or_team = \"continual-learning-tum\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-20T09:30:03.099555Z",
     "iopub.status.busy": "2024-06-20T09:30:03.098567Z",
     "iopub.status.idle": "2024-06-20T09:30:08.203981Z",
     "shell.execute_reply": "2024-06-20T09:30:08.203074Z",
     "shell.execute_reply.started": "2024-06-20T09:30:03.099555Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "      <iframe id=\"tensorboard-frame-1c80317fa3b1799d\" width=\"100%\" height=\"800\" frameborder=\"0\">\n",
       "      </iframe>\n",
       "      <script>\n",
       "        (function() {\n",
       "          const frame = document.getElementById(\"tensorboard-frame-1c80317fa3b1799d\");\n",
       "          const url = new URL(\"/\", window.location);\n",
       "          const port = 6006;\n",
       "          if (port) {\n",
       "            url.port = port;\n",
       "          }\n",
       "          frame.src = url;\n",
       "        })();\n",
       "      </script>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tensorboard setup\n",
    "out_dir=\"outputs/\"\n",
    "if not os.path.exists(out_dir):\n",
    "    os.makedirs(out_dir)\n",
    "if not os.path.exists(out_dir+\"runs\"):\n",
    "    os.makedirs(out_dir+\"runs\")\n",
    "%load_ext tensorboard\n",
    "%tensorboard --logdir outputs/runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-20T09:30:08.205738Z",
     "iopub.status.busy": "2024-06-20T09:30:08.205468Z",
     "iopub.status.idle": "2024-06-20T09:30:09.093404Z",
     "shell.execute_reply": "2024-06-20T09:30:09.092102Z",
     "shell.execute_reply.started": "2024-06-20T09:30:08.205738Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "!CUDA_LAUNCH_BLOCKING=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-20T09:30:09.096144Z",
     "iopub.status.busy": "2024-06-20T09:30:09.095346Z",
     "iopub.status.idle": "2024-06-20T09:30:09.173861Z",
     "shell.execute_reply": "2024-06-20T09:30:09.173039Z",
     "shell.execute_reply.started": "2024-06-20T09:30:09.096115Z"
    }
   },
   "outputs": [],
   "source": [
    "# Tensorboard logging\n",
    "writer = SummaryWriter(log_dir=out_dir + \"runs\")\n",
    "\n",
    "# Model checkpointing\n",
    "model_dir = out_dir + \"models/\"\n",
    "if not os.path.exists(model_dir):\n",
    "    print(\"Store weights in: \", model_dir)\n",
    "    os.makedirs(model_dir)\n",
    "\n",
    "best_model_dir = model_dir + f\"{base_run_name}/best_model/\"\n",
    "if not os.path.exists(best_model_dir):\n",
    "    print(\"Store best model weights in: \", best_model_dir)\n",
    "    os.makedirs(best_model_dir)\n",
    "final_model_dir = model_dir + f\"{base_run_name}/final_model/\"\n",
    "if not os.path.exists(final_model_dir):\n",
    "    print(\"Store final model weights in: \", final_model_dir)\n",
    "    os.makedirs(final_model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Test results on A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-20T09:30:09.175519Z",
     "iopub.status.busy": "2024-06-20T09:30:09.174729Z",
     "iopub.status.idle": "2024-06-20T09:30:13.323986Z",
     "shell.execute_reply": "2024-06-20T09:30:13.323389Z",
     "shell.execute_reply.started": "2024-06-20T09:30:09.175498Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ········································\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# WandB for team usage !!!!\n",
    "\n",
    "wandb.login() # use this one if a different person is going to run the notebook\n",
    "#wandb.login(relogin=False) # if the same person in the last run is going to run the notebook again"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-20T09:30:13.326928Z",
     "iopub.status.busy": "2024-06-20T09:30:13.326108Z",
     "iopub.status.idle": "2024-06-20T09:30:23.671397Z",
     "shell.execute_reply": "2024-06-20T09:30:23.670424Z",
     "shell.execute_reply.started": "2024-06-20T09:30:13.326907Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact best_model_M2F-Swin-Tiny-Train_Cadis:latest, 181.31MB. 1 files... \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n",
      "Done. 0:0:4.3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Mask2FormerForUniversalSegmentation(\n",
       "  (model): Mask2FormerModel(\n",
       "    (pixel_level_module): Mask2FormerPixelLevelModule(\n",
       "      (encoder): SwinBackbone(\n",
       "        (embeddings): SwinEmbeddings(\n",
       "          (patch_embeddings): SwinPatchEmbeddings(\n",
       "            (projection): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))\n",
       "          )\n",
       "          (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (encoder): SwinEncoder(\n",
       "          (layers): ModuleList(\n",
       "            (0): SwinStage(\n",
       "              (blocks): ModuleList(\n",
       "                (0-1): 2 x SwinLayer(\n",
       "                  (layernorm_before): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "                  (attention): SwinAttention(\n",
       "                    (self): SwinSelfAttention(\n",
       "                      (query): Linear(in_features=96, out_features=96, bias=True)\n",
       "                      (key): Linear(in_features=96, out_features=96, bias=True)\n",
       "                      (value): Linear(in_features=96, out_features=96, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (output): SwinSelfOutput(\n",
       "                      (dense): Linear(in_features=96, out_features=96, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                  (drop_path): SwinDropPath(p=0.1)\n",
       "                  (layernorm_after): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "                  (intermediate): SwinIntermediate(\n",
       "                    (dense): Linear(in_features=96, out_features=384, bias=True)\n",
       "                    (intermediate_act_fn): GELUActivation()\n",
       "                  )\n",
       "                  (output): SwinOutput(\n",
       "                    (dense): Linear(in_features=384, out_features=96, bias=True)\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (downsample): SwinPatchMerging(\n",
       "                (reduction): Linear(in_features=384, out_features=192, bias=False)\n",
       "                (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "            (1): SwinStage(\n",
       "              (blocks): ModuleList(\n",
       "                (0-1): 2 x SwinLayer(\n",
       "                  (layernorm_before): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "                  (attention): SwinAttention(\n",
       "                    (self): SwinSelfAttention(\n",
       "                      (query): Linear(in_features=192, out_features=192, bias=True)\n",
       "                      (key): Linear(in_features=192, out_features=192, bias=True)\n",
       "                      (value): Linear(in_features=192, out_features=192, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (output): SwinSelfOutput(\n",
       "                      (dense): Linear(in_features=192, out_features=192, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                  (drop_path): SwinDropPath(p=0.1)\n",
       "                  (layernorm_after): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "                  (intermediate): SwinIntermediate(\n",
       "                    (dense): Linear(in_features=192, out_features=768, bias=True)\n",
       "                    (intermediate_act_fn): GELUActivation()\n",
       "                  )\n",
       "                  (output): SwinOutput(\n",
       "                    (dense): Linear(in_features=768, out_features=192, bias=True)\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (downsample): SwinPatchMerging(\n",
       "                (reduction): Linear(in_features=768, out_features=384, bias=False)\n",
       "                (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "            (2): SwinStage(\n",
       "              (blocks): ModuleList(\n",
       "                (0-5): 6 x SwinLayer(\n",
       "                  (layernorm_before): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "                  (attention): SwinAttention(\n",
       "                    (self): SwinSelfAttention(\n",
       "                      (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "                      (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "                      (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (output): SwinSelfOutput(\n",
       "                      (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                  (drop_path): SwinDropPath(p=0.1)\n",
       "                  (layernorm_after): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "                  (intermediate): SwinIntermediate(\n",
       "                    (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                    (intermediate_act_fn): GELUActivation()\n",
       "                  )\n",
       "                  (output): SwinOutput(\n",
       "                    (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (downsample): SwinPatchMerging(\n",
       "                (reduction): Linear(in_features=1536, out_features=768, bias=False)\n",
       "                (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "            (3): SwinStage(\n",
       "              (blocks): ModuleList(\n",
       "                (0-1): 2 x SwinLayer(\n",
       "                  (layernorm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                  (attention): SwinAttention(\n",
       "                    (self): SwinSelfAttention(\n",
       "                      (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                      (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                      (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (output): SwinSelfOutput(\n",
       "                      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                  (drop_path): SwinDropPath(p=0.1)\n",
       "                  (layernorm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                  (intermediate): SwinIntermediate(\n",
       "                    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                    (intermediate_act_fn): GELUActivation()\n",
       "                  )\n",
       "                  (output): SwinOutput(\n",
       "                    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (hidden_states_norms): ModuleDict(\n",
       "          (stage1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "          (stage2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (stage3): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (stage4): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (decoder): Mask2FormerPixelDecoder(\n",
       "        (position_embedding): Mask2FormerSinePositionEmbedding()\n",
       "        (input_projections): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): Conv2d(192, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "          )\n",
       "        )\n",
       "        (encoder): Mask2FormerPixelDecoderEncoderOnly(\n",
       "          (layers): ModuleList(\n",
       "            (0-5): 6 x Mask2FormerPixelDecoderEncoderLayer(\n",
       "              (self_attn): Mask2FormerPixelDecoderEncoderMultiscaleDeformableAttention(\n",
       "                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)\n",
       "                (attention_weights): Linear(in_features=256, out_features=96, bias=True)\n",
       "                (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              )\n",
       "              (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "              (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "              (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "              (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (mask_projection): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (adapter_1): Sequential(\n",
       "          (0): Conv2d(96, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "        )\n",
       "        (layer_1): Sequential(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (transformer_module): Mask2FormerTransformerModule(\n",
       "      (position_embedder): Mask2FormerSinePositionEmbedding()\n",
       "      (queries_embedder): Embedding(100, 256)\n",
       "      (queries_features): Embedding(100, 256)\n",
       "      (decoder): Mask2FormerMaskedAttentionDecoder(\n",
       "        (layers): ModuleList(\n",
       "          (0-8): 9 x Mask2FormerMaskedAttentionDecoderLayer(\n",
       "            (self_attn): Mask2FormerAttention(\n",
       "              (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            )\n",
       "            (activation_fn): ReLU()\n",
       "            (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (cross_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "            )\n",
       "            (cross_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "            (fc2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "            (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (layernorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (mask_predictor): Mask2FormerMaskPredictor(\n",
       "          (mask_embedder): Mask2FormerMLPPredictionHead(\n",
       "            (0): Mask2FormerPredictionBlock(\n",
       "              (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (1): ReLU()\n",
       "            )\n",
       "            (1): Mask2FormerPredictionBlock(\n",
       "              (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (1): ReLU()\n",
       "            )\n",
       "            (2): Mask2FormerPredictionBlock(\n",
       "              (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (1): Identity()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (level_embed): Embedding(3, 256)\n",
       "    )\n",
       "  )\n",
       "  (class_predictor): Linear(in_features=256, out_features=13, bias=True)\n",
       "  (criterion): Mask2FormerLoss(\n",
       "    (matcher): Mask2FormerHungarianMatcher()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Construct the artifact path\n",
    "artifact_path = f\"{user_or_team}/{project_name}/best_model_{base_run_name}:latest\"\n",
    "# Load from W&B\n",
    "api = wandb.Api()\n",
    "artifact = api.artifact(artifact_path)\n",
    "model_dir = artifact.download()\n",
    "model_state_dict_path = os.path.join(model_dir, f\"best_model_{base_run_name}.pth\")\n",
    "model_state_dict = torch.load(model_state_dict_path, map_location=device)\n",
    "\n",
    "# Student\n",
    "student = Mask2FormerForUniversalSegmentation(mask2former_config)\n",
    "student.load_state_dict(model_state_dict)\n",
    "student.to(device)\n",
    "\n",
    "# Teacher\n",
    "teacher = Mask2FormerForUniversalSegmentation(mask2former_config)\n",
    "teacher.load_state_dict(model_state_dict)\n",
    "teacher.to(device)\n",
    "# Eval mode for teacher\n",
    "teacher.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-20T09:30:23.673510Z",
     "iopub.status.busy": "2024-06-20T09:30:23.672698Z",
     "iopub.status.idle": "2024-06-20T09:30:24.020380Z",
     "shell.execute_reply": "2024-06-20T09:30:24.019449Z",
     "shell.execute_reply.started": "2024-06-20T09:30:23.673510Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 100%|██████████| 12.9k/12.9k [00:00<00:00, 14.6MB/s]\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "NUM_EPOCHS = 200\n",
    "LEARNING_RATE = 1e-4\n",
    "LR_MULTIPLIER = 0.1\n",
    "BACKBONE_LR = LEARNING_RATE * LR_MULTIPLIER\n",
    "WEIGHT_DECAY = 0.05\n",
    "PATIENCE = 15\n",
    "metric = evaluate.load(\"mean_iou\")\n",
    "encoder_params = [\n",
    "    param\n",
    "    for name, param in student.named_parameters()\n",
    "    if name.startswith(\"model.pixel_level_module.encoder\")\n",
    "]\n",
    "decoder_params = [\n",
    "    param\n",
    "    for name, param in student.named_parameters()\n",
    "    if name.startswith(\"model.pixel_level_module.decoder\")\n",
    "]\n",
    "transformer_params = [\n",
    "    param\n",
    "    for name, param in student.named_parameters()\n",
    "    if name.startswith(\"model.transformer_module\")\n",
    "]\n",
    "class_prediction_params = [\n",
    "    param\n",
    "    for name, param in student.named_parameters()\n",
    "    if not name.startswith(\"model.pixel_level_module.encoder\")\n",
    "    and not name.startswith(\"model.transformer_module\")\n",
    "    and not name.startswith(\"model.pixel_level_module.decoder\")\n",
    "]\n",
    "optimizer = optim.AdamW(\n",
    "    [\n",
    "        {\"params\": encoder_params, \"lr\": BACKBONE_LR},\n",
    "        {\"params\": decoder_params},\n",
    "        {\"params\": transformer_params},\n",
    "        {\"params\": class_prediction_params},\n",
    "    ],\n",
    "    lr=LEARNING_RATE,\n",
    "    weight_decay=WEIGHT_DECAY,\n",
    ")\n",
    "\n",
    "scheduler = optim.lr_scheduler.PolynomialLR(\n",
    "    optimizer, total_iters=NUM_EPOCHS, power=0.9\n",
    ")\n",
    "# KD\n",
    "# LAMBDAS = {\"q\": 5e-4, \"c\": 1e-3, \"m\": 0.3, \"pod\": 0.1}\n",
    "LAMBDAS = {\"q\": 1e-4, \"c\": 5e-2, \"m\": 3e-2, \"pod\": 0.8}\n",
    "\n",
    "# Initialize EMA parameters\n",
    "EMA_DECAY = 0.999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-20T09:30:24.022329Z",
     "iopub.status.busy": "2024-06-20T09:30:24.021554Z",
     "iopub.status.idle": "2024-06-20T09:30:25.502937Z",
     "shell.execute_reply": "2024-06-20T09:30:25.501987Z",
     "shell.execute_reply.started": "2024-06-20T09:30:24.022329Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mkristiyan-sakalyan\u001b[0m (\u001b[33mcontinual-learning-tum\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "wandb version 0.17.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/notebooks/continual-learning/wandb/run-20240620_093024-yhehcp9z</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/continual-learning-tum/M2F_latest/runs/yhehcp9z' target=\"_blank\">M2F-Swin-Tiny-KD-EMA-Mean-Loss-Batch-Replay</a></strong> to <a href='https://wandb.ai/continual-learning-tum/M2F_latest' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/continual-learning-tum/M2F_latest' target=\"_blank\">https://wandb.ai/continual-learning-tum/M2F_latest</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/continual-learning-tum/M2F_latest/runs/yhehcp9z' target=\"_blank\">https://wandb.ai/continual-learning-tum/M2F_latest/runs/yhehcp9z</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wandb run id: yhehcp9z\n",
      "Store best model weights in:  outputs/models/M2F-Swin-Tiny-KD-EMA-Mean-Loss-Batch-Replay/best_model/\n",
      "Store final model weights in:  outputs/models/M2F-Swin-Tiny-KD-EMA-Mean-Loss-Batch-Replay/final_model/\n"
     ]
    }
   ],
   "source": [
    "wandb.init(\n",
    "    project=project_name,\n",
    "    config={\n",
    "        \"learning_rate\": LEARNING_RATE,\n",
    "        \"learning_rate_multiplier\": LR_MULTIPLIER,\n",
    "        \"backbone_learning_rate\": BACKBONE_LR,\n",
    "        \"learning_rate_scheduler\": scheduler.__class__.__name__,\n",
    "        \"optimizer\": optimizer.__class__.__name__,\n",
    "        \"backbone\": SWIN_BACKBONE,\n",
    "        \"m2f_preprocessor\": m2f_preprocessor_B.__dict__,\n",
    "        \"m2f_model_config\": student.config,\n",
    "        \"lambdas\": LAMBDAS,\n",
    "        \"ema_decay\": EMA_DECAY,\n",
    "    },\n",
    "    name=new_run_name,\n",
    "    notes=\"M2F with tiny Swin backbone pretrained on ImageNet-1K. \\\n",
    "        Scenario: Pretrained on A, Train on B, Test forgetting on A\",\n",
    ")\n",
    "\n",
    "print(\"wandb run id:\", wandb.run.id)\n",
    "\n",
    "# Tensorboard logging\n",
    "writer = SummaryWriter(log_dir=out_dir + \"runs\")\n",
    "# Model checkpointing\n",
    "model_dir = out_dir + \"models/\"\n",
    "if not os.path.exists(model_dir):\n",
    "    print(\"Store weights in: \", model_dir)\n",
    "    os.makedirs(model_dir)\n",
    "\n",
    "best_model_dir = model_dir + f\"{new_run_name}/best_model/\"\n",
    "if not os.path.exists(best_model_dir):\n",
    "    print(\"Store best model weights in: \", best_model_dir)\n",
    "    os.makedirs(best_model_dir)\n",
    "final_model_dir = model_dir + f\"{new_run_name}/final_model/\"\n",
    "if not os.path.exists(final_model_dir):\n",
    "    print(\"Store final model weights in: \", final_model_dir)\n",
    "    os.makedirs(final_model_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Results on A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-20T09:30:25.504628Z",
     "iopub.status.busy": "2024-06-20T09:30:25.504354Z",
     "iopub.status.idle": "2024-06-20T09:31:41.574606Z",
     "shell.execute_reply": "2024-06-20T09:31:41.573695Z",
     "shell.execute_reply.started": "2024-06-20T09:30:25.504605Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test loop: 100%|██████████| 37/37 [01:07<00:00,  1.84s/it, loss=79.0629] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 16.4622, Test mIoU: 0.7464\n"
     ]
    }
   ],
   "source": [
    "student.eval()\n",
    "test_running_loss = 0\n",
    "CURR_TASK=\"A\"\n",
    "test_loader = tqdm(dataloaders[CURR_TASK][\"test\"], desc=\"Test loop\")\n",
    "\n",
    "BATCH_INDEX = 0\n",
    "table = wandb.Table(columns=[\"ID\", \"Image\"])\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        # Move everything to the device\n",
    "        batch[\"pixel_values\"] = batch[\"pixel_values\"].to(device)\n",
    "        batch[\"pixel_mask\"] = batch[\"pixel_mask\"].to(device)\n",
    "        batch[\"mask_labels\"] = [entry.to(device) for entry in batch[\"mask_labels\"]]\n",
    "        batch[\"class_labels\"] = [entry.to(device) for entry in batch[\"class_labels\"]]\n",
    "        # Compute output and loss\n",
    "        student_outputs = student(**batch)\n",
    "\n",
    "        loss = student_outputs.loss\n",
    "        # Record losses\n",
    "        current_loss = loss.item() * batch[\"pixel_values\"].size(0)\n",
    "        test_running_loss += current_loss\n",
    "        test_loader.set_postfix(loss=f\"{current_loss:.4f}\")\n",
    "\n",
    "        # Extract and compute metrics\n",
    "        pred_maps, masks = m2f_extract_pred_maps_and_masks(\n",
    "            batch, student_outputs, m2f_preprocessor_A\n",
    "        )\n",
    "        metric.add_batch(references=masks, predictions=pred_maps)\n",
    "        if BATCH_INDEX <5:\n",
    "            # Visualize\n",
    "            log_table_of_images(\n",
    "                table, # common table for all batches\n",
    "                batch[\"pixel_values\"],\n",
    "                pixel_mean_A, # remove normalization\n",
    "                pixel_std_A, # remove normalization\n",
    "                pred_maps,\n",
    "                masks,\n",
    "                BATCH_INDEX, # correct indexing in table\n",
    "            )\n",
    "            BATCH_INDEX += 1\n",
    "# Log table\n",
    "wandb.log({f\"{CURR_TASK}_TEST_AFTER_TRAINING_A\": table})\n",
    "\n",
    "# After compute the batches that were added are deleted\n",
    "test_metrics_A = metric.compute(\n",
    "    num_labels=NUM_CLASSES, ignore_index=BG_VALUE_255, reduce_labels=False\n",
    ")\n",
    "mean_test_iou = test_metrics_A[\"mean_iou\"]\n",
    "final_test_loss = test_running_loss / len(dataloaders[CURR_TASK][\"test\"].dataset)\n",
    "wandb.log({\n",
    "    f\"Loss/test_{CURR_TASK}\": final_test_loss,\n",
    "    f\"mIoU/test_{CURR_TASK}\": mean_test_iou\n",
    "})\n",
    "print(f\"Test Loss: {final_test_loss:.4f}, Test mIoU: {mean_test_iou:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test results on B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-20T09:31:41.577492Z",
     "iopub.status.busy": "2024-06-20T09:31:41.577163Z",
     "iopub.status.idle": "2024-06-20T09:32:11.747334Z",
     "shell.execute_reply": "2024-06-20T09:32:11.746226Z",
     "shell.execute_reply.started": "2024-06-20T09:31:41.577478Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test loop: 100%|██████████| 15/15 [00:26<00:00,  1.77s/it, loss=125.9442]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 40.9604, Test mIoU: 0.2717\n"
     ]
    }
   ],
   "source": [
    "student.eval()\n",
    "test_running_loss = 0\n",
    "CURR_TASK=\"B\"\n",
    "test_loader = tqdm(dataloaders[CURR_TASK][\"test\"], desc=\"Test loop\")\n",
    "\n",
    "BATCH_INDEX = 0\n",
    "table = wandb.Table(columns=[\"ID\", \"Image\"])\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        # Move everything to the device\n",
    "        batch[\"pixel_values\"] = batch[\"pixel_values\"].to(device)\n",
    "        batch[\"pixel_mask\"] = batch[\"pixel_mask\"].to(device)\n",
    "        batch[\"mask_labels\"] = [entry.to(device) for entry in batch[\"mask_labels\"]]\n",
    "        batch[\"class_labels\"] = [entry.to(device) for entry in batch[\"class_labels\"]]\n",
    "        # Compute output and loss\n",
    "        student_outputs = student(**batch)\n",
    "\n",
    "        loss = student_outputs.loss\n",
    "        # Record losses\n",
    "        current_loss = loss.item() * batch[\"pixel_values\"].size(0)\n",
    "        test_running_loss += current_loss\n",
    "        test_loader.set_postfix(loss=f\"{current_loss:.4f}\")\n",
    "\n",
    "        # Extract and compute metrics\n",
    "        pred_maps, masks = m2f_extract_pred_maps_and_masks(\n",
    "            batch, student_outputs, m2f_preprocessor_B\n",
    "        )\n",
    "        metric.add_batch(references=masks, predictions=pred_maps)\n",
    "        if BATCH_INDEX <5:\n",
    "            # Visualize\n",
    "            log_table_of_images(\n",
    "                table, # common table for all batches\n",
    "                batch[\"pixel_values\"],\n",
    "                pixel_mean_B, # remove normalization\n",
    "                pixel_std_B, # remove normalization\n",
    "                pred_maps,\n",
    "                masks,\n",
    "                BATCH_INDEX, # correct indexing in table\n",
    "            )\n",
    "            BATCH_INDEX += 1\n",
    "\n",
    "# Log table\n",
    "wandb.log({f\"{CURR_TASK}_TEST_AFTER_TRAINING_A\": table})\n",
    "\n",
    "# After compute the batches that were added are deleted\n",
    "test_metrics_B_before = metric.compute(\n",
    "    num_labels=NUM_CLASSES, ignore_index=BG_VALUE_255, reduce_labels=False\n",
    ")\n",
    "mean_test_iou = test_metrics_B_before[\"mean_iou\"]\n",
    "final_test_loss = test_running_loss / len(dataloaders[CURR_TASK][\"test\"].dataset)\n",
    "wandb.log({\n",
    "    f\"Loss/test_{CURR_TASK}\": final_test_loss,\n",
    "    f\"mIoU/test_{CURR_TASK}\": mean_test_iou\n",
    "})\n",
    "print(f\"Test Loss: {final_test_loss:.4f}, Test mIoU: {mean_test_iou:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Version 3: Train on B with knowledge distillation, EMA and Mean Loss sampled Batch Level Replay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-20T09:32:11.749499Z",
     "iopub.status.busy": "2024-06-20T09:32:11.749182Z",
     "iopub.status.idle": "2024-06-20T09:32:13.663624Z",
     "shell.execute_reply": "2024-06-20T09:32:13.662584Z",
     "shell.execute_reply.started": "2024-06-20T09:32:11.749478Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cumulative size: 32659200\n"
     ]
    }
   ],
   "source": [
    "# Calculate the byte size of one sample (image + mask)\n",
    "def calculate_sample_size(image, mask):\n",
    "    image_size = image.numel() * image.element_size()  # Number of elements * bytes per element (for RGB)\n",
    "    \n",
    "    # if mask sizes also need to be taken into account, uncomment the below 2 lines!!\n",
    "    #mask_size = mask.numel() * mask.element_size()  \n",
    "    #return image_size + mask_size\n",
    "    \n",
    "    return image_size\n",
    "\n",
    "\n",
    "# Function to sample without replacement until target size is reached\n",
    "def sample_until_target_size(dataset, target_size_bytes):\n",
    "    sampled_indices = []\n",
    "    cumulative_size = 0\n",
    "\n",
    "    indices = list(range(len(dataset)))\n",
    "    random.shuffle(indices)\n",
    "\n",
    "    for idx in indices:\n",
    "        image, mask = dataset[idx]\n",
    "        sample_size = calculate_sample_size(image, mask)\n",
    "        if cumulative_size + sample_size <= target_size_bytes:\n",
    "            sampled_indices.append(idx)\n",
    "            cumulative_size += sample_size\n",
    "        else:\n",
    "            break\n",
    "    print(\"cumulative size:\",cumulative_size)\n",
    "    return sampled_indices\n",
    "\n",
    "\n",
    "# Target size in bytes (32MB)\n",
    "target_size_bytes = 32 * 1024 * 1024\n",
    "\n",
    "# Get the sampled indices\n",
    "sampled_indices = sample_until_target_size(datasets[\"A\"][0], target_size_bytes)\n",
    "N=len(sampled_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-20T09:32:13.664999Z",
     "iopub.status.busy": "2024-06-20T09:32:13.664814Z",
     "iopub.status.idle": "2024-06-20T09:50:38.307426Z",
     "shell.execute_reply": "2024-06-20T09:50:38.306449Z",
     "shell.execute_reply.started": "2024-06-20T09:32:13.664996Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3550/3550 [18:23<00:00,  3.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'A': {'train': <torch.utils.data.dataloader.DataLoader object at 0x7feb883e3140>, 'val': <torch.utils.data.dataloader.DataLoader object at 0x7feb883e37d0>, 'test': <torch.utils.data.dataloader.DataLoader object at 0x7feb883e3320>}, 'B': {'train': <torch.utils.data.dataloader.DataLoader object at 0x7feb883e3620>, 'val': <torch.utils.data.dataloader.DataLoader object at 0x7feb883e1b20>, 'test': <torch.utils.data.dataloader.DataLoader object at 0x7feb883e1e20>}}\n"
     ]
    }
   ],
   "source": [
    "losses = []\n",
    "\n",
    "# Collect losses\n",
    "student.eval()\n",
    "with torch.no_grad():\n",
    "    for sample in tqdm(m2f_datasets[\"A\"][\"train\"]):\n",
    "        sample[\"pixel_values\"] = sample[\"pixel_values\"].to(device)\n",
    "        sample[\"pixel_mask\"] = sample[\"pixel_mask\"].to(device)\n",
    "        sample[\"mask_labels\"] = [entry.to(device) for entry in sample[\"mask_labels\"]]\n",
    "        sample[\"class_labels\"] = [entry.to(device) for entry in sample[\"class_labels\"]]\n",
    "        losses.append(student(**sample).loss.item())\n",
    "\n",
    "losses_np = np.array(losses)\n",
    "\n",
    "# Sample images with mean loss\n",
    "mean_loss = np.mean(losses_np)\n",
    "differences = np.abs(losses_np - mean_loss)\n",
    "closest_indices = np.argsort(differences)[:N] # N was calculated above\n",
    "\n",
    "# Create a subset of B using the mean loss sampled indices\n",
    "subset_A = [m2f_datasets[\"A\"][\"train\"][i] for i in closest_indices]\n",
    "NUM_REPLAY_SAMPLES_PER_BATCH = 8\n",
    "\n",
    "# Calculate new mean and std\n",
    "pixel_mean_B=np.array(CAT1K_PIXEL_MEAN)\n",
    "pixel_std_B=np.array(CAT1K_PIXEL_STD)\n",
    "\n",
    "m2f_preprocessor_B = Mask2FormerImageProcessor(\n",
    "    reduce_labels=False,\n",
    "    ignore_index=255,\n",
    "    do_resize=False,\n",
    "    do_rescale=False,\n",
    "    do_normalize=True,\n",
    "    image_std=pixel_std_B,\n",
    "    image_mean=pixel_mean_B,\n",
    ")\n",
    "\n",
    "# Create Mask2Former Datasets\n",
    "\n",
    "m2f_datasets = {\n",
    "    \"A\": {\n",
    "        \"train\": Mask2FormerDataset(datasets[\"A\"][0], m2f_preprocessor_A),\n",
    "        \"val\": Mask2FormerDataset(datasets[\"A\"][1], m2f_preprocessor_A),\n",
    "        \"test\": Mask2FormerDataset(datasets[\"A\"][2], m2f_preprocessor_A),\n",
    "    },\n",
    "    \"B\": {\n",
    "        \"train\": Mask2FormerDataset(datasets[\"B\"][0], m2f_preprocessor_B),\n",
    "        \"val\": Mask2FormerDataset(datasets[\"B\"][1], m2f_preprocessor_B),\n",
    "        \"test\": Mask2FormerDataset(datasets[\"B\"][2], m2f_preprocessor_B),\n",
    "    },\n",
    "}\n",
    "\n",
    "# DataLoader parameters\n",
    "N_WORKERS = 4\n",
    "BATCH_SIZE = 8\n",
    "SHUFFLE = True\n",
    "DROP_LAST = True\n",
    "\n",
    "dataloader_params = {\n",
    "    \"batch_size\": BATCH_SIZE,\n",
    "    \"shuffle\": SHUFFLE,\n",
    "    \"num_workers\": N_WORKERS,\n",
    "    \"drop_last\": DROP_LAST,\n",
    "    \"pin_memory\": True,\n",
    "    \"collate_fn\": m2f_dataset_collate,\n",
    "}\n",
    "\n",
    "def create_dataloaders_bl_replay(\n",
    "    dataset, batch_size, shuffle, num_workers, drop_last, pin_memory, collate_fn\n",
    "):\n",
    "    return {\n",
    "        \"train\": DataLoader(\n",
    "            dataset[\"train\"],\n",
    "            batch_size=batch_size,\n",
    "            shuffle=shuffle,\n",
    "            num_workers=num_workers,\n",
    "            drop_last=drop_last,\n",
    "            pin_memory=pin_memory,\n",
    "            collate_fn=collate_fn,2\n",
    "        ),\n",
    "        \"val\": DataLoader(\n",
    "            dataset[\"val\"],\n",
    "            batch_size=batch_size * 2, # 16\n",
    "            shuffle=shuffle,\n",
    "            num_workers=num_workers,\n",
    "            drop_last=drop_last,\n",
    "            pin_memory=pin_memory,\n",
    "            collate_fn=collate_fn,\n",
    "        ),\n",
    "        \"test\": DataLoader(\n",
    "            dataset[\"test\"],\n",
    "            batch_size=batch_size * 2, # 16\n",
    "            shuffle=False,\n",
    "            num_workers=num_workers,\n",
    "            drop_last=False,\n",
    "            pin_memory=pin_memory,\n",
    "            collate_fn=collate_fn,\n",
    "        ),\n",
    "    }\n",
    "\n",
    "# Create DataLoaders\n",
    "dataloaders = {\n",
    "    key: create_dataloaders_bl_replay(m2f_datasets[key], **dataloader_params)\n",
    "    for key in m2f_datasets\n",
    "}\n",
    "\n",
    "print(dataloaders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-20T09:50:38.308712Z",
     "iopub.status.busy": "2024-06-20T09:50:38.308466Z",
     "iopub.status.idle": "2024-06-20T09:50:38.378670Z",
     "shell.execute_reply": "2024-06-20T09:50:38.377542Z",
     "shell.execute_reply.started": "2024-06-20T09:50:38.308712Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['outputs/models/M2F-Swin-Tiny-KD-EMA-Mean-Loss-Batch-Replay/preprocessor_config.json']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Save the preprocessor\n",
    "m2f_preprocessor_B.save_pretrained(model_dir + new_run_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-20T09:50:38.382142Z",
     "iopub.status.busy": "2024-06-20T09:50:38.381740Z",
     "iopub.status.idle": "2024-06-20T17:55:42.152705Z",
     "shell.execute_reply": "2024-06-20T17:55:42.151649Z",
     "shell.execute_reply.started": "2024-06-20T09:50:38.382142Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/200 Training: 100%|██████████| 225/225 [13:27<00:00,  3.59s/it, loss=133.2546]\n",
      "Epoch 1/200 Validation: 100%|██████████| 14/14 [00:22<00:00,  1.59s/it, loss=116.9182]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200, Train Loss: 8.8905, Train mIoU: 0.7050, Validation Loss: 8.4535, Validation mIoU: 0.6079\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/200 Training: 100%|██████████| 225/225 [13:31<00:00,  3.61s/it, loss=90.7405] \n",
      "Epoch 2/200 Validation: 100%|██████████| 14/14 [00:20<00:00,  1.44s/it, loss=96.6220] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/200, Train Loss: 6.3140, Train mIoU: 0.8142, Validation Loss: 7.7162, Validation mIoU: 0.7157\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/200 Training: 100%|██████████| 225/225 [13:30<00:00,  3.60s/it, loss=81.1362] \n",
      "Epoch 3/200 Validation: 100%|██████████| 14/14 [00:20<00:00,  1.45s/it, loss=115.2563]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/200, Train Loss: 5.5779, Train mIoU: 0.8350, Validation Loss: 7.2797, Validation mIoU: 0.6584\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/200 Training: 100%|██████████| 225/225 [13:27<00:00,  3.59s/it, loss=68.4185] \n",
      "Epoch 4/200 Validation: 100%|██████████| 14/14 [00:19<00:00,  1.37s/it, loss=133.6031]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/200, Train Loss: 5.2685, Train mIoU: 0.8598, Validation Loss: 7.2879, Validation mIoU: 0.7214\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/200 Training: 100%|██████████| 225/225 [13:31<00:00,  3.61s/it, loss=75.5991]\n",
      "Epoch 5/200 Validation: 100%|██████████| 14/14 [00:19<00:00,  1.43s/it, loss=109.7639]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/200, Train Loss: 4.9393, Train mIoU: 0.8751, Validation Loss: 6.9486, Validation mIoU: 0.6868\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/200 Training: 100%|██████████| 225/225 [13:26<00:00,  3.58s/it, loss=76.3087]\n",
      "Epoch 6/200 Validation: 100%|██████████| 14/14 [00:20<00:00,  1.45s/it, loss=95.4402] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/200, Train Loss: 4.7525, Train mIoU: 0.8883, Validation Loss: 6.9793, Validation mIoU: 0.7302\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/200 Training: 100%|██████████| 225/225 [13:31<00:00,  3.61s/it, loss=75.8322]\n",
      "Epoch 7/200 Validation: 100%|██████████| 14/14 [00:20<00:00,  1.45s/it, loss=113.5607]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/200, Train Loss: 4.5391, Train mIoU: 0.8977, Validation Loss: 6.9993, Validation mIoU: 0.6686\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/200 Training: 100%|██████████| 225/225 [13:37<00:00,  3.63s/it, loss=66.3060]\n",
      "Epoch 8/200 Validation: 100%|██████████| 14/14 [00:20<00:00,  1.49s/it, loss=112.8808]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/200, Train Loss: 4.4455, Train mIoU: 0.9067, Validation Loss: 6.8585, Validation mIoU: 0.7298\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/200 Training: 100%|██████████| 225/225 [13:33<00:00,  3.62s/it, loss=64.8831]\n",
      "Epoch 9/200 Validation: 100%|██████████| 14/14 [00:20<00:00,  1.49s/it, loss=119.0426]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/200, Train Loss: 4.2527, Train mIoU: 0.9038, Validation Loss: 6.5016, Validation mIoU: 0.7688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/200 Training: 100%|██████████| 225/225 [13:35<00:00,  3.63s/it, loss=90.8659] \n",
      "Epoch 10/200 Validation: 100%|██████████| 14/14 [00:20<00:00,  1.45s/it, loss=119.6588]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/200, Train Loss: 4.6809, Train mIoU: 0.8932, Validation Loss: 7.5551, Validation mIoU: 0.6618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 11/200 Training: 100%|██████████| 225/225 [13:30<00:00,  3.60s/it, loss=58.4679]\n",
      "Epoch 11/200 Validation: 100%|██████████| 14/14 [00:21<00:00,  1.57s/it, loss=88.3468] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 11/200, Train Loss: 4.5561, Train mIoU: 0.9027, Validation Loss: 6.7510, Validation mIoU: 0.7495\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 12/200 Training: 100%|██████████| 225/225 [13:43<00:00,  3.66s/it, loss=62.8905]\n",
      "Epoch 12/200 Validation: 100%|██████████| 14/14 [00:20<00:00,  1.45s/it, loss=110.4379]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 12/200, Train Loss: 4.0575, Train mIoU: 0.9233, Validation Loss: 6.6635, Validation mIoU: 0.7666\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 13/200 Training: 100%|██████████| 225/225 [13:32<00:00,  3.61s/it, loss=65.5465]\n",
      "Epoch 13/200 Validation: 100%|██████████| 14/14 [00:20<00:00,  1.47s/it, loss=91.8744] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 13/200, Train Loss: 3.8545, Train mIoU: 0.9246, Validation Loss: 6.9113, Validation mIoU: 0.7753\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 14/200 Training: 100%|██████████| 225/225 [13:38<00:00,  3.64s/it, loss=58.0131]\n",
      "Epoch 14/200 Validation: 100%|██████████| 14/14 [00:19<00:00,  1.40s/it, loss=87.5055] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 14/200, Train Loss: 3.7539, Train mIoU: 0.9326, Validation Loss: 6.6767, Validation mIoU: 0.7724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 15/200 Training: 100%|██████████| 225/225 [13:33<00:00,  3.62s/it, loss=57.0098]\n",
      "Epoch 15/200 Validation: 100%|██████████| 14/14 [00:21<00:00,  1.56s/it, loss=95.3182] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15/200, Train Loss: 3.7112, Train mIoU: 0.9263, Validation Loss: 6.8140, Validation mIoU: 0.7775\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 16/200 Training: 100%|██████████| 225/225 [13:31<00:00,  3.61s/it, loss=52.6774]\n",
      "Epoch 16/200 Validation: 100%|██████████| 14/14 [00:21<00:00,  1.52s/it, loss=100.0147]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 16/200, Train Loss: 3.6660, Train mIoU: 0.9356, Validation Loss: 6.8004, Validation mIoU: 0.7603\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 17/200 Training: 100%|██████████| 225/225 [13:39<00:00,  3.64s/it, loss=60.0174]\n",
      "Epoch 17/200 Validation: 100%|██████████| 14/14 [00:19<00:00,  1.39s/it, loss=116.5215]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 17/200, Train Loss: 3.6351, Train mIoU: 0.9332, Validation Loss: 6.8513, Validation mIoU: 0.7796\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 18/200 Training: 100%|██████████| 225/225 [13:39<00:00,  3.64s/it, loss=61.0190]\n",
      "Epoch 18/200 Validation: 100%|██████████| 14/14 [00:18<00:00,  1.34s/it, loss=146.4614]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 18/200, Train Loss: 3.5679, Train mIoU: 0.9393, Validation Loss: 6.8968, Validation mIoU: 0.7832\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 19/200 Training: 100%|██████████| 225/225 [13:29<00:00,  3.60s/it, loss=81.7097] \n",
      "Epoch 19/200 Validation: 100%|██████████| 14/14 [00:20<00:00,  1.49s/it, loss=119.0084]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19/200, Train Loss: 3.6205, Train mIoU: 0.9321, Validation Loss: 7.6265, Validation mIoU: 0.6321\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/200 Training: 100%|██████████| 225/225 [13:40<00:00,  3.65s/it, loss=62.2051]\n",
      "Epoch 20/200 Validation: 100%|██████████| 14/14 [00:21<00:00,  1.50s/it, loss=107.8263]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20/200, Train Loss: 5.4390, Train mIoU: 0.8155, Validation Loss: 6.9940, Validation mIoU: 0.7263\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 21/200 Training: 100%|██████████| 225/225 [13:40<00:00,  3.65s/it, loss=54.8702]\n",
      "Epoch 21/200 Validation: 100%|██████████| 14/14 [00:21<00:00,  1.51s/it, loss=163.3374]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 21/200, Train Loss: 3.7619, Train mIoU: 0.9347, Validation Loss: 7.0329, Validation mIoU: 0.7453\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 22/200 Training: 100%|██████████| 225/225 [13:33<00:00,  3.61s/it, loss=56.8037]\n",
      "Epoch 22/200 Validation: 100%|██████████| 14/14 [00:20<00:00,  1.48s/it, loss=117.2545]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 22/200, Train Loss: 3.5347, Train mIoU: 0.9420, Validation Loss: 7.0456, Validation mIoU: 0.7551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 23/200 Training: 100%|██████████| 225/225 [13:32<00:00,  3.61s/it, loss=51.3676]\n",
      "Epoch 23/200 Validation: 100%|██████████| 14/14 [00:21<00:00,  1.52s/it, loss=108.4912]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 23/200, Train Loss: 3.4825, Train mIoU: 0.9445, Validation Loss: 7.0952, Validation mIoU: 0.7528\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 24/200 Training: 100%|██████████| 225/225 [13:35<00:00,  3.62s/it, loss=63.1316]\n",
      "Epoch 24/200 Validation: 100%|██████████| 14/14 [00:20<00:00,  1.49s/it, loss=123.4949]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 24/200, Train Loss: 3.4532, Train mIoU: 0.9451, Validation Loss: 7.2189, Validation mIoU: 0.7591\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 25/200 Training: 100%|██████████| 225/225 [13:26<00:00,  3.58s/it, loss=63.9117]\n",
      "Epoch 25/200 Validation: 100%|██████████| 14/14 [00:18<00:00,  1.34s/it, loss=103.6207]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25/200, Train Loss: 3.5617, Train mIoU: 0.9455, Validation Loss: 7.2879, Validation mIoU: 0.7515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 26/200 Training: 100%|██████████| 225/225 [13:39<00:00,  3.64s/it, loss=53.8492]\n",
      "Epoch 26/200 Validation: 100%|██████████| 14/14 [00:21<00:00,  1.50s/it, loss=127.6100]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 26/200, Train Loss: 3.6393, Train mIoU: 0.9430, Validation Loss: 7.2138, Validation mIoU: 0.7682\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 27/200 Training: 100%|██████████| 225/225 [13:32<00:00,  3.61s/it, loss=56.4501]\n",
      "Epoch 27/200 Validation: 100%|██████████| 14/14 [00:21<00:00,  1.53s/it, loss=176.9449]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 27/200, Train Loss: 3.4895, Train mIoU: 0.9470, Validation Loss: 7.1215, Validation mIoU: 0.7645\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 28/200 Training: 100%|██████████| 225/225 [13:32<00:00,  3.61s/it, loss=55.2431]\n",
      "Epoch 28/200 Validation: 100%|██████████| 14/14 [00:20<00:00,  1.44s/it, loss=141.9866]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 28/200, Train Loss: 3.3556, Train mIoU: 0.9390, Validation Loss: 7.1294, Validation mIoU: 0.7583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 29/200 Training: 100%|██████████| 225/225 [13:25<00:00,  3.58s/it, loss=47.9491]\n",
      "Epoch 29/200 Validation: 100%|██████████| 14/14 [00:20<00:00,  1.47s/it, loss=107.2960]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 29/200, Train Loss: 3.2746, Train mIoU: 0.9426, Validation Loss: 7.3787, Validation mIoU: 0.7763\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 30/200 Training: 100%|██████████| 225/225 [13:30<00:00,  3.60s/it, loss=49.7532]\n",
      "Epoch 30/200 Validation: 100%|██████████| 14/14 [00:21<00:00,  1.54s/it, loss=103.6086]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30/200, Train Loss: 3.1980, Train mIoU: 0.9488, Validation Loss: 7.2217, Validation mIoU: 0.7389\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 31/200 Training: 100%|██████████| 225/225 [13:31<00:00,  3.61s/it, loss=52.2211]\n",
      "Epoch 31/200 Validation: 100%|██████████| 14/14 [00:20<00:00,  1.48s/it, loss=94.0195] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 31/200, Train Loss: 3.1381, Train mIoU: 0.9482, Validation Loss: 7.4282, Validation mIoU: 0.7652\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 32/200 Training: 100%|██████████| 225/225 [13:30<00:00,  3.60s/it, loss=50.8301]\n",
      "Epoch 32/200 Validation: 100%|██████████| 14/14 [00:19<00:00,  1.41s/it, loss=102.1194]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 32/200, Train Loss: 3.1117, Train mIoU: 0.9532, Validation Loss: 7.2509, Validation mIoU: 0.7633\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 33/200 Training: 100%|██████████| 225/225 [13:21<00:00,  3.56s/it, loss=49.2956]\n",
      "Epoch 33/200 Validation: 100%|██████████| 14/14 [00:21<00:00,  1.51s/it, loss=119.2853]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 33/200, Train Loss: 3.0886, Train mIoU: 0.9538, Validation Loss: 7.4804, Validation mIoU: 0.7664\n",
      "Early stopping at epoch 32\n"
     ]
    }
   ],
   "source": [
    "# To avoid making stupid errors\n",
    "CURR_TASK = \"B\"\n",
    "\n",
    "# For storing the model\n",
    "best_val_metric = -np.inf\n",
    "best_model_weights = None  # best model weights are stored here\n",
    "\n",
    "# Move model to device\n",
    "student.to(device)\n",
    "teacher.to(device)\n",
    "\n",
    "# Teacher should be in eval mode\n",
    "teacher.eval()\n",
    "\n",
    "# Define a function to update the teacher's parameters.\n",
    "def update_teacher_ema(student_model, teacher_model, decay):\n",
    "    student_params = dict(student_model.named_parameters())\n",
    "    teacher_params = dict(teacher_model.named_parameters())\n",
    "\n",
    "    for name in teacher_params.keys():\n",
    "        teacher_params[name].data = (\n",
    "            decay * teacher_params[name].data + (1 - decay) * student_params[name].data\n",
    "        )\n",
    "\n",
    "\n",
    "counter = 0\n",
    "for epoch in range(NUM_EPOCHS):\n",
    "    student.train()\n",
    "    train_running_loss = 0.0\n",
    "    val_running_loss = 0.0\n",
    "\n",
    "    # Set up tqdm for the training loop\n",
    "    train_loader = tqdm(\n",
    "        dataloaders[CURR_TASK][\"train\"], desc=f\"Epoch {epoch + 1}/{NUM_EPOCHS} Training\"\n",
    "    )\n",
    "\n",
    "    for batch in train_loader:\n",
    "        # Replay batch level\n",
    "        sampled_replay_indices = random.sample(range(len(subset_A)), NUM_REPLAY_SAMPLES_PER_BATCH)\n",
    "        replay_samples = [subset_A[i] for i in sampled_replay_indices]\n",
    "        \n",
    "        new_batch = {}\n",
    "        batch_replay = m2f_dataset_collate(replay_samples)\n",
    "        new_batch[\"pixel_values\"] = torch.cat([batch[\"pixel_values\"], batch_replay[\"pixel_values\"]])\n",
    "        new_batch[\"pixel_mask\"] = torch.cat([batch[\"pixel_mask\"], batch_replay[\"pixel_mask\"]])\n",
    "        new_batch[\"mask_labels\"] = batch[\"mask_labels\"] + batch_replay[\"mask_labels\"]\n",
    "        new_batch[\"class_labels\"] = batch[\"class_labels\"] + batch_replay[\"class_labels\"]\n",
    "        \n",
    "        # Move everything to the device\n",
    "        new_batch[\"pixel_values\"] = new_batch[\"pixel_values\"].to(device)\n",
    "        new_batch[\"pixel_mask\"] = new_batch[\"pixel_mask\"].to(device)\n",
    "        new_batch[\"mask_labels\"] = [entry.to(device) for entry in new_batch[\"mask_labels\"]]\n",
    "        new_batch[\"class_labels\"] = [entry.to(device) for entry in new_batch[\"class_labels\"]]\n",
    "        \n",
    "        # Compute output and loss\n",
    "        student_outputs = student(**new_batch, output_hidden_states=True)\n",
    "\n",
    "        # Compute output for teacher model\n",
    "        with torch.no_grad():\n",
    "            teacher_outputs = teacher(**new_batch, output_hidden_states=True)\n",
    "\n",
    "        # Compute KD Loss\n",
    "        kd_loss = compute_kd_loss(student_outputs, teacher_outputs, lambdas=LAMBDAS)\n",
    "        loss = student_outputs.loss + kd_loss\n",
    "\n",
    "        # Compute gradient and perform step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Record losses\n",
    "        current_loss = loss.item() * (batch[\"pixel_values\"].size(0) + NUM_REPLAY_SAMPLES_PER_BATCH)\n",
    "        train_running_loss += current_loss\n",
    "        train_loader.set_postfix(loss=f\"{current_loss:.4f}\")\n",
    "\n",
    "        # Extract and compute metrics\n",
    "        pred_maps, masks = m2f_extract_pred_maps_and_masks(\n",
    "            new_batch, student_outputs, m2f_preprocessor_B\n",
    "        )\n",
    "        metric.add_batch(references=masks, predictions=pred_maps)\n",
    "\n",
    "        # Update teacher model using EMA\n",
    "        update_teacher_ema(student, teacher, EMA_DECAY)\n",
    "\n",
    "    # After compute the batches that were added are deleted\n",
    "    mean_train_iou = metric.compute(\n",
    "        num_labels=NUM_CLASSES, ignore_index=BG_VALUE_255, reduce_labels=False\n",
    "    )[\"mean_iou\"]\n",
    "\n",
    "    # Validation phase\n",
    "    student.eval()\n",
    "    val_loader = tqdm(\n",
    "        dataloaders[CURR_TASK][\"val\"], desc=f\"Epoch {epoch + 1}/{NUM_EPOCHS} Validation\"\n",
    "    )\n",
    "    with torch.no_grad():\n",
    "        for batch in val_loader:\n",
    "            # Move everything to the device\n",
    "            batch[\"pixel_values\"] = batch[\"pixel_values\"].to(device)\n",
    "            batch[\"pixel_mask\"] = batch[\"pixel_mask\"].to(device)\n",
    "            batch[\"mask_labels\"] = [entry.to(device) for entry in batch[\"mask_labels\"]]\n",
    "            batch[\"class_labels\"] = [\n",
    "                entry.to(device) for entry in batch[\"class_labels\"]\n",
    "            ]\n",
    "            # Compute output and loss\n",
    "            student_outputs = student(**batch)\n",
    "\n",
    "            loss = student_outputs.loss\n",
    "            # Record losses\n",
    "            current_loss = loss.item() * batch[\"pixel_values\"].size(0)\n",
    "            val_running_loss += current_loss\n",
    "            val_loader.set_postfix(loss=f\"{current_loss:.4f}\")\n",
    "\n",
    "            # Extract and compute metrics\n",
    "            pred_maps, masks = m2f_extract_pred_maps_and_masks(\n",
    "                batch, student_outputs, m2f_preprocessor_B\n",
    "            )\n",
    "            metric.add_batch(references=masks, predictions=pred_maps)\n",
    "\n",
    "    # After compute the batches that were added are deleted\n",
    "    mean_val_iou = metric.compute(\n",
    "        num_labels=NUM_CLASSES, ignore_index=BG_VALUE_255, reduce_labels=False\n",
    "    )[\"mean_iou\"]\n",
    "\n",
    "    epoch_train_loss = train_running_loss / (\n",
    "        len(dataloaders[CURR_TASK][\"train\"].dataset)\n",
    "        + (NUM_REPLAY_SAMPLES_PER_BATCH * len(dataloaders[CURR_TASK][\"train\"]))\n",
    "    )\n",
    "    epoch_val_loss = val_running_loss / len(dataloaders[CURR_TASK][\"val\"].dataset)\n",
    "\n",
    "    writer.add_scalar(\n",
    "        f\"Loss/train_{new_run_name}_{CURR_TASK}\", epoch_train_loss, epoch + 1\n",
    "    )\n",
    "    writer.add_scalar(f\"Loss/val_{new_run_name}_{CURR_TASK}\", epoch_val_loss, epoch + 1)\n",
    "    writer.add_scalar(\n",
    "        f\"mIoU/train_{new_run_name}_{CURR_TASK}\", mean_train_iou, epoch + 1\n",
    "    )\n",
    "    writer.add_scalar(f\"mIoU/val_{new_run_name}_{CURR_TASK}\", mean_val_iou, epoch + 1)\n",
    "\n",
    "    wandb.log(\n",
    "        {\n",
    "            f\"Loss/train_{CURR_TASK}\": epoch_train_loss,\n",
    "            f\"Loss/val_{CURR_TASK}\": epoch_val_loss,\n",
    "            f\"mIoU/train_{CURR_TASK}\": mean_train_iou,\n",
    "            f\"mIoU/val_{CURR_TASK}\": mean_val_iou,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    tqdm.write(\n",
    "        f\"Epoch {epoch + 1}/{NUM_EPOCHS}, Train Loss: {epoch_train_loss:.4f}, Train mIoU: {mean_train_iou:.4f}, Validation Loss: {epoch_val_loss:.4f}, Validation mIoU: {mean_val_iou:.4f}\"\n",
    "    )\n",
    "\n",
    "    if mean_val_iou > best_val_metric:\n",
    "        best_val_metric = mean_val_iou\n",
    "        # model.save_pretrained(f\"{best_model_dir}{CURR_TASK}/\")\n",
    "        best_model_weights = deepcopy(student.state_dict())\n",
    "        counter = 0\n",
    "    else:\n",
    "        counter += 1\n",
    "        if counter == PATIENCE:\n",
    "            print(\"Early stopping at epoch\", epoch)\n",
    "            break\n",
    "\n",
    "os.makedirs(f\"{best_model_dir}{CURR_TASK}/\", exist_ok=True)\n",
    "artifact = wandb.Artifact(f\"best_model_{new_run_name}\", type=\"model\")\n",
    "artifact.add_file(\n",
    "    f\"{best_model_dir}{CURR_TASK}/best_model_{new_run_name}.pth\",\n",
    "    torch.save(\n",
    "        best_model_weights, f\"{best_model_dir}{CURR_TASK}/best_model_{new_run_name}.pth\"\n",
    "    ),\n",
    ")\n",
    "wandb.run.log_artifact(artifact)\n",
    "\n",
    "if os.path.exists(model_dir + f\"{new_run_name}\"):\n",
    "    shutil.rmtree(model_dir + f\"{new_run_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Test results on B first"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-20T18:02:16.858577Z",
     "iopub.status.busy": "2024-06-20T18:02:16.858223Z",
     "iopub.status.idle": "2024-06-20T18:02:19.764559Z",
     "shell.execute_reply": "2024-06-20T18:02:19.763362Z",
     "shell.execute_reply.started": "2024-06-20T18:02:16.858554Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Downloading large artifact best_model_M2F-Swin-Tiny-KD-EMA-Mean-Loss-Batch-Replay:latest, 181.32MB. 1 files... \n",
      "\u001b[34m\u001b[1mwandb\u001b[0m:   1 of 1 files downloaded.  \n",
      "Done. 0:0:0.3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Mask2FormerForUniversalSegmentation(\n",
       "  (model): Mask2FormerModel(\n",
       "    (pixel_level_module): Mask2FormerPixelLevelModule(\n",
       "      (encoder): SwinBackbone(\n",
       "        (embeddings): SwinEmbeddings(\n",
       "          (patch_embeddings): SwinPatchEmbeddings(\n",
       "            (projection): Conv2d(3, 96, kernel_size=(4, 4), stride=(4, 4))\n",
       "          )\n",
       "          (norm): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "        (encoder): SwinEncoder(\n",
       "          (layers): ModuleList(\n",
       "            (0): SwinStage(\n",
       "              (blocks): ModuleList(\n",
       "                (0-1): 2 x SwinLayer(\n",
       "                  (layernorm_before): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "                  (attention): SwinAttention(\n",
       "                    (self): SwinSelfAttention(\n",
       "                      (query): Linear(in_features=96, out_features=96, bias=True)\n",
       "                      (key): Linear(in_features=96, out_features=96, bias=True)\n",
       "                      (value): Linear(in_features=96, out_features=96, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (output): SwinSelfOutput(\n",
       "                      (dense): Linear(in_features=96, out_features=96, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                  (drop_path): SwinDropPath(p=0.1)\n",
       "                  (layernorm_after): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "                  (intermediate): SwinIntermediate(\n",
       "                    (dense): Linear(in_features=96, out_features=384, bias=True)\n",
       "                    (intermediate_act_fn): GELUActivation()\n",
       "                  )\n",
       "                  (output): SwinOutput(\n",
       "                    (dense): Linear(in_features=384, out_features=96, bias=True)\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (downsample): SwinPatchMerging(\n",
       "                (reduction): Linear(in_features=384, out_features=192, bias=False)\n",
       "                (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "            (1): SwinStage(\n",
       "              (blocks): ModuleList(\n",
       "                (0-1): 2 x SwinLayer(\n",
       "                  (layernorm_before): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "                  (attention): SwinAttention(\n",
       "                    (self): SwinSelfAttention(\n",
       "                      (query): Linear(in_features=192, out_features=192, bias=True)\n",
       "                      (key): Linear(in_features=192, out_features=192, bias=True)\n",
       "                      (value): Linear(in_features=192, out_features=192, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (output): SwinSelfOutput(\n",
       "                      (dense): Linear(in_features=192, out_features=192, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                  (drop_path): SwinDropPath(p=0.1)\n",
       "                  (layernorm_after): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "                  (intermediate): SwinIntermediate(\n",
       "                    (dense): Linear(in_features=192, out_features=768, bias=True)\n",
       "                    (intermediate_act_fn): GELUActivation()\n",
       "                  )\n",
       "                  (output): SwinOutput(\n",
       "                    (dense): Linear(in_features=768, out_features=192, bias=True)\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (downsample): SwinPatchMerging(\n",
       "                (reduction): Linear(in_features=768, out_features=384, bias=False)\n",
       "                (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "            (2): SwinStage(\n",
       "              (blocks): ModuleList(\n",
       "                (0-5): 6 x SwinLayer(\n",
       "                  (layernorm_before): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "                  (attention): SwinAttention(\n",
       "                    (self): SwinSelfAttention(\n",
       "                      (query): Linear(in_features=384, out_features=384, bias=True)\n",
       "                      (key): Linear(in_features=384, out_features=384, bias=True)\n",
       "                      (value): Linear(in_features=384, out_features=384, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (output): SwinSelfOutput(\n",
       "                      (dense): Linear(in_features=384, out_features=384, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                  (drop_path): SwinDropPath(p=0.1)\n",
       "                  (layernorm_after): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "                  (intermediate): SwinIntermediate(\n",
       "                    (dense): Linear(in_features=384, out_features=1536, bias=True)\n",
       "                    (intermediate_act_fn): GELUActivation()\n",
       "                  )\n",
       "                  (output): SwinOutput(\n",
       "                    (dense): Linear(in_features=1536, out_features=384, bias=True)\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "              (downsample): SwinPatchMerging(\n",
       "                (reduction): Linear(in_features=1536, out_features=768, bias=False)\n",
       "                (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
       "              )\n",
       "            )\n",
       "            (3): SwinStage(\n",
       "              (blocks): ModuleList(\n",
       "                (0-1): 2 x SwinLayer(\n",
       "                  (layernorm_before): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                  (attention): SwinAttention(\n",
       "                    (self): SwinSelfAttention(\n",
       "                      (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                      (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                      (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                    (output): SwinSelfOutput(\n",
       "                      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                      (dropout): Dropout(p=0.0, inplace=False)\n",
       "                    )\n",
       "                  )\n",
       "                  (drop_path): SwinDropPath(p=0.1)\n",
       "                  (layernorm_after): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "                  (intermediate): SwinIntermediate(\n",
       "                    (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "                    (intermediate_act_fn): GELUActivation()\n",
       "                  )\n",
       "                  (output): SwinOutput(\n",
       "                    (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "                    (dropout): Dropout(p=0.0, inplace=False)\n",
       "                  )\n",
       "                )\n",
       "              )\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (hidden_states_norms): ModuleDict(\n",
       "          (stage1): LayerNorm((96,), eps=1e-05, elementwise_affine=True)\n",
       "          (stage2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
       "          (stage3): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
       "          (stage4): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "      (decoder): Mask2FormerPixelDecoder(\n",
       "        (position_embedding): Mask2FormerSinePositionEmbedding()\n",
       "        (input_projections): ModuleList(\n",
       "          (0): Sequential(\n",
       "            (0): Conv2d(768, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "          )\n",
       "          (1): Sequential(\n",
       "            (0): Conv2d(384, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "          )\n",
       "          (2): Sequential(\n",
       "            (0): Conv2d(192, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "            (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "          )\n",
       "        )\n",
       "        (encoder): Mask2FormerPixelDecoderEncoderOnly(\n",
       "          (layers): ModuleList(\n",
       "            (0-5): 6 x Mask2FormerPixelDecoderEncoderLayer(\n",
       "              (self_attn): Mask2FormerPixelDecoderEncoderMultiscaleDeformableAttention(\n",
       "                (sampling_offsets): Linear(in_features=256, out_features=192, bias=True)\n",
       "                (attention_weights): Linear(in_features=256, out_features=96, bias=True)\n",
       "                (value_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "                (output_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              )\n",
       "              (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "              (fc1): Linear(in_features=256, out_features=1024, bias=True)\n",
       "              (fc2): Linear(in_features=1024, out_features=256, bias=True)\n",
       "              (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "        (mask_projection): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        (adapter_1): Sequential(\n",
       "          (0): Conv2d(96, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "        )\n",
       "        (layer_1): Sequential(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (1): GroupNorm(32, 256, eps=1e-05, affine=True)\n",
       "          (2): ReLU()\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (transformer_module): Mask2FormerTransformerModule(\n",
       "      (position_embedder): Mask2FormerSinePositionEmbedding()\n",
       "      (queries_embedder): Embedding(100, 256)\n",
       "      (queries_features): Embedding(100, 256)\n",
       "      (decoder): Mask2FormerMaskedAttentionDecoder(\n",
       "        (layers): ModuleList(\n",
       "          (0-8): 9 x Mask2FormerMaskedAttentionDecoderLayer(\n",
       "            (self_attn): Mask2FormerAttention(\n",
       "              (k_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (v_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (q_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (out_proj): Linear(in_features=256, out_features=256, bias=True)\n",
       "            )\n",
       "            (activation_fn): ReLU()\n",
       "            (self_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (cross_attn): MultiheadAttention(\n",
       "              (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
       "            )\n",
       "            (cross_attn_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "            (fc1): Linear(in_features=256, out_features=2048, bias=True)\n",
       "            (fc2): Linear(in_features=2048, out_features=256, bias=True)\n",
       "            (final_layer_norm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "          )\n",
       "        )\n",
       "        (layernorm): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
       "        (mask_predictor): Mask2FormerMaskPredictor(\n",
       "          (mask_embedder): Mask2FormerMLPPredictionHead(\n",
       "            (0): Mask2FormerPredictionBlock(\n",
       "              (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (1): ReLU()\n",
       "            )\n",
       "            (1): Mask2FormerPredictionBlock(\n",
       "              (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (1): ReLU()\n",
       "            )\n",
       "            (2): Mask2FormerPredictionBlock(\n",
       "              (0): Linear(in_features=256, out_features=256, bias=True)\n",
       "              (1): Identity()\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (level_embed): Embedding(3, 256)\n",
       "    )\n",
       "  )\n",
       "  (class_predictor): Linear(in_features=256, out_features=13, bias=True)\n",
       "  (criterion): Mask2FormerLoss(\n",
       "    (matcher): Mask2FormerHungarianMatcher()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load best model and evaluate on test\n",
    "#model = Mask2FormerForUniversalSegmentation.from_pretrained(f\"{best_model_dir}{CURR_TASK}/\").to(device)\n",
    "\n",
    "# Construct the artifact path\n",
    "artifact_path = f\"{user_or_team}/{project_name}/best_model_{new_run_name}:latest\"\n",
    "\n",
    "# Load from W&B\n",
    "api = wandb.Api()\n",
    "artifact=api.artifact(artifact_path)\n",
    "model_dir=artifact.download()\n",
    "model_state_dict_path = os.path.join(model_dir, f\"best_model_{new_run_name}.pth\" )\n",
    "model_state_dict = torch.load(model_state_dict_path)\n",
    "student = Mask2FormerForUniversalSegmentation(mask2former_config)\n",
    "student.load_state_dict(model_state_dict)\n",
    "student.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-20T18:02:19.944393Z",
     "iopub.status.busy": "2024-06-20T18:02:19.943936Z",
     "iopub.status.idle": "2024-06-20T18:02:51.570291Z",
     "shell.execute_reply": "2024-06-20T18:02:51.569265Z",
     "shell.execute_reply.started": "2024-06-20T18:02:19.944393Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test loop: 100%|██████████| 15/15 [00:28<00:00,  1.89s/it, loss=14.0646] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 6.9802, Test mIoU: 0.8151\n"
     ]
    }
   ],
   "source": [
    "student.eval()\n",
    "test_running_loss = 0\n",
    "CURR_TASK=\"B\"\n",
    "test_loader = tqdm(dataloaders[CURR_TASK][\"test\"], desc=\"Test loop\")\n",
    "BATCH_INDEX = 0\n",
    "table = wandb.Table(columns=[\"ID\", \"Image\"])\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        # Move everything to the device\n",
    "        batch[\"pixel_values\"] = batch[\"pixel_values\"].to(device)\n",
    "        batch[\"pixel_mask\"] = batch[\"pixel_mask\"].to(device)\n",
    "        batch[\"mask_labels\"] = [entry.to(device) for entry in batch[\"mask_labels\"]]\n",
    "        batch[\"class_labels\"] = [entry.to(device) for entry in batch[\"class_labels\"]]\n",
    "        # Compute output and loss\n",
    "        student_outputs = student(**batch)\n",
    "\n",
    "        loss = student_outputs.loss\n",
    "        # Record losses\n",
    "        current_loss = loss.item() * batch[\"pixel_values\"].size(0)\n",
    "        test_running_loss += current_loss\n",
    "        test_loader.set_postfix(loss=f\"{current_loss:.4f}\")\n",
    "\n",
    "        # Extract and compute metrics\n",
    "        pred_maps, masks = m2f_extract_pred_maps_and_masks(\n",
    "            batch, student_outputs, m2f_preprocessor_B\n",
    "        )\n",
    "        metric.add_batch(references=masks, predictions=pred_maps)\n",
    "        if BATCH_INDEX <5:\n",
    "            # Visualize\n",
    "            log_table_of_images(\n",
    "                table, # common table for all batches\n",
    "                batch[\"pixel_values\"],\n",
    "                pixel_mean_B, # remove normalization\n",
    "                pixel_std_B, # remove normalization\n",
    "                pred_maps,\n",
    "                masks,\n",
    "                BATCH_INDEX, # correct indexing in table\n",
    "            )\n",
    "            BATCH_INDEX += 1\n",
    "# Log table\n",
    "wandb.log({f\"{CURR_TASK}_TEST_AFTER_TRAINING_B\": table})\n",
    "\n",
    "# After compute the batches that were added are deleted\n",
    "test_metrics_B = metric.compute(\n",
    "    num_labels=NUM_CLASSES, ignore_index=BG_VALUE_255, reduce_labels=False\n",
    ")\n",
    "mean_test_iou = test_metrics_B[\"mean_iou\"]\n",
    "final_test_loss = test_running_loss / len(dataloaders[CURR_TASK][\"test\"].dataset)\n",
    "wandb.log({\n",
    "    f\"Loss/test_{CURR_TASK}\": final_test_loss,\n",
    "    f\"mIoU/test_{CURR_TASK}\": mean_test_iou\n",
    "})\n",
    "print(f\"Test Loss: {final_test_loss:.4f}, Test mIoU: {mean_test_iou:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Test results on A after training on B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-20T18:02:51.574855Z",
     "iopub.status.busy": "2024-06-20T18:02:51.572848Z",
     "iopub.status.idle": "2024-06-20T18:03:56.607202Z",
     "shell.execute_reply": "2024-06-20T18:03:56.600599Z",
     "shell.execute_reply.started": "2024-06-20T18:02:51.574855Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test loop: 100%|██████████| 37/37 [00:56<00:00,  1.53s/it, loss=160.1931]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 28.3663, Test mIoU: 0.6353\n"
     ]
    }
   ],
   "source": [
    "# To avoid making stupid errors\n",
    "CURR_TASK = \"A\"\n",
    "\n",
    "student.eval()\n",
    "test_running_loss = 0\n",
    "test_loader = tqdm(dataloaders[CURR_TASK][\"test\"], desc=\"Test loop\")\n",
    "BATCH_INDEX = 0\n",
    "table = wandb.Table(columns=[\"ID\", \"Image\"])\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        # Move everything to the device\n",
    "        batch[\"pixel_values\"] = batch[\"pixel_values\"].to(device)\n",
    "        batch[\"pixel_mask\"] = batch[\"pixel_mask\"].to(device)\n",
    "        batch[\"mask_labels\"] = [entry.to(device) for entry in batch[\"mask_labels\"]]\n",
    "        batch[\"class_labels\"] = [entry.to(device) for entry in batch[\"class_labels\"]]\n",
    "        # Compute output and loss\n",
    "        student_outputs = student(**batch)\n",
    "\n",
    "        loss = student_outputs.loss\n",
    "        # Record losses\n",
    "        current_loss = loss.item() * batch[\"pixel_values\"].size(0)\n",
    "        test_running_loss += current_loss\n",
    "        test_loader.set_postfix(loss=f\"{current_loss:.4f}\")\n",
    "\n",
    "        # Extract and compute metrics\n",
    "        pred_maps, masks = m2f_extract_pred_maps_and_masks(\n",
    "            batch, student_outputs, m2f_preprocessor_A\n",
    "        )\n",
    "        metric.add_batch(references=masks, predictions=pred_maps)\n",
    "        if BATCH_INDEX <5:\n",
    "            # Visualize\n",
    "            log_table_of_images(\n",
    "                table, # common table for all batches\n",
    "                batch[\"pixel_values\"],\n",
    "                pixel_mean_A, # remove normalization\n",
    "                pixel_std_A, # remove normalization\n",
    "                pred_maps,\n",
    "                masks,\n",
    "                BATCH_INDEX, # correct indexing in table\n",
    "            )\n",
    "            BATCH_INDEX += 1\n",
    "# Log table\n",
    "wandb.log({f\"{CURR_TASK}_TEST_AFTER_TRAINING_B\": table})\n",
    "\n",
    "# After compute the batches that were added are deleted\n",
    "test_metrics_forgetting_A = metric.compute(\n",
    "    num_labels=NUM_CLASSES, ignore_index=BG_VALUE_255, reduce_labels=False\n",
    ")\n",
    "mean_test_iou = test_metrics_forgetting_A[\"mean_iou\"]\n",
    "final_test_loss = test_running_loss / len(dataloaders[CURR_TASK][\"test\"].dataset)\n",
    "wandb.log({\n",
    "    f\"Loss/test_naive_forgetting_{CURR_TASK}\": final_test_loss,\n",
    "    f\"mIoU/test_naive_forgetting_{CURR_TASK}\": mean_test_iou\n",
    "})\n",
    "print(f\"Test Loss: {final_test_loss:.4f}, Test mIoU: {mean_test_iou:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-20T18:03:56.611113Z",
     "iopub.status.busy": "2024-06-20T18:03:56.609063Z",
     "iopub.status.idle": "2024-06-20T18:04:02.913265Z",
     "shell.execute_reply": "2024-06-20T18:04:02.912333Z",
     "shell.execute_reply.started": "2024-06-20T18:03:56.611067Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**** Overall mIoU ****\n",
      "mIoU on task A before training on B: 0.7464105108377289\n",
      "mIoU on task B before training on B: 0.27166345992563895\n",
      "\n",
      "\n",
      "mIoU on task B after training on B: 0.8150631765771962\n",
      "mIoU on task A after training on B: 0.6353404073766877\n",
      "\n",
      "**** Per category mIoU ****\n",
      "Per category mIoU on task A before training on B: [0.97407111 0.90755863 0.78471722 0.60486064 0.57420317 0.43836531\n",
      " 0.78411077 0.66921035 0.71825757 0.71685732 0.94148954 0.84322449]\n",
      "Per category mIoU on task B before training on B: [0.929421   0.34517766 0.04450129 0.09932577 0.11234525 0.093081\n",
      " 0.04285594 0.08668445 0.04981619 0.15111389 0.77349505 0.53214402]\n",
      "\n",
      "\n",
      "Per category mIoU on task B after training on B: [0.9914874  0.80837453 0.77269391 0.66505836 0.66853    0.74212978\n",
      " 0.85957265 0.78151629 0.83306427 0.78752616 0.96152434 0.90928043]\n",
      "Per category mIoU on task A after training on B: [0.97268009 0.84597724 0.62531142 0.47774713 0.41881355 0.02267958\n",
      " 0.74043197 0.46997359 0.61592195 0.65102676 0.93893134 0.84459026]\n",
      "\n",
      "**** Average learning accuracies ****\n",
      "Average learning acc.: 0.7807368437074625\n",
      "Per category Average learning acc.: [0.98277925 0.85796658 0.77870557 0.6349595  0.62136659 0.59024755\n",
      " 0.82184171 0.72536332 0.77566092 0.75219174 0.95150694 0.87625246]\n",
      "\n",
      "**** Forgetting ****\n",
      "Total forgetting: 0.11107010346104118\n",
      "Per category forgetting: [ 0.00139102  0.06158139  0.1594058   0.12711352  0.15538962  0.41568573\n",
      "  0.04367881  0.19923676  0.10233562  0.06583056  0.0025582  -0.00136578]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>\n",
       "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
       "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
       "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
       "    </style>\n",
       "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>Loss/test_A</td><td>▁</td></tr><tr><td>Loss/test_B</td><td>█▁</td></tr><tr><td>Loss/test_naive_forgetting_A</td><td>▁</td></tr><tr><td>Loss/train_B</td><td>█▅▄▄▃▃▃▃▂▃▃▂▂▂▂▂▂▂▂▄▂▂▁▁▂▂▁▁▁▁▁▁▁</td></tr><tr><td>Loss/val_B</td><td>█▅▄▄▃▃▃▂▁▅▂▂▂▂▂▂▂▂▅▃▃▃▃▄▄▄▃▃▄▄▄▄▅</td></tr><tr><td>eval/avg_learning_acc</td><td>▁</td></tr><tr><td>eval/total_forgetting</td><td>▁</td></tr><tr><td>mIoU/test_A</td><td>▁</td></tr><tr><td>mIoU/test_B</td><td>▁█</td></tr><tr><td>mIoU/test_naive_forgetting_A</td><td>▁</td></tr><tr><td>mIoU/train_B</td><td>▁▄▅▅▆▆▆▇▇▆▇▇▇▇▇▇▇█▇▄▇████████████</td></tr><tr><td>mIoU/val_B</td><td>▁▅▃▆▄▆▃▆▇▃▇▇███▇██▂▆▆▇▇▇▇▇▇▇█▆▇▇▇</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>Loss/test_A</td><td>16.46218</td></tr><tr><td>Loss/test_B</td><td>6.98016</td></tr><tr><td>Loss/test_naive_forgetting_A</td><td>28.36631</td></tr><tr><td>Loss/train_B</td><td>3.08858</td></tr><tr><td>Loss/val_B</td><td>7.48039</td></tr><tr><td>eval/avg_learning_acc</td><td>0.78074</td></tr><tr><td>eval/total_forgetting</td><td>0.11107</td></tr><tr><td>mIoU/test_A</td><td>0.74641</td></tr><tr><td>mIoU/test_B</td><td>0.81506</td></tr><tr><td>mIoU/test_naive_forgetting_A</td><td>0.63534</td></tr><tr><td>mIoU/train_B</td><td>0.95381</td></tr><tr><td>mIoU/val_B</td><td>0.76641</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">M2F-Swin-Tiny-KD-EMA-Mean-Loss-Batch-Replay</strong> at: <a href='https://wandb.ai/continual-learning-tum/M2F_latest/runs/yhehcp9z' target=\"_blank\">https://wandb.ai/continual-learning-tum/M2F_latest/runs/yhehcp9z</a><br/> View project at: <a href='https://wandb.ai/continual-learning-tum/M2F_latest' target=\"_blank\">https://wandb.ai/continual-learning-tum/M2F_latest</a><br/>Synced 5 W&B file(s), 5 media file(s), 970 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20240620_093024-yhehcp9z/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Collect overall mIoU\n",
    "mIoU_A_before = test_metrics_A[\"mean_iou\"]\n",
    "mIoU_B_before=test_metrics_B_before[\"mean_iou\"]\n",
    "mIoU_forgetting_A = test_metrics_forgetting_A[\"mean_iou\"]\n",
    "mIoU_B = test_metrics_B[\"mean_iou\"]\n",
    "\n",
    "# Collect per category mIoU\n",
    "per_category_mIoU_A_before = np.array(test_metrics_A[\"per_category_iou\"])\n",
    "per_category_mIoU_A = np.array(test_metrics_forgetting_A[\"per_category_iou\"])\n",
    "per_category_mIoU_B = np.array(test_metrics_B[\"per_category_iou\"])\n",
    "per_category_mIoU_B_before=np.array(test_metrics_B_before[\"per_category_iou\"])\n",
    "\n",
    "# Average learning accuracies (mIoUs)\n",
    "avg_learning_acc = (mIoU_A_before + mIoU_B) / 2\n",
    "per_category_avg_learning_acc = (per_category_mIoU_A_before + per_category_mIoU_B) / 2\n",
    "\n",
    "# Forgetting\n",
    "total_forgetting = mIoU_A_before - mIoU_forgetting_A\n",
    "per_category_forgetting = (per_category_mIoU_A_before - per_category_mIoU_A)\n",
    "\n",
    "# Export evaluation metrics to WandB\n",
    "wandb.log({\n",
    "    \"eval/avg_learning_acc\": avg_learning_acc,\n",
    "    \"eval/total_forgetting\": total_forgetting,\n",
    "})\n",
    "\n",
    "columns=[\"categories\",\"per_category_mIoU_A_before\",\"per_category_mIoU_B_before\",\n",
    "         \"per_category_mIoU_B\", \"per_category_mIoU_A\",\n",
    "         \"per_category_avg_learning_acc\",\"per_category_forgetting\"]\n",
    "data=[]\n",
    "\n",
    "data.append([\"background\",per_category_mIoU_A_before[0],\n",
    "                 per_category_mIoU_B_before[0],\n",
    "                 per_category_mIoU_B[0],\n",
    "                per_category_mIoU_A[0],per_category_avg_learning_acc[0],\n",
    "                per_category_forgetting[0]])\n",
    "\n",
    "for cat_id in range(1,12):\n",
    "    data.append([ZEISS_CATEGORIES[cat_id],per_category_mIoU_A_before[cat_id],\n",
    "                 per_category_mIoU_B_before[cat_id],\n",
    "                 per_category_mIoU_B[cat_id],\n",
    "                per_category_mIoU_A[cat_id],per_category_avg_learning_acc[cat_id],\n",
    "                per_category_forgetting[cat_id]])\n",
    "    \n",
    "    \n",
    "table = wandb.Table(columns=columns, data=data)\n",
    "wandb.log({\"per_category_metrics_table\": table})\n",
    "\n",
    "print(\"**** Overall mIoU ****\")\n",
    "print(f\"mIoU on task A before training on B: {mIoU_A_before}\")\n",
    "print(f\"mIoU on task B before training on B: {mIoU_B_before}\")\n",
    "print(\"\\n\")\n",
    "print(f\"mIoU on task B after training on B: {mIoU_B}\")\n",
    "print(f\"mIoU on task A after training on B: {mIoU_forgetting_A}\")\n",
    "\n",
    "print(\"\\n**** Per category mIoU ****\")\n",
    "print(f\"Per category mIoU on task A before training on B: {per_category_mIoU_A_before}\")\n",
    "print(f\"Per category mIoU on task B before training on B: {per_category_mIoU_B_before}\")\n",
    "print(\"\\n\")\n",
    "print(f\"Per category mIoU on task B after training on B: {per_category_mIoU_B}\")\n",
    "print(f\"Per category mIoU on task A after training on B: {per_category_mIoU_A}\")\n",
    "\n",
    "print(\"\\n**** Average learning accuracies ****\")\n",
    "print(f\"Average learning acc.: {avg_learning_acc}\")\n",
    "print(f\"Per category Average learning acc.: {per_category_avg_learning_acc}\")\n",
    "\n",
    "print(\"\\n**** Forgetting ****\")\n",
    "print(f\"Total forgetting: {total_forgetting}\")\n",
    "print(f\"Per category forgetting: {per_category_forgetting}\")\n",
    "wandb.finish()\n",
    "\n",
    "if os.path.exists(\"artifacts/\"):\n",
    "    shutil.rmtree(\"artifacts/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-20T18:16:54.610971Z",
     "iopub.status.busy": "2024-06-20T18:16:54.610578Z",
     "iopub.status.idle": "2024-06-20T18:16:54.689197Z",
     "shell.execute_reply": "2024-06-20T18:16:54.688564Z",
     "shell.execute_reply.started": "2024-06-20T18:16:54.610971Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([21, 3, 270, 480])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_replay = m2f_dataset_collate(subset_A)\n",
    "batch_replay[\"pixel_values\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-20T18:13:34.333446Z",
     "iopub.status.busy": "2024-06-20T18:13:34.333104Z",
     "iopub.status.idle": "2024-06-20T18:13:36.008244Z",
     "shell.execute_reply": "2024-06-20T18:13:36.007037Z",
     "shell.execute_reply.started": "2024-06-20T18:13:34.333423Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "wandb version 0.17.2 is available!  To upgrade, please run:\n",
       " $ pip install wandb --upgrade"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.17.0"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/notebooks/continual-learning/wandb/run-20240620_181334-15s6u4gq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/continual-learning-tum/M2F_latest/runs/15s6u4gq' target=\"_blank\">Mean Loss Samples</a></strong> to <a href='https://wandb.ai/continual-learning-tum/M2F_latest' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/continual-learning-tum/M2F_latest' target=\"_blank\">https://wandb.ai/continual-learning-tum/M2F_latest</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/continual-learning-tum/M2F_latest/runs/15s6u4gq' target=\"_blank\">https://wandb.ai/continual-learning-tum/M2F_latest/runs/15s6u4gq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<button onClick=\"this.nextSibling.style.display='block';this.style.display='none';\">Display W&B run</button><iframe src='https://wandb.ai/continual-learning-tum/M2F_latest/runs/15s6u4gq?jupyter=true' style='border:none;width:100%;height:420px;display:none;'></iframe>"
      ],
      "text/plain": [
       "<wandb.sdk.wandb_run.Run at 0x7feb88447aa0>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wandb.init(\n",
    "    project=project_name,\n",
    "    config={\n",
    "        \"learning_rate\": LEARNING_RATE,\n",
    "        \"learning_rate_multiplier\": LR_MULTIPLIER,\n",
    "        \"backbone_learning_rate\": BACKBONE_LR,\n",
    "        \"learning_rate_scheduler\": scheduler.__class__.__name__,\n",
    "        \"optimizer\": optimizer.__class__.__name__,\n",
    "        \"backbone\": SWIN_BACKBONE,\n",
    "        \"m2f_preprocessor\": m2f_preprocessor_B.__dict__,\n",
    "        \"m2f_model_config\": student.config,\n",
    "        \"lambdas\": LAMBDAS,\n",
    "        \"ema_decay\": EMA_DECAY,\n",
    "    },\n",
    "    name=\"Mean Loss Samples\",\n",
    "    notes=\"M2F with tiny Swin backbone pretrained on ImageNet-1K. \\\n",
    "        Scenario: Pretrained on A, Train on B, Test forgetting on A\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-06-20T18:17:03.074463Z",
     "iopub.status.busy": "2024-06-20T18:17:03.073937Z",
     "iopub.status.idle": "2024-06-20T18:17:06.687894Z",
     "shell.execute_reply": "2024-06-20T18:17:06.686970Z",
     "shell.execute_reply.started": "2024-06-20T18:17:03.074441Z"
    }
   },
   "outputs": [],
   "source": [
    "table = wandb.Table(columns=[\"ID\", \"Image\"])\n",
    "\n",
    "student.eval()\n",
    "with torch.no_grad():\n",
    "    batch = batch_replay\n",
    "    batch[\"pixel_values\"] = batch[\"pixel_values\"].to(device)\n",
    "    batch[\"pixel_mask\"] = batch[\"pixel_mask\"].to(device)\n",
    "    batch[\"mask_labels\"] = [entry.to(device) for entry in batch[\"mask_labels\"]]\n",
    "    batch[\"class_labels\"] = [entry.to(device) for entry in batch[\"class_labels\"]]\n",
    "    # Compute output and loss\n",
    "    outputs = student(**batch)\n",
    "\n",
    "    loss = outputs.loss\n",
    "    # Record losses\n",
    "    current_loss = loss.item() * batch[\"pixel_values\"].size(0)\n",
    "    test_running_loss += current_loss\n",
    "    test_loader.set_postfix(loss=f\"{current_loss:.4f}\")\n",
    "\n",
    "    # Extract and compute metrics\n",
    "    pred_maps, masks = m2f_extract_pred_maps_and_masks(\n",
    "        batch, outputs, m2f_preprocessor_B\n",
    "    )\n",
    "    metric.add_batch(references=masks, predictions=pred_maps)\n",
    "    # Visualize\n",
    "    log_table_of_images(\n",
    "        table, # common table for all batches\n",
    "        batch[\"pixel_values\"],\n",
    "        pixel_mean_A, # remove normalization\n",
    "        pixel_std_A, # remove normalization\n",
    "        pred_maps,\n",
    "        masks,\n",
    "        0, # correct indexing in table\n",
    "    )\n",
    "# Log table\n",
    "wandb.log({f\"Mean_Loss_Samples_All\": table})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
